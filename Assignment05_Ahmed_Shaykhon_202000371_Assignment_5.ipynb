{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment05-Ahmed Shaykhon-202000371- Assignment 5.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "XAEdiF4Ymizi"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmedShaykhon/NU-Deep-Learning-Assignments/blob/master/Assignment05_Ahmed_Shaykhon_202000371_Assignment_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAEdiF4Ymizi"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H418uCYl6OlQ"
      },
      "source": [
        "# First we need to mount the Google drive \n",
        "#import os\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "# # Here specify the path to your directory\n",
        "# !ls \"/content/gdrive/My Drive/CIFAR10_ResNet.ipynb\"  \n",
        "# root_path = '/content/gdrive' \n",
        "# path ='/content/gdrive'\n",
        "# os.chdir(path)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjvbl18HsjnT"
      },
      "source": [
        "---\n",
        "## ResNet Architecture\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9Y2hYRwB-qg",
        "outputId": "4e9b5fc5-70fe-4af0-fd71-cadfbacffff4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "!pip install --upgrade git+https://github.com/broadinstitute/keras-resnet\n",
        "import keras\n",
        "import keras_resnet as resnet\n",
        "import torch.nn.functional as F\n",
        "#import resnet\n",
        "# We define all the classes and function regarding the ResNet architecture in this code cell\n",
        "__all__ = ['ResNet', 'resnet20', 'resnet32', 'resnet44', 'resnet56']\n",
        " \n",
        "def _weights_init(m):\n",
        "    \"\"\"\n",
        "        Initialization of CNN weights\n",
        "    \"\"\"\n",
        "    classname = m.__class__.__name__\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "        init.kaiming_normal_(m.weight)\n",
        "\n",
        "\n",
        "class LambdaLayer(nn.Module):\n",
        "    \"\"\"\n",
        "      Identity mapping between ResNet blocks with diffrenet size feature map\n",
        "    \"\"\"\n",
        "    def __init__(self, lambd):\n",
        "        super(LambdaLayer, self).__init__()\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lambd(x)\n",
        "\n",
        "# A basic block as shown in Fig.3 (right) in the paper consists of two convolutional blocks, each followed by a Bach-Norm layer. \n",
        "# Every basic block is shortcuted in ResNet architecture to construct f(x)+x module. \n",
        "# Expansion for option 'A' in the paper is equal to identity with extra zero entries padded\n",
        "# for increasing dimensions between layers with different feature map size. This option introduces no extra parameter. \n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            if option == 'A':\n",
        "                \"\"\"\n",
        "                For CIFAR10 experiment, ResNet paper uses option A.\n",
        "                \"\"\"\n",
        "                self.shortcut = LambdaLayer(lambda x:\n",
        "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
        "            elif option == 'B':\n",
        "                self.shortcut = nn.Sequential(\n",
        "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                     nn.BatchNorm2d(self.expansion * planes)\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "# Stack of 3 times 2*n (n is the number of basic blocks) layers are used for making the ResNet model, \n",
        "# where each 2n layers have feature maps of size {16,32,64}, respectively. \n",
        "# The subsampling is performed by convolutions with a stride of 2.\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 16\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "        self.apply(_weights_init)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, out.size()[3])\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def resnet20():\n",
        "    return ResNet(BasicBlock, [3, 3, 3])\n",
        "\n",
        "\n",
        "def resnet32():\n",
        "    return ResNet(BasicBlock, [5, 5, 5])\n",
        "\n",
        "\n",
        "def resnet44():\n",
        "    return ResNet(BasicBlock, [7, 7, 7])\n",
        "\n",
        "\n",
        "def resnet56():\n",
        "    return ResNet(BasicBlock, [9, 9, 9])\n",
        "\n",
        "\n",
        "\n",
        "def test(net):\n",
        "    total_params = 0\n",
        "\n",
        "    for x in filter(lambda p: p.requires_grad, net.parameters()):\n",
        "        total_params += np.prod(x.data.numpy().shape)\n",
        "    print(\"Total number of params\", total_params)\n",
        "    print(\"Total layers\", len(list(filter(lambda p: p.requires_grad and len(p.data.size())>1, net.parameters()))))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    for net_name in __all__:\n",
        "        if net_name.startswith('resnet'):\n",
        "            print(net_name)\n",
        "            test(globals()[net_name]())\n",
        "            print()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/broadinstitute/keras-resnet\n",
            "  Cloning https://github.com/broadinstitute/keras-resnet to /tmp/pip-req-build-2k8cqu4y\n",
            "  Running command git clone -q https://github.com/broadinstitute/keras-resnet /tmp/pip-req-build-2k8cqu4y\n",
            "Requirement already satisfied: keras>=2.2.4 in /usr/local/lib/python3.7/dist-packages (from keras-resnet==0.2.0) (2.8.0)\n",
            "resnet20\n",
            "Total number of params 269722\n",
            "Total layers 20\n",
            "\n",
            "resnet32\n",
            "Total number of params 464154\n",
            "Total layers 32\n",
            "\n",
            "resnet44\n",
            "Total number of params 658586\n",
            "Total layers 44\n",
            "\n",
            "resnet56\n",
            "Total number of params 853018\n",
            "Total layers 56\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANRMm1zq2J7a"
      },
      "source": [
        "\n",
        "---\n",
        "# Hyperparameter Setting\n",
        "\n",
        "We define a class referred to as ```MyResNetArgs```in the following to assign the hyperparameters such as a number of training epochs, learning rate, momentum, batch size, etc. to the training function. The objects of this class are initialized inherently once created with a void argument. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIXmCsEZ6dFV"
      },
      "source": [
        " class MyResNetArgs:\n",
        "   \"\"\"\n",
        "    Passing the hyperparameters to the model\n",
        "   \"\"\"\n",
        "   def __init__(self, arch='resnet20' ,epochs=200, start_epoch=0, batch_size=128, lr=0.1, momentum=0.9, weight_decay=1e-4, print_freq=55,\n",
        "                 evaluate=0, pretrained=0, half=0, save_dir='save_temp', save_every=10):\n",
        "        self.save_every = save_every #Saves checkpoints at every specified number of epochs\n",
        "        self.save_dir = save_dir #The directory used to save the trained models\n",
        "        self.half = half #use half-precision(16-bit)\n",
        "        self.evaluate = evaluate #evaluate model on the validation set\n",
        "        self.pretrained = pretrained #evaluate the pretrained model on the validation set\n",
        "        self.print_freq = print_freq #print frequency \n",
        "        self.weight_decay = weight_decay\n",
        "        self.momentum = momentum \n",
        "        self.lr = lr #Learning rate\n",
        "        self.batch_size = batch_size \n",
        "        self.start_epoch = start_epoch\n",
        "        self.epochs = epochs\n",
        "        self.arch = arch #ResNet model\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpNzFe-13pWj"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djW0RO80_prY",
        "outputId": "6bcff284-94ae-42f8-a62d-38be7fe80357",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from torchsummary import summary\n",
        "args=MyResNetArgs('resnet20',pretrained=0)\n",
        "#model = resnet.__dict__[args.arch]()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
        "model =globals()[\"resnet20\"]()\n",
        "summary(model.to(device), (3,32,32))\n",
        "best_prec1 = 0"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 32, 32]             432\n",
            "       BatchNorm2d-2           [-1, 16, 32, 32]              32\n",
            "            Conv2d-3           [-1, 16, 32, 32]           2,304\n",
            "       BatchNorm2d-4           [-1, 16, 32, 32]              32\n",
            "            Conv2d-5           [-1, 16, 32, 32]           2,304\n",
            "       BatchNorm2d-6           [-1, 16, 32, 32]              32\n",
            "        BasicBlock-7           [-1, 16, 32, 32]               0\n",
            "            Conv2d-8           [-1, 16, 32, 32]           2,304\n",
            "       BatchNorm2d-9           [-1, 16, 32, 32]              32\n",
            "           Conv2d-10           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-11           [-1, 16, 32, 32]              32\n",
            "       BasicBlock-12           [-1, 16, 32, 32]               0\n",
            "           Conv2d-13           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-14           [-1, 16, 32, 32]              32\n",
            "           Conv2d-15           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-16           [-1, 16, 32, 32]              32\n",
            "       BasicBlock-17           [-1, 16, 32, 32]               0\n",
            "           Conv2d-18           [-1, 32, 16, 16]           4,608\n",
            "      BatchNorm2d-19           [-1, 32, 16, 16]              64\n",
            "           Conv2d-20           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-21           [-1, 32, 16, 16]              64\n",
            "      LambdaLayer-22           [-1, 32, 16, 16]               0\n",
            "       BasicBlock-23           [-1, 32, 16, 16]               0\n",
            "           Conv2d-24           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-25           [-1, 32, 16, 16]              64\n",
            "           Conv2d-26           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-27           [-1, 32, 16, 16]              64\n",
            "       BasicBlock-28           [-1, 32, 16, 16]               0\n",
            "           Conv2d-29           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-30           [-1, 32, 16, 16]              64\n",
            "           Conv2d-31           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-32           [-1, 32, 16, 16]              64\n",
            "       BasicBlock-33           [-1, 32, 16, 16]               0\n",
            "           Conv2d-34             [-1, 64, 8, 8]          18,432\n",
            "      BatchNorm2d-35             [-1, 64, 8, 8]             128\n",
            "           Conv2d-36             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-37             [-1, 64, 8, 8]             128\n",
            "      LambdaLayer-38             [-1, 64, 8, 8]               0\n",
            "       BasicBlock-39             [-1, 64, 8, 8]               0\n",
            "           Conv2d-40             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-41             [-1, 64, 8, 8]             128\n",
            "           Conv2d-42             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-43             [-1, 64, 8, 8]             128\n",
            "       BasicBlock-44             [-1, 64, 8, 8]               0\n",
            "           Conv2d-45             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-46             [-1, 64, 8, 8]             128\n",
            "           Conv2d-47             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-48             [-1, 64, 8, 8]             128\n",
            "       BasicBlock-49             [-1, 64, 8, 8]               0\n",
            "           Linear-50                   [-1, 10]             650\n",
            "================================================================\n",
            "Total params: 269,722\n",
            "Trainable params: 269,722\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 3.63\n",
            "Params size (MB): 1.03\n",
            "Estimated Total Size (MB): 4.67\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNj2JHKP1vsz",
        "outputId": "0c30416f-f18d-40f3-db20-434e4e0886c5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): LambdaLayer()\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): LambdaLayer()\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (linear): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAb3r6OQ43ve"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Training and Validation \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTjp-tkWtmmI"
      },
      "source": [
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "    \"\"\"\n",
        "        Run one train epoch\n",
        "    \"\"\"\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (input, target) in enumerate(train_loader):\n",
        "\n",
        "        # measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        target = target.cuda()\n",
        "        input_var = input.cuda()\n",
        "        target_var = target\n",
        "        if args.half:\n",
        "            input_var = input_var.half()\n",
        "\n",
        "        # compute output\n",
        "        output = model(input_var)\n",
        "        loss = criterion(output, target_var)\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        output = output.float()\n",
        "        loss = loss.float()\n",
        "        # measure accuracy and record loss\n",
        "        prec1 = accuracy(output.data, target)[0]\n",
        "        losses.update(loss.item(), input.size(0))\n",
        "        top1.update(prec1.item(), input.size(0))\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % args.print_freq == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
        "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
        "                      data_time=data_time, loss=losses, top1=top1))\n",
        "\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B57y5hgMtzDe"
      },
      "source": [
        "def validate(val_loader, model, criterion):\n",
        "    \"\"\"\n",
        "    Run evaluation\n",
        "    \"\"\"\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    with torch.no_grad():\n",
        "        for i, (input, target) in enumerate(val_loader):\n",
        "            target = target.cuda()\n",
        "            input_var = input.cuda()\n",
        "            target_var = target.cuda()\n",
        "\n",
        "            if args.half:\n",
        "                input_var = input_var.half()\n",
        "\n",
        "            # compute output\n",
        "            output = model(input_var)\n",
        "            loss = criterion(output, target_var)\n",
        "\n",
        "            output = output.float()\n",
        "            loss = loss.float()\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            prec1 = accuracy(output.data, target)[0]\n",
        "            losses.update(loss.item(), input.size(0))\n",
        "            top1.update(prec1.item(), input.size(0))\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "\n",
        "    print('Test\\t  Prec@1: {top1.avg:.3f} (Err: {error:.3f} )\\n'\n",
        "          .format(top1=top1,error=100-top1.avg))\n",
        "\n",
        "    return top1.avg\n",
        "\n",
        "def save_checkpoint(state, filename='checkpoint.th'):\n",
        "    \"\"\"\n",
        "    Save the training model\n",
        "    \"\"\"\n",
        "    torch.save(state, filename)\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKt6Q9Zlt8MU"
      },
      "source": [
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwM5UxsVXdRd"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Dataset \n",
        "\n",
        "Now that we defined our ResNet model, we need to download and prepare [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset to start the experiment. This is impressively easy with ```\n",
        "torchvision```. We use ```Dataloader``` to download the train and the validation set. We show random samples in a batch of training images for a better understanding of data. Note that CIFAR-10 images are quite small in size $32 \\times 32$. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5R-ck_wWQDD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "1f8dc34c-e95a-4b85-a396-c39ef1e746cc"
      },
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.CIFAR10(root='./data', train=True, transform=transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomCrop(32, 4),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ]), download=True),\n",
        "        batch_size=args.batch_size, shuffle=True,\n",
        "        num_workers=4, pin_memory=True)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "        datasets.CIFAR10(root='./data', train=False, transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ])),\n",
        "        batch_size=128, shuffle=False,\n",
        "        num_workers=4, pin_memory=True)\n",
        "\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# functions to show an image\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "plt.figure(figsize=(20,10)) \n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images[0:8,:,:]))\n",
        "# print labels\n",
        "print(' '.join('%15s' % classes[labels[j]] for j in range(8)))\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           deer           plane           horse            frog            deer            bird             car           horse\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAACPwAAAFmCAYAAADz4hazAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzde3xcd33n//eMZjSajEeSZVmRL4rt2HEcx4ntOAm5QEISQrkvC6SU0gKlLYVS2tLtr79uKQu03Xa3P7bbQlta2vIDyrZQwj0EQhISQm7ESRw7juPIF1mWJSuyrNt4PBppNLN/JPltf30074+C7EhjXs/How/TvOd8z5lzvud7O8fjRK1WEwAAAAAAAAAAAAAAAID6kJzvAwAAAAAAAAAAAAAAAAAwe7zwAwAAAAAAAAAAAAAAANQRXvgBAAAAAAAAAAAAAAAA6ggv/AAAAAAAAAAAAAAAAAB1hBd+AAAAAAAAAAAAAAAAgDrCCz8AAAAAAAAAAAAAAABAHeGFHwAAAAAAAAAAAAAAAKCO8MIPAAAAAAAAAAAAAAAAUEd44QcAAAAAAAAAAAAAAACoI7zwAwAAAAAAAAAAAAAAANQRXvgBAAAAAAAAAAAAAAAA6ggv/AAAAAAAAAAAAAAAAAB1hBd+AAAAAAAAAAAAAAAAgDrCCz8AAAAAAAAAAAAAAABAHZnXF34SicTKRCLxmUQiMZBIJMqJROJQIpH480QisXg+jwsAAAAAAAAAAAAAAABYqBK1Wm1+dpxIrJV0v6QOSd+QtFfS5ZKuk/SUpKtrtdrxH7PsHknNkg6dkoMFAAAAAAAAAAAAAAAATq3VkiZqtdqaF7ph6tQfy6z9tZ552efXa7XaJ5/7j4lE4s8kfVDSf5X03h+z7OZUKtW2dOnStrkfJgAAAAAAAAAAAAAAAHBqHTt2TJVK5cfadl5+4efZX/fZr2d+gWdtrVar/qssL+mopISkjlqtVvwxyn9k2bJll7znPe85RUcMAAAAAAAAAAAAAAAAnDqf/vSndfTo0Udrtdq2F7pt8nQc0Cxc9+yf3/vXL/tIUq1WK0i6T9JZkq54sQ8MAAAAAAAAAAAAAAAAWMjm65/0Ov/ZP7ufJ98n6ZWS1ku68/kKSSQSjzxPtOHHPzQAAAAAAAAAAAAAAABg4ZqvX/hpefbP8efJn/vvrS/CsQAAAAAAAAAAAAAAAAB1Y75+4eeUeL5/w+zZX/655EU+HAAAAAAAAAAAAAAAAOC0m69f+HnuF3xanid/7r+PvQjHAgAAAAAAAAAAAAAAANSN+Xrh56ln/1z/PPl5z/7Z/SIcCwAAAAAAAAAAAAAAAFA35uuFn7ue/fOViUTi/3cMiUQiL+lqSSclPfhiHxgAAAAAAAAAAAAAAACwkM3LCz+1Wu2ApO9JWi3p/f8m/piknKR/rNVqxRf50AAAAAAAAAAAAAAAAIAFLTWP+/5VSfdL+kQikbhB0pOSXiLpOj3zT3l9aB6PDQAAAAAAAAAAAAAAAFiQ5uuf9HruV34ulfRZPfOiz3+StFbSX0i6olarHZ+vYwMAAAAAAAAAAAAAAAAWqvn8hR/VarU+Sb8wn8cAAAAAAAAAAAAAAAAA1JN5+4UfAAAAAAAAAAAAAAAAAC8cL/wAAAAAAAAAAAAAAAAAdYQXfgAAAAAAAAAAAAAAAIA6kprvA1io/v7JH9l8IHcgLGP5hRt9GX0Fv31Hh80zOb//3v5um88U/P6XtLf7HSg+xt5dgzafeHTY7yBd8vnIkM/7Kj5vncUt0Nnk85zfx/pLr7D5x3/pGzYf7Gm2+Zdvud/mSzrX2FyS2hf587z74e/Y/OHhHTa/5G15fwBtvp7kmzN+e0nfvvFz4Wfm4iMf+cictm/beJPNS+XJsIwndj5u80LR39PTlarNqzMzNk81NNi84jdXJt3oPyDpnDWrbD41NWXzXNbXtaSyNh+ZKNr8+LjPS+XgJEiqyl+HVCZt81zON/7VGV9+LP4OcxYdYvA6cnG0z+YPfPfbNp840hscwHSQS9KTs/jM85trm/LRj350Ttt/4otfDj/z2//p123etWK5zRtn/IUc7Dti87HREzZXJcglKZGw8bqtF9o8mfHfYWi43+ZTZV/Zs1nfJpV8kyNJKhf8WCYZDHXSaX+Mubzvg4tjT9v8ZMF/iUQy7uNrpaDRqPm+IRacpHynzwu+Lr8Y5tqmfOxjHztFR/Lj63rPSpuv6Fxn86j3Ghx/wuYnd/h6tDQ5HuxB+pUbL7D5j/p8H39fsWzzvvufsvnmt22x+fCI7z/bcvH9mGoIOunpoE0JJrHjKX8/rly/1eYDt/g5sCTt/ObDNv/Vz33U5q956c/bPJ/zdSUTtDnFop9f5tp93yFJi1N+TD0k3zbf8dG/DPfhLIQ25WStZvO5jnj9zCg2iy4+HDIHKxXyLU4sOkfR8cUzXGkkyINlr2B2F4tWhPwMe3aLqtF1iGbJ0Xme6zl4MfzJHOcuC6FNAbBwnAlzHwALx0JvU37tz14ZfqYw5EfVK1OLbN6+zq/HtGT8WkF6xM+Bh0r++DpWBOtukhqb/Kg3Ga1VpPxz1vFh/zwgF6xf5rN+ZlCZxeQsGT0TKYz6fZzwa+XTU36GVpzxa1KT4cxEmg4eyaerfvYTLCmpMOZn0sUgV8k/C88vii9UttFfqOlF8T17OvALPwAAAAAAAAAAAAAAAEAd4YUfAAAAAAAAAAAAAAAAoI7wwg8AAAAAAAAAAAAAAABQR3jhBwAAAAAAAAAAAAAAAKgjvPADAAAAAAAAAAAAAAAA1BFe+AEAAAAAAAAAAAAAAADqCC/8AAAAAAAAAAAAAAAAAHUkNd8HsFD95s99zOafuPP/jgspFWycb6zafLTQa/NqsWTzmaLfv9KNNr5k61a/vaRzVnTZfHryIZtPnJiweWLFYptnM6tsfvL2R2yu7imfS9LeEz5v9/H4Or/97nu227yj7Qabt+TyNk83xO/1VWZ8Xcy2+n2c6C767dPrbJ5vO9vmB7oP2rweXLz1Qptns3EZWy5cY/Ncky8k3+zzY0MjNu873GfzqSnfJo2M+PIlqVj0dWmy6u/ZJU2+XcsF++9o9PfLUHQ7NcQXcqTkv2Nhxuf5nD+I0eA8R1+hWPH51LRvL2ajXCnbvFTydWnfQ/fafOLI/uAIhoJ8WZDXv472oPOSlMn4/islX9cmo/u5cMzmQZOmyWmfS1LXuf57ZjN+HDKtoL4nfV1Wgz8HS5b5L1kc8/eCJPUPj9l8plizeTU4z/ncIpvnFqVtns34Pv74kUF/AKfAz/zqf7b5u37x3Tb/g//i5wX3f/sL/gDO8rEkLVntx7w/CTpa2vwHpvxNf3D/AZuXq+M2X7N8rc1vCOY9ktTR2mnzH37lDptXzj3H5ul1vjKlMn563xV8h5lK3Oakc77R6Fzrx6vFUT9HHT/k24RM1rc55fSkzSWp9YbVNs+3+7lPR6dv1x7efo/Nl7T6vimd8vt/bPsOm0vSxZtnbD4948/jmSDooaMePhRtHw1TgiH3rPYRfcdMkEcLgtG8IfoOcYsSl/HwSV9K90G/brZ+wwabXx6chAEfy4/CZidaefMz3LiunQoNC+AYAAAAfhItzzaHnzme9OuPa5s7bJ5LBjOHGT8mHx8ZtnljKhjRluNnmKVjfh06muFW037mMRUM7MdH/LOx6Ple46L4OqaC03Cy6M9zSn4doBw8l5pK+RloeRaz2GrVH0My5c9DMniClyn6kxS8lqFC8B3SwXsbktSQ9p9JhCWcHvzCDwAAAAAAAAAAAAAAAFBHeOEHAAAAAAAAAAAAAAAAqCO88AMAAAAAAAAAAAAAAADUEV74AQAAAAAAAAAAAAAAAOoIL/wAAAAAAAAAAAAAAAAAdYQXfgAAAAAAAAAAAAAAAIA6wgs/AAAAAAAAAAAAAAAAQB3hhR8AAAAAAAAAAAAAAACgjqTm+wAWqj0P9di8oXgiLGOmccrm6Sn/vtXEWL/Nz1rWbvNF2Q6bn9i71+Y/nL7H5pI0OTTmP9A3HJSQt2ltsGDzk7msL344eKet5GNJUktwm6QrNn56/4DffrP/jrnUjM1Tqtq8eMKXL0mNbcEHUmkbN+RyvvxUi82HB309eWpn/B0WunKpaPNSoRyWceiJnTZf3Oqvw0uuuNzmqy72FeGCtT4vBffT6HjQXkga7PftXjKoi/mMbxOmRiZsXihO+v0HjcZ4MW5U1na12nxqZtrmi9v8dTjum1VNFIJ2tezbzZJvciRJ01VfnwtFX0iy7NvVSiloExobfB70v5Kvh2eCatB3SFJLix/HtLT569CQ8/tYefYSm5eCepDMBmMASc1534fPVH3bXE37G6pY9W3SVNnXxVyrv9/TKZ9LUttFQdtc9N8x1ejPc2tbxuZjBd/37N1+2OazsWzllTa/b9f9Nl+zeG77H3zf+2x+/+1f9QWcPBnuY2zE9x8zQf90Jjg+7MeD01N+HNGUarL5+V0X23z7vXts3tkTX8c1wVis/9GggAZ/v7zx3f/R5j1HD9j8+PiIzS/cuMHmkjTpmz01nu3nqAcG/NyoMeOvY2O20ebHC9H8U7rsRt+mjJbGbb5Y/jqXqv5+Hhr244wNm66yebYwaHNJUtL3P21n+fnZmWA2U30nGinN9W/PzWJIrWgUEIx45Xt4BTVZmmstmc05emzgCZvv7vZrc50X+ftlZbCcE0ydwnMwUovn8Rr232HJUt/2Rtc5Et0Ls/gG8rOC2dVnAFgoDtVqNr/jQ++y+ehBv7b4wV9+c3gMd3z9yzb/g09+w+Zrzz/L5u/62XfY/MoP/LzNs4v9OoIkjd22w+bF/+WfLaWnfS88In8MG6718w5t2Orz9HqfS1Kq0+djwdxjaZfPG1f5PBf00qVg/yv8GrQkKetHO/uDkcjjQVXZ/pD/DlO7vmPz7pJ/NtaxOL6OhUXB+mHvF8Iy5lO2LRqxSstTftA7Oeqvw2iPn6M2pvxo8Ej3UZvncr6u57LB/Sypozm4n6Lnb3m/vtnU6ke0h6eGbN5/wq+35Gc1YvafKQTrHZ3L/DsBXRu22TwZ3CqDI/FaRO+Qr0tDQdteDWbB+bS/julGv2hVDCbJ45l4dpSp+kLWhSWcHvzCDwAAAAAAAAAAAAAAAFBHeOEHAAAAAAAAAAAAAAAAqCO88AMAAAAAAAAAAAAAAADUEV74AQAAAAAAAAAAAAAAAOoIL/wAAAAAAAAAAAAAAAAAdYQXfgAAAAAAAAAAAAAAAIA6wgs/AAAAAAAAAAAAAAAAQB1JzfcBLFSdrRmbr23rCssYro7avLB/zBdQabNxS+N6v/l01ebJc/13uOntr7W5JB3p9d9xz0OHbN53zwG/g7v7fZ6Y8nltxuez0brWH0J+mc1TQ9M2X96+3OZrz22wefvj/joPThVtLkkNmaCuta+yeWvG3y+TQyM2P1LYY/OTPQ/avB6sWpaz+a6dfWEZ2++/x+ZJ+bqw6+HtNs8t8sdYKpVsvmbtOpu3tbbYXJKKwT62bN5s81TSv8dazvhub7FNpXM2+vZgtOCPX5IOdHfbvFws27yloWLztmXtNt87Pmzz5uB+XtLhy5ekR3c+ZPPxEd8uTY37c5Cs+FxT/jtKQd+huN2sd6nG+DvmmrM2T2d9rkbfJqnq79dKshBsfsKXL2lqKhgLJf39lAyGEcdH/DjopO/+1Jj1dXGq5I9PksoFfx7SwWi/7E+zKml/Dosj8TE6H/qvfxl+5o9+7/1z2ocfiUnpIG9v7bB5a6cfk48dfizYgzQzOBl+5kxXbPB1Ld/hx6upoE0pVfyYWu2+/H33nPTbS+q5yt/0i16xyOaLz/bzirGRcZtXK/4cDo/446um/RhAksplP9Y50t1r8+y0b5QKhQmbT1V8u3ns6KDNJalpme+/Jkr+PEVtRsuyVptPBeOg5CK/h2Q2vk7Lmzpt3qigDz8D+KsQ9w1R7xYtpkV/u242vWf0mWCkFX7H6DvENW1u+5ek3Y/7tYBrf+pNNt8k37Y3BvuPvmM08ykm4rOUXepLCXqnOYvu9ugcSVI8ywWA+hGt/WU7N9r8Fz/wAV9AZzzOKvzzF2x+3dUX2Px3/vh3bX7LPTtsnlrs5z4zB/z2kvSZT3/W5r+xdoPNn9zj18K/tcevn/7n5q0210Uv9flM3ueStDi4luVoxBfk0fxrUZC3+rV8pWcx4gzWfDoSfqRybrBE27/az60erl5p8/bdPTYvNgSLWpKK+ebwMwtZ70T83Kg14+tCQf46jPT7efTgvUM2z0/40WIh55/zlrujtXzpSPDcJ5f28+iODSts3rnOP6fNrfJtWjHvj+/IuD+HkjQ+7d8ZqCR9uzXaHLQZjdEzEa/SEreblayf+xTK0bX2bU65ErRr7X6W2zbt10qS0SRbUiUdtO3xpT4t+IUfAAAAAAAAAAAAAAAAoI7wwg8AAAAAAAAAAAAAAABQR3jhBwAAAAAAAAAAAAAAAKgjvPADAAAAAAAAAAAAAAAA1BFe+AEAAAAAAAAAAAAAAADqCC/8AAAAAAAAAAAAAAAAAHWEF34AAAAAAAAAAAAAAACAOpKa7wNYqDqqjTZf2b45LOORnV+3+dhI0RcwWLDx0af32vysrmU2b1mWt/n1L7nO5pK08iXrbZ756Q6bf+wPPmnz72z/kD+AWsXniUSwvY8laeuFN9n8w7/3hzYv9fvrvGVZzuaL222sVSvabJ4L6rIkDY+N27yz1e/j99/xQZsfKD1i892PP2BzqTnIF75SacbmyVm8fnn1S1/qP1D190OlMm3z3kO9Nj/S12fzXM7X5cGBfptLUvGEv1/O6eqyeVtrq81z+eAYh4ZtXq3aWBdv9u3ubArZvcPfL5Olks0v2LjB5qmUr2xP7t5u87Zc3KZce5XvGx7c7vuvwpBvk048PWTzREunzWvjYzaXJoK8/g0OdYef6e721+FI3ucnR4IdpH3cEFS1dCYoX3EfGtwOSqd9m7Skwx9kY3bKl5/x93OuOb7fGpadZfN83p+oZNK3i6r4+2mw7Df/5p1fs/nrr7/cFzALPcf8efz2LbfZ/Bd/4Y0237Jxnc2vv/anbD5e3mpzScqf7fvgVKPvn84EyzedZ/OW5rNtfqTbj1MKwdRrw1XbbL58/IgvQNLxoI8//6orbJ6r+O0LRd8/dXT6udd0MPsfLwU3tKRq8JlSv2/8V7b5hrlzjb/fpuXHs6uvWG1zScq0+XaxLevHk9EiSq7Ft6u5lG/bCyf8dR4cjtuDXZmdNu/MrwjLqHd+NUU6etKfx8KEvw4N8vXIz/6kdC4bfEJakvfz8GgoFN3R0RTUt6pSIbgfw8GepKmqn+uvUYPNo/vxeJAHoyD5VbP4GkjScgUD0kC0bBWseoX8GX5G9D2DafIZ4X8M+ytRjipjcMMlo5Mc3E6VE8H2kqqT8WecaGaSDZqElmD7ruCGy8dNSniM1eA6jPtphYJY6aD8ptnccIFisA8/A5VSwXWqRgXM8a+PR+tqklQJ9rHzLz46p2OIvsIlr3yL/0Bn0IdPxmtKjx30c4tX3OSfR9x6+49s/pXtfs3nZ+Xn+f/9lnttLkm39Pk1oV/a5McxS1b4G6L4oF+fVDG4IzuDRiUzi8rcFowUGqIxdVBXVgWbNwV52IvPvdHJH/Hr1Mr6L9Ge9+Ogl63yo/bdzX4OPXTQr6VI0tTYLDrJBex4PG3Q8ZKfh2ebfOO7fGOwHlL221f3+7X+zqV+++RM3DmcLPi60hjcb029fv43U/AzuPb1fq2iK+/b1Y5O3yZK0mDFH+OTE76+F4IZ4EjxoM0bgsuQTMZtSrXqB8XtVd8mRGO5yowfqAxPB2tWk/4ZaiZYr5Gk6ZlTMKA7DfiFHwAAAAAAAAAAAAAAAKCO8MIPAAAAAAAAAAAAAAAAUEd44QcAAAAAAAAAAAAAAACoI7zwAwAAAAAAAAAAAAAAANQRXvgBAAAAAAAAAAAAAAAA6ggv/AAAAAAAAAAAAAAAAAB1hBd+AAAAAAAAAAAAAAAAgDqSmu8DWLAmqjbu7NgcFtGqHTYfG3nEF3Ci7PN0xca5Rf7yZjONNv/h9of8/iW96rJ2m5+nVTb/uTf9ks2/85G/CI5gwMfNOZ83rgjKl3q699r8njvvtPmVGzbZfHykZPPOZf4cp8onbH5gzwGbS1KpOm3zi6+71uYd0zM2HxzzdbHU57fXiK+r9eCbX/u6za++6qqwjE3r1vgPzPg2oVD0dSXS3u7rYjabtXlLa0u4j8ZMxuaZIG8KjqFhxrft6Uza5gNPD9k8uWmZzSXpkm2+3ZkuF22+b/9+mx/u77d5dI6Wdvi8MT1uc0m6fNtlNs9mfNs81H+P30Gt28bJ1BU2X37xBpv37Wr2+5ck7ZrFZxaufD4ffqbZ3/IKmgSlViVs3tLaavNc3r+Xnsv4uipJ1Yr/nkm12XzTJj/eS2X8WO1I/x6bl8pHbT5Vie+3Gfl2LZPy7Wap4M/j5JjPP/EXfqz22pdvtfmp8IXPf97mP3robps/tf9+m69c4dvtfd1+zH/x5dtsLklqGrNxy9l+vKhCvIuFrmVV0KhM+vutMTVh80KTb9vbh/05fsVVq20uSd9+4qDND2SC+VnWf8ecb7LUmg7a1bQfiw08Meh3IGntVj8eHdzvxyEtRd+2t1/UZfPqtL9OF111oc0lqTjm297z1/m2Py0/d1qc9heqodVf50d2PmDzVClexin2+0ah0BH3L/XuW5/7uM0f2+HbbgX9az7n77dMtsnmy9f4ui5J5eAYBsdHbN7Q4o9x9TK/XjM57PsmZRtsvHS9H3NLUrHg56h+hiv5UY702DHfZvR1+3W5jVf5PrwjEY9Hg1Up+dmd5K+itCTI/YhcClYeJUl+hvqTIeVvR01Pza38hqCyzwR/ZTc5i/1H90vON1vheHNm2OcNwTlMBcPdYHNJ0mTwoWR0EgLVoAueCZqEaAqbnM2TmpqPG4Il1mR0IoMl2KRv+sO/Xl6ZRaMzPZuLPQfRV2hZfu6cyv/c7344/Mx/ePd7bX7Hdj9O+dBf/IPNX/X6nw2OwPdOT+ztCbaXHhvx45DDWV8ZNm3baPNtwTMZbTzb5yuCtb2mYHIlKazQHUGjUgpGAVG7uwAkVvjxYtv+H9g8X/ZrtGrz6y3Ng0/bvNDuj0+SCuHU58thGfOpIxOslUhKBp1srtmvj37kp//e5jO7HrT59z71P2xeLftBQnu0CC2pUPQDhemgf8kHC9nplO+kkyf8OW4MxkGrwlmB1JL1A4G2Vv8d+op+PaZa8YO5ZDDQSc/4Z2eSVBr3A5H2Rt/2XrDa39OjJd/37Cn22nxf0l+o0VQ8qM5FA4l5wi/8AAAAAAAAAAAAAAAAAHWEF34AAAAAAAAAAAAAAACAOsILPwAAAAAAAAAAAAAAAEAd4YUfAAAAAAAAAAAAAAAAoI7wwg8AAAAAAAAAAAAAAABQR3jhBwAAAAAAAAAAAAAAAKgjvPADAAAAAAAAAAAAAAAA1JHUfB/AQlXOpG3eUW4Py7io7TKbH8p2+wIayj6v+nhsbMTmjS1tNt+9c5/fgaRLLh22eaF0r807cxtsnljq89qxAZtr/ITPO30sSWPdD9n8z//YX8dbN6y3+Wsu22Lz/3b5h2y+eFGDzUsjR20uSavWr7N5Ljlt893799h88bn+fqlMTdlcfb6e1YMDBw7Y/IINvq5LUqP8eepasdzmpZKvK6vXrLF5R8fZNq9WZ2yeSvt2VZIOH+q1+UB/v81b8nmbZzNZv31Li81L/lbQrp1P+w9ISqb9dUhmMzYfGvX3QyXoHDrO7rB5Z6evR5mGcZtL0uE+f52yOX+dCmOFYA81m84c77F5+tzofgsu9Blg1equ8DObNi+2eb7F30/Viq+LjZlGm2fk24zkjN+/JHUu32jzm256r82v2XRduA9n//GDNn/8ibtt3tPzeLiP4eEhm1eD86hWPxj6yKf+m83jqzB3wYhYR/p83zE47Mcp1fRemz/2eMmXP+LbrL5b/RhAkhqyvm2/4XX+Oi31XUddOD44aPOWjJ+6tuV8XR8Pxikt+33/9rKbNttckr7yw0M2Hxv184bcVj8/G+o9bvOgWdXgXv8dj/eN+gIk5ZvHbD40eMTmM42+zRrN+mPMd/ixWjaoB5KUiuqC/BwyL98mXHnWlcH23gVXrrL5o4fjNqWz8TybL+/0+3hE3wr3sdA9+dDdNh/o8X10Y9b3cJV8zua51mabj2cnbS5J+3r327z3qG83c+1+Hj7W4cf9rZlWm196w/U2b2mLaru0NO/7v+6CHwWszvsO8I7v3Wrzv/zwb9r8nR/0Y8WPfcCv10iSv0rSk0F+QbgHL/qbnr5Fe0awYvMTsbhcDU5U0LWEa7jVF+Gv5DYGXWRj8B1KQWVuOOrXAXJJf7+3l4vB9vE8PdXg295qya+HjFeCdqvRl1/0XYMUjNmD4e4zEkEe1DUF17kabR/tPpi8JWdR/iwu9ZxEc9jv7vHzw699/Js2/8ebPxEew6f/6u9svm/Yj4mv9UNivep114TH4Ez3+edKkrQ46xuuseCZiK690cb/8dJg+xP+mY2aooZ1NqsZwRro4mB9MevbtYXAr7BK1YQfTy7u9Ndx5Y7bbH6wtM3mI23n2ry4d8LmkjTU+GKsXJ0+a+XH5JI0UfYDlW3b/HWK/CCYW02W/HVoDEaLBw4Gz8oltaSDBY+sX0+ZCgZbhXE/TqkO+XFOYdS3m9mZaJAg5Zf573BF+yabX9Li5/kjvX02nyr69Z7ePr+eI0kDI/48rN7qj3HNtB9rZZ/27fJZlWU2z6f8/XQkEz2XkvrKvi7MF37hBwAAAAAAAAAAAAAAAKgjvPADAAAAAAAAAAAAAAAA1BFe+AEAAAAAAAAAAAAAAADqCC/8AAAAAAAAAAAAAAAAAHWEF34AAAAAAAAAAAAAAACAOsILPwAAAAAAAAAAAAAAAEAd4YUfAAAAAAAAAAAAAAAAoI6k5vsAFqpKu38XanJwJCxjRWqZzZe1rbL50f7RYA9lm06X/DH27x3y+c7dwf6l0lTJ5sVDfh/vuvwXbH7pJf4cbb/NxrGxYvyZfIBVNggAACAASURBVFdQhj8H3d1P2/zl2zI2bwzu0o7FeZuvOXeFL0DS1ddeZfO7vnurzc/yX0Ed7attfmLY1xMNj/u8Dpyzytfl48PDYRnJyqTNO9rbbT415duMXG6RzSuV6SCv2Hx0JGrTpIGBfpt3dvp2tVyesnmyWrV5sejv50LRtxmP7njI5pJULBVs3rmiw+Zr1q7z5Rd9+f1HB22eqfp6kpz250iShkb7bL5q/VabN7YuDvYQNIwpfw6Tjdmg/FyQ17/qjL8XJGnaVwWp0hjsxO+jOu3HWj17/f125daX+/1L+pOPftLmixJRXfBqQb5uybk+v8bnuuaFHc+Zau8+Xxn3PLHX5vlmXxc7u3z/1hm0CWvW+T7+a7fcbXNJamvxfbhmzvx2afDB/TafWeHHtPmMP0crW1tsXq302LxYWWtzSbr48ktsfmCnr6sdeT+WygR/Xaep6MdBLWl/L9xwY/wdX7Fpjc2/3+jHCY+X/P2cD+YVK9O+3X7s/h2+AEnnrfdjqYN9B2z+yDkPBHvwda0oP+5vlx/HjJcngv1LP/rBv9h8zeYNYRn17m2//E6b33Xn920+EszPLt680ebFsh/HHOg9aHNJKsvfT51dfswczW0GerptvvaqG23escy3y1uWbra5JG16tW9T/vY7N9u8cMUrbP7w7h/Z/Nf+8CM2f/fb32jz2SyqPhnkwwXfLk4FDWO0OhmN+qMh/2zKCJruM0KwHBL/jVrfxavhhRzMv7f/WfyV3ug6TQXLUpXDfp7f1eDXc1Zn/NrfOeWjNl80m6lbUzCmbvbtYte0X2MdnvLl7yv6Pjw15q90ahbXMdXk80za56WgLgZLe4qWEpJBHiwTSJIag+9wuvUN+br+jzf/1Zz38Z73/7LNzw6WW771/kttXm33a7ySP8kNwRqzJL28q83mV77x3UEJfnstjsYRK4O8Nchn06jE6/VW0xzn8ZNB59M095slEeRR/3QsuKlPrrvB5tM7/TlOZoN16FR8HYtTdf67F+WgHkgKpg26dNPL53QIl2zeZPOePr+ekzvb34+P3hU/6K2U/LOlplxQF4L5WXXEj6qHj/bavLK00+ZDQbsrSS2l4JmGfF3I5ZptXhnxz6qrZf8cNleN62IyOM/Fgn82VShG/ZefvUTPttaU/Azuyovixfh/GnnE5qXjYRGnRZ23dAAAAAAAAAAAAAAAAMBPFl74AQAAAAAAAAAAAAAAAOoIL/wAAAAAAAAAAAAAAAAAdYQXfgAAAAAAAAAAAAAAAIA6wgs/AAAAAAAAAAAAAAAAQB3hhR8AAAAAAAAAAAAAAACgjpySF34SicRbEonEJxOJxA8TicREIpGoJRKJLwTbXJVIJG5NJBIjiUSilEgkdiUSid9MJBINp+KYAAAAAAAAAAAAAAAAgDNR6hSV8/uSNks6IemIpA3uw4lE4j9I+oqkSUlfkjQi6fWS/qekqyXddIqOCwAAAAAAAAAAAAAAADijnKoXfj6oZ1702S/pWkl3Pd8HE4lEs6S/kzQj6eW1Wu3hZ//7hyV9X9JbEonEz9RqtS+eomP7sZz3mg6b77u1LywjN+R/QGlN+VybHx3a7XfQVg2OYDLIZ3zckAm2l57Y0euPYL8/TweXHrD5y6/abPPtt9k4VhmJP5Pt8nlyhY1XdK2x+eK8r2uq+Hiq4j9QbYzqifTdu+60+a5Hdtn8d97xFps/ttdfZ+3p97nKQb7wFYpFm/cNHAnLGOwp2Twd3NMXXHSRL//oUZsPDQ3ZfHxszOYd7e02l6Qtm/0xnrNqlc2LE/48TxX9OczlcjZfXPXt+vkb1ttcko70+3axPD1l86nKtM1/eO/9vvyyv59y6bzNr7nqpTaXpE3nbrX5kaO+Lg0MjwZ78NdJLW02Xtnl69H+xfuD/UuKDnGBSzbEP/KYzmZtnsr5PFn1P9p4cswfw467D9n8zdf5eiZJixL+GCO1IJ8K+uhU8LuVDYkXdDg/sR7fucPmg8M9Nr/0whabZ/O+76hW0zbPtfi6/LIb4r4hm15s83xzMC4P6mI9OH77MZuPbSjYfEvQP21Z76/D0MP7bP67/+WfbC5J2/3USJt/Za3Nl2/zdTUz5vvPzrTv3x699z6bX5f320vS286+wub5Kd9HT034cdCuex61+SXb/Nxpy6Jg7ibpB7f4uU3bar+Pv7n5z21eLvnGP5X0fVMm7evB4N5o7iTd8ZXv2nzt1k02f9vqN4f7WOi2bfF/j+wHO/fafGjYz22uf8MbbN7Xd8jmlZy/zpJU6vP3y6H+AZvv691p8wtW+XYxv8KvdTS2tNo8XlGSMmq2+fUv3WLz3/rYh23e0ubbtd94+y/ZfIX83Ov7g4/bXJJSnX6Ouynvz1S0ohMNAaLVlNkMIeJVpTNfea5jLV+VwnPsR6Oz/Al/vxyisT7/gcygbzebsr7Nqk77efbxBj/JPjl9wuaStCTr76eGDf5+1DLfZrSnx23eVPbzisExP8aYLPs2UZJmgsY1ehoQTG1iweOEdFB+chZPo4LLeNpt2uDH7OvW+vHw/gPx84Z0wl/rVMb38T39fm70ynb79/E1YVNpd68fB0nSK67xz20K8mtzfhQxG9fOuYRYUOFPtya/Rhz3DqffdHC/jgz6debB9UE96fb3Uzblt5ekQp0PZIqloAOXpJKvC606e07H0Bo8M+kI1to7X+bXcNdfFq/x/svH/8x/oOTHEdXgfpkqDPvtS75dnD7R6MuvxBXxZDByL03461At+XFKecx/x8qU/47JZNxBZ6t+LFQ46hfOCot9m5FKB88jglFxbtD3n+W9wcKepEyHP4ZZ3LGnxSn5J71qtdpdtVptX61Wi56LSNJbJC2V9MXnXvZ5toxJPfNLQZL0vlNxXAAAAAAAAAAAAAAAAMCZ5pS88PMCXf/sn//eXzm7R9JJSVclEol5fpcbAAAAAAAAAAAAAAAAWHhO1T/p9UKc/+yf3f82qNVqlUQi0SPpQknnSnrSFZRIJB55nsj/ZiEAAAAAAAAAAAAAAABQp+bjF36e+4fKn+8fk3vuv8/9n/EEAAAAAAAAAAAAAAAAzjDz8Qs/p0ytVtv27/33Z3/555IX+XAAAAAAAAAAAAAAAACA024+fuHnuV/waXme/Ln/PvYiHAsAAAAAAAAAAAAAAABQV+bjhZ+nnv1z/b8NEolEStIaSRVJB1/MgwIAAAAAAAAAAAAAAADqwXz8k17fl/R2Sa+S9M//JrtG0lmS7qnVauUX+8D+tUJLv807N7aGZVRL/itcsu7f/RfJ/j979g7ZfGzocZunt2ZsnsnZWCeG4h9Zmhw+6j9QKtq496j/ju9/3Rtt/v8klvj91477PFXxuSSlhn2ea7PxmmW+rqxqa7d59BV7+0Zsvm/HHl+ApJbgGN76utfaPN/iz8GX/vZPgyMoBfl8vJt4aj25v9vm1770qrCMlopvsvfu3O0LqKZtnGwMDqChauPl7WfbvL3t+X7Y7f9Ykm+2eSZoUzJZf46qzb6u9/f78o8d9u1BdcpvL0mvedV1Nt/+qG/bD/f02nxl5xqbt7d32HzN6gttnsvnbS5J1WRwT4/4dqtY9fn/+bHA53H8kI0P7Nnhtx/1fdOZIJlqCD9TTvlxzFRq1OaNVX/P9/cGAxH5Rql0Ih6n9D/2hM1XrN/gCwjOUyZqN+foRFDVJel4MF5bucqPQxqi71AL8kSQB6Zn8Zm93T+yea7dn4Nsm2+3iuWszatV3/+lqv5CLe+K+7/jw8F4MxnPPerdWV0+T/rLoErJf+CHX/mOzV++zNeDb+3z+5+NzvPOs/mh4oDfft0qmzcW/XfIBFOfI/cHcztJg13+M8tzvq5uCcaTF2/0+1+b8WO589/q54+S9PDjH7L5wEE/Dlm5fpnNdz2+0+aLW/11rBT9/nse8eVL0qplvnFvy82m9T2zNXb6yjZ60I+5W85aa/OJTn8NVs74ObQkDcjPIXv3+v5n8drLbL7lxhtsPtXm5w0DhQmbr4i7P/k9SBfn/dxkzyPbbZ5O+nb14Vftt/mRpF9P+fKtt9tckn7u9z5p88XB9lNBHnSP4WrKbBaGo334lYYzw9SMz6PzHI1jwu2DD2RmcxH8LaupYb8WrmH/d3MHxh6w+eSU3/7AjP8SDZVo7VA6p8WfqFVDgzZv7wpapS7f9i9acbnff8VXhAOj0d0mDZf8eK/cGDwP8Et3ygbT9Kiu5oLtg8OTJM3iI3MSzLw0XfRrHfsPPGLz1iY/XpWkTM5fx/7jvv966788ZfMnP+TnHclgXrC7N35m8tSn77b53bvfavO1N77J5rv29PjtZ/yVvOWvPm7z2QlumNNu4Y/Zk01+/leZ9m33U0Hb/1TSj5kf2xX/bsTMk8Hi2vlhEfNqbDxeHMxPx+3OXBwd8G3Site8zuZjbUHvMRK3/G0X+flb31232bzdP3bSxIhf60hW/HOfbIMftWcz8ag7FTxTKQz5dnEyeMDXFKzHVE74tYipeJiilpS/p6Px5u4d/nnEpkuvsHkm7etSpeqf+wwe8nNwSSonfR8uLQ3LOB3m4yn6zXpmXPUziUTi0uf+YyKRaJL0R8/+v5+ah+MCAAAAAAAAAAAAAAAAFrxT8gs/iUTijZKe+6t0nc/+eWUikfjss/97uFar/bYk1Wq1iUQi8ct65sWfuxOJxBcljUh6g555l/JmSV86FccFAAAAAAAAAAAAAAAAnGlO1T/ptUXSO//Nfzv32f+TpF5Jv/1cUKvVvp5IJK6V9CFJb5bUJGm/pN+S9IlarRb94wEAAAAAAAAAAAAAAADAT6RT8sJPrVb7qKSPvsBt7pP0mlOxfwAAAAAAAAAAAAAAAOAnRXK+DwAAAAAAAAAAAAAAAADA7PHCDwAAAAAAAAAAAAAAAFBHeOEHAAAAAAAAAAAAAAAAqCOp+T6AheqX3vsum69vuCos4xUbX23zjrYVNv/w+3/f5p+5+bM239N3h82nNWZz5bI+l6SGIE/6Knag/6jNl3eusXnXpS+1ed/2b9hcbS0+l6SKP09Lu/x5uni9/w4ve9kNwf59/GT3fptvWX+xL0DSlS+91ubTwfZf+f43bX5of094DN7UHLeff0taW20+POjvBUnKtTfbPNvabvN8e5fNq6kZm4+NDNm8JZe3eSrjj1+ShkYnbJ5J+htieMJvPx7cT5VyxuZjxUmbFwojfgeS9j35uM0f/dH3bX74YL/NX/eGN9m8pdnXk0zeX8dCuWxzSRoZ9ce4Z+cjNl+cDVqdRKePm/391lCNvsNwkNe/bLYx/Exrq+/fcll/v8wU0jZPV/1758vWbrD5+HivzSXpoYf8/XRh2d/TQ0Vf/kuv3xYeg1Or+fxQT/wdi8Xgfkn665DNNtl8aYcf7CWC2cSIP8X60pdu9R+Q9L3bv27zfLtv3JONBZuXCiWbV6tVmxdL/hokg2sgSeWyr2wDg+M2X9kW7mLBO3/bUpunqr7/qlT9dei+eZ/Nt/zMEpvPiu9CNbhn1OZP9HTb/KIP+vllbsyP1d64daXNL2v08xZJGi/22Txf9m37Oy650Oa5vD+GkT5/rzxw106bS9KaDRttft+dO2x+Rcm3Gecs9fP8J3cO2vzgDn+O1RfPjd753gtsXozm8T8BDvQO2HzlOj8Okfw4abzkr1MpGAdJ0uCw77/a2lfZ/M1v8GsNTVnfiU8GdX333gM2b2v394IkdTT5z0SrUhdf5K/TPZ/8os3fdv0TNn/TW7favNy+zOZSvJ4SDDflRyFzN5vVlqi2/iT8bdLoOqaDC9UYnKRckGeD4WRqFiv8U0EZ5ZIfRwzs9fP4fd132Xy877DN7/PdY3ivSNIrlyZs/q5X+Xv6nMf9OGLj+r02T1/nO9h0h68oG5K+3ZekcsnP3wZKfsxcyp1t81RUV/0ygPL+8Gb1MOp0t3tRXVq1wq85rTvnPJsPDu0Jj2HsuB+HzNXHb7/N5vf96XdsPtPj506S5FeRpcyYPw+/9fZftfmPKj9l8699/EvBEZwJ4rWE0y1a6X44GEh8r88/W3tgt+979u73a/3a4dtlSVIxOMjz4yLmU2UWjWLXinguPxfLXvH24BN+rf0L3/m0zQ/s92MMSVox5tfrp0eC9fxisPY37etJLuN7sLbF/lnzdDV4MPXMUfi4fMLGE5NBD5cKFgKq/hw1Nub89pKSFf89C8O+VSmM+PXP4gV+/rck7+eXPSV/jkang/cmJDWn/HNWfxZPn5+EORkAAAAAAAAAAAAAAABwxuCFHwAAAAAAAAAAAAAAAKCO8MIPAAAAAAAAAAAAAAAAUEd44QcAAAAAAAAAAAAAAACoI7zwAwAAAAAAAAAAAAAAANQRXvgBAAAAAAAAAAAAAAAA6ggv/AAAAAAAAAAAAAAAAAB1JDXfB7BgffOYjUdfUw2L2D3SY/P8+BGbX7bBv4/13z/8IZt/6bsrbH739rttPlkt21ySRkpjNp8pl2y+f/t2m9/61W/a/PU3vM7mf7392zbX2JTPJemijTZ+x5t/2uZLqh02P2dTxuZ/+dnHbV5NF21+8dZrbC5JB4b9te4tFGy+6+iQ30GlEh6D1TiLpmpqjvs4zV5/vb8O56zuCssYfHrY5tlFvq5df2W7zf3dLD3Zc7bN77jlTpu3LMoGe5CywaUujvlzsHy1b/eWrz/P5gP9ozbPtC22+VBpxOaS9LVvf92X0bfP5qlko8337fmRzUdHJmx+7kU32PzKG3wuScXxPps/es/tNu9s8+3ia9/8JptnW5fb/OEHH7O55Nu8M0Gp5PtnSSoVfH1uVKvNU6W8zc9bs9rmF6xvsnmh5OuZJO3a6fvI6DTs6vPnIN2as/mFmzbYfLI0bfPhYtQyS0f6fB88NOa/Q3uH7xuWLD/X5g02lfbsPWrzf/i7vw5KkA717bT55ev9dZicCsYpQd+Tjr5k0o/Zp8rxGCWb9WVUFc896l01ONGdHW02r1R933HZG1fbvFTx84KzFi+yuSRd/a7X2PzuL/+LzdN+mKNNr/Z1qWXQN2qFEd+mrO2K29WU0jZf1ezblNZG3+6p4Ot6U8qPJwvDfg4uSTMlv4/Je2s2v/vBJ23efIXvvybumbT52ZtabF7ujOewP7jbH+OF11zgC/C3U134vb/5lM0/+/nP2/xXfvPdNh8IZk/fvNPPjXbs6bW5JB066vfR0eX76F17um2eCvqvA/v99oWiH2dNTcfz+M5X32TzNcH2v/wef53u+eQXfQHjh2z81c/4/Ny3v9OXLym6Y/0MVOEIwM9Q47/pGa/8nRFNwpz53k9qDE50dA4zwXgzF9xOyegAJaWj9ZZhPxDZ+/hemw/uOmxz36KcGl895vvw6a88avOP/8oWm6fTwTimd4fPM0FFaVnvc0mbk36sVekft3nPAd92Zzr8ulq+w9fmVNAoJWexxHu6V3jDdi/vr1PvgH+mk8n6tRJJ0qS/X0KLN9n46/f7ZyrHD+2Z2/4l6Sw/P/rCP/6uzdfkr/V5zd9Ph6+50OYPHPN1/cqlfh1hYQhq61TwHfwSsiTpaDBQ+UGfX0f+7E7fd/zgXr8eMzkSLMz1By1CeRajlFR9j2RGZ+LPJHPzW5//8tt/Y/MH9z5o89QsfpvkZJtfC8ivX2fz9F6/VnBOm5+HJ5O+HjW2LbN5afxpm0vSymB+l27xY4Due/xzp4lgTWhq2t+P+Va/f0nKpnxd7Gr117F3zD+X6e/2fUNug99/sRw8K6j6XJLyWV8X4it9evALPwAAAAAAAAAAAAAAAEAd4YUfAAAAAAAAAAAAAAAAoI7wwg8AAAAAAAAAAAAAAABQR3jhBwAAAAAAAAAAAAAAAKgjvPADAAAAAAAAAAAAAAAA1BFe+AEAAAAAAAAAAAAAAADqCC/8AAAAAAAAAAAAAAAAAHUkNd8HsGBVzrHx+z/0O2ER1arP/+Dnf9fmT3UftPngSK/Nr978UptfftEVNh8tjthcko4M99m851i/zYf7hm3ee3DA5lfecKPNmzdeY/OJ8pjNJenX3/UBm6cKGZunm1pt/t17Z2z+2X/+R5t3tLfZ/B9u/qrNJSnftdXmG17i60r7ijV+B9Woqcn6uCUXbC/p2JH4M/No+dIWm7flG8MyRov+PA0c8/fsvQdW2LzrXL//84LLPP2aG2ze2eG3l6T1eZ/7u0VqiHdh1c5vtnki2L5PwUmU9Oh97TYf6N5h88ZGfz+MjxRtPjjk290VXb6ezOZV4egjlWLJ5sMTvi53XtBl86Gn/Xc8tPt+my/qvMzmknRi8I7wMwtaNEiRVJ0+6fOSv1868p0233aF76N7evfafKTPj4Mkqa9v0Obliu/DP/Pl223+5dtusfkf/vEf23zNGt+wDo74uixJPf1+LFYq77d5Z4dvnBub/XXu6vJt2oM/vNfm+7oft7kkZVt9m5HP+2PQjN++MlW2eTWVtnkyaPQafTV7tgxfSEPyzJ+2lcb8dzxU8nX94nXrbX6kNGXzO58csvnyi7bYXJIOdPv7bYXvovWOrStt/oregs3H9vtz1Na2yOZruvz9LknJ6rTN87mgfxn3x6hpfz8vyvjyX7JplS9f0oO3+TmqfFUJ84nvT4bH4OQrvqK0dy0PyxiWb/eOZ85+QcdUj/7kY3/kPzDo1xq+0eXPUVX+Ov3TzV+yef93H7W5JCmsSn52cucaP2Y+Kxj3n7z/Ab/7jZtsXBjz/askJYMx6fte+1abv2GTXxPa9s632PyRz91sc1V8/LLXvcl/QJJfCZDGgzxamYvKj0b9wVeUJEUrMmf+KEXKB4sN0YpONEeOxpPR9K0xvt3CypQt+XHGkb09No9mLn7lUfIrIbMTrdm0BsuP66+5yn/ghot9vv0enz/t57ia8eMsSVLOrztta/aVKXMoWKsv+zHxZMavEY8Hc5/ZzI3SwdrgXH3vO/46vOLVfo48LX+dpqeDiibp5a/9WZsfz/g+uqev22/f5+/XRJt/XrHqPb9mc0m6tMWXsSa/MSjBr+l0BC3n2gbf8JX2B2sNSzf4XJLk1wL2HvMr1cUZ38smg/thaypouId8D31v8NxKkp4KOrCBit9Hasr3HZPl4IauBB3gomi1/0SQS5pFu7OQzWacdWDPdv+BG/2zZuV933Lz9z9v81t33mXzZNrfr+0z8VpEMePvx7MuXGvz0rCvq5lWP6puCjqwqaCqJlN+PUaS1r/t7f4D6/z8q2enX4MdCJ63j/lTpK7KMf8BSbmuoA/M+fPYEjwLLpwInn0d3WfzZNLXxcac79skqZg9zQOVHxO/8AMAAAAAAAAAAAAAAADUEV74AQAAAAAAAAAAAAAAAOoIL/wAAAAAAAAAAAAAAAAAdYQXfgAAAAAAAAAAAAAAAIA6wgs/AAAAAAAAAAAAAAAAQB3hhR8AAAAAAAAAAAAAAACgjvDCDwAAAAAAAAAAAAAAAFBHUvN9AAvWqg4bf+P2z4ZF/L//19/bfPmXb7H5e9/0DpuX77/f5tsff8Tmbc2tNu9sa7e5JF2wYYPNL+3cavNzXrXW5oVS0ea33vYtm//sG99k81w+a3NJGts/ZPOv3PKgzW+66V02//btvh6MlqZtrmrOxuMnqn57SR0TJZtPF2Zsnk/m/Q5O+PKl4DtW6r+p+tSn/qfNn9q/JyzjJde92ub9Rys2//KXbrN5y9JlNl9znr9ff+MDV9h8hU1np+EUlOEk5rh912w+c7VvFxXlp1n3cZ8vXxKX8dSOtM2f2HPfCziif28HB2y87pIbgwJ8m3JisOcFHlD9yWQy4Wea0v485bOLbX7xhZfbfMtFm2x+oPchmzfkpmwuSU1tvp9/4MF7bX5s9Mic8s9+/jM2v/wq324e6eu3uSQVx8o2L0/5vuGxnb4PTmd9XXnoQf/3B/7hM5+2+cSxwzaXpM5zF9k8mfR1dabiv0MyaBOqZT+WqjYE+Yy/RrORSgc9oG9268K+x319r1VqNr+g7VybLyn7uvr4YX+v7D/8sM1n4xc2+rr2rjdss3lSBZunKr4ibLvxGpsfnfDlS9LQ4FGbJ1f50VBrJpg3lIP+qdlfx9QP/BxYkq6u+jJG1vg25zs9J8J9zMX+fl8XL9vk1yok6WVXbLb5vvGnfQHxFHLhK/v+rfmyq21+uKfP5p/553+2+Vg0B843+lySJqOxjm8X1eP72JMTQfm+Kkrdh2ycvMK3OZL0utf6NZu2YPuJIN+y9TybP/K/ggIqfoY4PjYWFCDd0dNr88HRgzYffdrPTZ46e6PNK8FyS6EQt/2dnZ02X9IWXan6lwv+ymwqajej7YPNw+FkfBmVHPDryC3lvTZf2+EbhWk/NdK1V/r53+v7R2x+x+EBvwNJwShD775xi//AimBVJx/U9etf4/NdO33eN4u1iK5grBTM3zZ1+Tly77ivrPue3m/zsel1Nm/1jyMkSS2neRn44XvvsnnXCr8u96Z3vc3m7/vtj4THcP75Po+64E997nabb7nC328q+2cuK9evCY5AqpYmbb5n3M/vGvf5NZ/jJf/MYzp4dtWb8nPof9ke32+Zqm98L875VqflO9ttXsz5G6L8at/Hj3f4+70YVSTFj11Szf48btvmV/wH1vh8n6+KqvpHcypNBAVIyvV1h59ZyFKzeDZ2oNv34W+76WU2b9/m1yIKWd93tGd831GNnupU4glossGfh+ONfj0k07Xc5j2DwzY/L+fPwUwyGHTPZt1uvX/+pia/7nXBK99g855eX08K435+WJpF/zzS6Nulgap/1jyZ9+1qMZi7jHc/YfNk1tfF1g1B/ynpeNi4zmKufxrwCz8AAAAAAAAAAAAAAABAHeGFHwAAAAAAAAAAAAAAAKCO8MIPAAAAAAAAAAAAAAAAUEd44QcAAAAAAAAAAAAAAACoI7zwAwAAAAAAAAAAAAAAANQRXvgBAAAAAAAAAAAA9CqzowAAIABJREFUAAAA6ggv/AAAAAAAAAAAAAAAAAB1hBd+AAAAAAAAAAAAAAAAgDqSmu8DWLAyPp6aLoZFTAV5S1ve5omGFps35861+SN77vIHcGC3z2dRPRLnrLP5mtVrbd6YWmTzzo52m1enTtg8o6rfvlqxuSQNHh2xeUd7q833dT9o8wMH7rd5rmuzzceDr1Bs8PkzO0nbuFD19X20EtT2Rdlg/yt8vjHIJen7h+LPzKOBkX6b59uCcyRp7xPbbf7Du/favFYYCvaw2Mdnrbbx+PBv2PwNr3t1sH/p/PW+8V3nm0XMgm81pQce7LX59TesCveRb+l4AUf04xi36f5HH7b5ivO32ry/L7pXJJ08EH9mASuVpsPPjI/498I78m02P3/dJpt3tuRsXpwetnl/dcLmkpST30c6+A4NCT8Wyq1rsvnAyA6bf/0Wn/f3DtpckiYnZvwHKr6Pnxwds3kmU7L5xRuusPnex+6w+WysWd9s80yDHwxVgureoGiw5PNiyZ+j8nQ83qxU/Jg1lQn+noavynWhtrvmP+CnBbrrK35M/YdvfpvN7777c34Hs7A6yF+//jybJ3010GAwPxtq8uPJqYP7bd7d59tdSUqnfZsyID/m3bRqjc07iv4YMmf7ccjWc+N5Q2u/vyff83vvs/kv/uFf2fxzh0/6A/DVQEtWLbf5w917fAGS2q/YaPPSTNAuJcJdLHhLN/vxXlPKzzv6en1dLgfzt5liweaqBjf8KeHHKRqPZgaBSb/9wJBfS5GkrHybsr3SZ/O//fSf2fxw34DNExvPsXmt5K/zD++8zeaSdN/uB2x+7KBfM2pq98ewpNm3i6Mjvi6eLMRj6nTQv6xY5tvedzb781wPzgqGYtEdXQ22T0cFBMvQk333BgVIqb5bbd5W8nOTV17u17G3rPB9+PCQX4e++CK/hvy7L9tic0k6Z5Vfi9h84+W+gKmg3dod3C+b/NxIXb5/VqHb55IUzBt0IhjPBWu0qxb5OfRwf4/NB8Z9/1po7rK5JKXi6dOcVI7673DBej9vSGW/avOhiafDY8iP+snN2sV+Dvqn77wx3Mdp1+Tryszi9Ta/r93XhYcfecTmhTHfvyWD5z6VmbLNJaky5T8zNuPre3uw1D7V4NcSHnrEr/VPJ/14Nd0WTKIlKek7qMH/zd6dx8l112e+f87pWrpcqt7UarWWttzaLMuyjbxgY4iNMcaEAJks3JAwZLhZJ8tMMnNn7mSY5CaTm0yYJHe4mVwmGSavDIHLZQlgwwDBBgwYLxjjXciyVrfbklrtVqurS6XqWrpq/pD8ug7g59uJpEglPu9/9LKes/zqnPNb67hU9h1QJeefgz8euNzv/zoby48EpXyw7idJlWf8vGD+I58Oj3EuFQrxgk9lyD9Leyb9ev/z+/2zNrre1+eh1Pct9eC3R9J2vE5dn/Prn9XU19f+Nats/swBf40qjzxn81ds9+vg9Xrc5mhv8H3DNl+fVr3jnTa/vu6fk4c+89c2n5qP53eHgq+KZ9attHm55Ot0seXz+oS/j5m2vwYDS/g+vVrxa+nSRfFBzgJ+4QcAAAAAAAAAAAAAAADoIrzwAwAAAAAAAAAAAAAAAHQRXvgBAAAAAAAAAAAAAAAAuggv/AAAAAAAAAAAAAAAAABdhBd+AAAAAAAAAAAAAAAAgC7CCz8AAAAAAAAAAAAAAABAF+GFHwAAAAAAAAAAAAAAAKCLZM51Ac5bzbKNa/V2eIj3/c1/tfndH3zS5pk0b/NSabUvwGzF55oL8k6QS53nfL6/1vQbNFo23tX017lv5ZDNr9621eaFfFwFZg9P23zDxhGbl2f8/oWgCNmMfw6yvf02rxz3z7IkVWvzNj866z/D8tWj/gQj/j6pUPN5PniOukA+9c9y/4C/j5JUnava/PWv8c/7w9/wx587tsdvcOJZG7//d78a5P7wkqTcK2z8qc993uYrVq/yx8/6+MpNPu/zse7dGTzLkt73vvfZ/Ns7dtt8cdG3m+PrN9h8+rBvEx556AGbv+Od77S5JB2cPhZuc3r8Z5T8fTj4zD6bLx/fEpbg6IFwk/NaczEex5RKvn8bHV5n883j622+POgAR9cM23yiOmVzSXr6wCGbXzJwpc1/4FbfJm242ZdRbX+dDx305VMav5s/P123eXn2uM1LQxfZvLfQsPm9991r80jfxfE2wyNFm7ea/hpEj3s+78daaXgfCn7/TDzebLZ8294OnqULQlCdFAwHX9jjn9X7v36fzXtX+ONfdvUlfgNJvzLixyG9B2dsvu+Afw7ufta3GeXWrM0vmfD5ljXBmF5SMedHQ81ZXx+fb07afKrtxykbav4aDm25zuaStPZpP7dJywdt/u6fuMXms5/6nM2fvcoPOKfk63unEvcNO3bssPnQumAtwT+KXWHTuG+b9+zaZfPlQZtUGvF1YSIYj3Z813ZSoTfIg3n2jJ8/Sr6+asulPn/wGRsf+fJXg/NLv/VH/6fN9+x/xOaP3fUZf4JCMIOb9G1Oz/ZX2fzoLl/XJEn7gg7s4NM2XrjBj1fLrQmbn5j07aYO+75Bkpplv82zmzf6A9zyU+E5znfZoOkNapOCpj0+QNn3n82Dvq5IUubgQza/dMAXYnyzn/8Vi36e/uA3Hrb5k7v8PH3bNdtsLknbLgsa17q/jpoK6sPuYM0nG7Q5hUGfB/MKSfF6fy46QLTG6r8vuHLetylTR/w1ag6NBeeXGkH3drouW+nnfwO9vsK35MeKH/zAX4RluHTjZpuPbvTt6rar/Brw2ov9Z9xu0zOjJ8hv6vXP+02vfo3NDwfH/+bDT9l8atr3n5LUrPlnYf+sH2s9XfL7l1u+vlUP+japWPHtQSYftyn9GV/GajCcfGbO9x3NXZ+y+YZr/Bz60St837O8WLK5JGWKwRcC57mbb7w93ObpCf88p/2+fyxmfY1dSP261mJwidNM0DlF0xZJheBZbbX9Z6gF1WFo21U23/fVr9h8W33R5j3y7bIkPbzjCZtfus5/Z3H/vXfZvFX030G2tt9g80P799pcktIVvhMvF/2NqEa/UzPk97/6Kj9efPNNN9r8a1+5x59fUr3p2+aMgvWWs4Rf+AEAAAAAAAAAAAAAAAC6CC/8AAAAAAAAAAAAAAAAAF2EF34AAAAAAAAAAAAAAACALsILPwAAAAAAAAAAAAAAAEAX4YUfAAAAAAAAAAAAAAAAoIvwwg8AAAAAAAAAAAAAAADQRXjhBwAAAAAAAAAAAAAAAOgimXNdgPPW+hEbz8zUw0M8/vAemx87Nm3zvn5fBqnl43o72L8Y5EtRCfKsTdcMr7F5qd+XsbDMH/+FGX+N52aO2FySGrWGzdeOjNl8zZpxm5f6ftDmk7WCzaeqNtbsVNlvIGlwmT/H2pFhm6fy+2suKGR1xueaDPLz39zhQzbvz68LjzFS9PVhdHjA5q/acqvNm/Xbbd4/4OvrVx74ps0femKXzSVpZGyzzZupf9YqQbPYWPD5gw/7/C/f/x6bf/wv3u0PIEnqLGGbv79d95/Vw+vD/+Usn2AJelcP2nzt8Cab7zvg+65MJvd3LtOFKA3anHzq3xtfMeD3by/O2bx/oM+fv12yuSQ9de/jNs9d5/u3kXE/VC7PH7B5JuvHKf3+9Lq0GI0FpbTm70O77ceDaeqvYzbr9//q39xp88j4JcvDbXL5vM0bzabNiwXfd7RavvNIg2c9V/BtxmI7/n8scm3/GftKwbg96N+6gr+Nyi5LbF5s+P51V9mP+xde8Oe/eXy930DSSNZ/iPKkH/NWjgzZfKK6aPM16/28oyf149GMfwyXtE1vwW/QH4xXh4f8dX7ovrts/qp83DdkNgZta973PyNNP7f56duutPn7a37/p3but/kl6zbaXJIWa0GFmg3mZ/4SdIVtG4OHtebjUp/vpGtB299f8PtX+v0YQYr78FrNtwlHfHVTdBEGNvt1r7nKRf7wE8FFlnTHHR+3ebNy0B9gPphbzcTrIc7iET/W03S0liEpH4wD5oP9q74+H69O+P3rwflb0dqhpLngOk92/5pNJBrNRVcxqM5qBn1868hemxcqh4MSSOtKfkx82Rrfbg2MBPPkQo+N3zJym89vnvXHz8Ttpup+DVcz0SJq0H/6jygdDOpjPjj/XNxuqhI1GoEjx3ze59dbsjm/NviKRr/NnzseXGNJmeYS7vVpmJk9bvMHH3zQ5tdu2m7zkcvfHJbhY5/1Y9r7d+6z+TefesKXYc0qmy8f8uPhQjT/lHTZ5lGbv2V1sKYTnsHzn1D64euuCLaIculLB3y7dP99D9i8Neu/m7qs4a9COuLnNs8EffixZtzHR/3bcL+v01cO+4axvNm3WVuDuVGm4udGz+z111iSZmf8OvCly4Ix7TmWLqG2tIOmtRE8Kz3B+mW9HnwfHnwnU8j4MUi9Fvd/+WBtsJD145R2248RCmMbbL7prX69pveIn7ekwTWSpC/t3WnzT77/vf4Awfpou+0L0c77dfDFzf67O0lqBY9rJhgULw+arXa/L2NtxI9Tlm27yeblL/t2XZIqFV8f/Ejq7OEXfgAAAAAAAAAAAAAAAIAuwgs/AAAAAAAAAAAAAAAAQBfhhR8AAAAAAAAAAAAAAACgi/DCDwAAAAAAAAAAAAAAANBFeOEHAAAAAAAAAAAAAAAA6CK88AMAAAAAAAAAAAAAAAB0EV74AQAAAAAAAAAAAAAAALpI5lwX4HzVW1pn82wjHx7jiQd22zyp+WMsHyjaPDdU8AVY5ffXvrbPFeVS+AiVZ32c959hODtk83Wj/j49/sTDNh8o+ONL0tv+8btsXuofs3m5XLN5vblo88psxebZtr+GV49vtbkkFXP+OlSm/X1MM76MylZ9XgjqUyaub9ILS9jm3Gk06jZv1pvhMZYPr7Z5Lt9j80zQZJSGfJuxeqTf5q/Pb7f5K264xhdAUmFki80PHD7o96/5dmvDOv+s//UH/qvNP/7BP7T5VbdfZXNJyuT8u7atlm8zJF+f6vWGzZ/dO2XzheeC058HelP/rI+O+Ps8M9Oy+eo1w2EZjuwJNzm/+UsoSWq0fX1avXKlzZddlLP5w48/4guw6NvF6hF/H5fikYeftPntmy+z+ciqks2j+pjN+nFUvRa/m5/W/TbtoM2Q7560b6dvd6XTuw+lfn8NJSmbBu1m8Dz3BG1GT87nzZZ/Fmt1fxFbPXGFi+50tRrcxyXU6fPeMR83X+jY/NW3X2rz/NhAsL8fZ71xvR/zS9KzD9xp89XrfJ2vpf4+txp+jHDJllU2v3LM92/5fNzmHDr4vM0vKvrx5PCwzwdX+Os8stnPbb6wd6fNJWl8ox+zqurnPoMtP/dZmPXtZi7oX9ePrbH5/gfiQciKrWv9BsU+nwdxN+ht+7Z57ZBvE1pB218Kllv6i1mbV/LxclwpuE89GT/Wmp71bcahsr9Gi5q2eXnUtxmdoVGbS1K7Nu83GA7GCWkwd5pd8Ln/iFLruM+DNk2SVC37PDrEkUM+zwdjhOENPm8v4f8FnQk66ePBdb4AZIIl0ihvBWPu1tyMzcuH/Bpz6/CkP4Gk3lW+zQifhNngOSgER4guQinofCYP+1ySssHCVzCvCKc2C0Gb88Q3g/MHnzEbz420GHzGRvAwPvWYz4N1bG293sZrbtjo84GgTZM03/br/Z8Lj+A9N+cnT3fc8TWb56t+nfoX/5lf25Sk+3fvt3lzr6/TBw76tb09E348GlWFTBp/L/S1YNz/zSv8dfi1d77J5n5E/A/j9eN+ffHm8Tfb/EvP+7b9B//pJ30B6v452PFWX1cevM7PDyVpZI0fr0X92/PTEzYfKvo56NiWK22+7//+lM2ritdwiwN+XK7W+T2Oee5gtC4XrxkVi75/aYZfFftGI2oy6rWgb6lH34dI9WCbbDa4z8Gge3beX8N1wff15WCcU5mJJh7SsTX++7dq6vufaH2yHdyHTM/p/0bMQrBulcv4PrgQrZUH37M+uXuXzXfv22fz3BLeGWjNBmOZJDzEWcEv/AAAAAAAAAAAAAAAAABdhBd+AAAAAAAAAAAAAAAAgC7CCz8AAAAAAAAAAAAAAABAF+GFHwAAAAAAAAAAAAAAAKCL8MIPAAAAAAAAAAAAAAAA0EVO+4WfJEmWJ0nyc0mS3JEkyd4kSWpJkpSTJLkvSZKfTZLke54jSZIbkyT5fJIks6f2eTJJkl9PkqTndMsEAAAAAAAAAAAAAAAAXKgyZ+AYb5P0Z5IOS/qKpOckrZT0o5L+QtIPJknytk6n03lxhyRJfljSJyUtSPqYpFlJb5H0XkmvPnVMAAAAAAAAAAAAAAAAAN/hTLzws1vSWyV9rtPptF/8yyRJ3i3pm5J+TCdf/vnkqb/vk/TfJC1Kem2n0/nWqb//LUn3SPrxJEne3ul0PnoGyvb3tjDVtnl/vhAf5EjFxp3ZWZu3r9pi88JwyebLrtlg8+Mlf35Vaz6XpEr0I1FZX4YBf53LRf+I7jgwYfPeQtHmq1eO2FySvv71B2xea/j9l68et/nA0CqbV6t1m+cz/h5cvvFym0vS8zP+WZjYu8/m2cK8P0Gu6vNiUJ/aS6hv57u2f5afn5gKD9GX9tn8RJ9/FnK+yVC+4J+1mTnfpqVBl1Jr+/ooSYWMbzPKNf8stQq+jJ+5816bf/ADf2Lz195+lc1Lq4K6IGn/pG+35uf8Z8wXWjZv1X0+uDLo+odyNh4f2+r3l1TIr7H5lz/x6eAIozbdutm3q61W0ObIPyf7dj8Z7N/92ou+/5WkTNHX2VKQN4/6dm3H7kds/vyhwzafPbSEccppqjT9s1Kt+h+mrAZjqfZi0+bNhfg+5ReDOt3jP0Mm9X3sMzunwzKcjnb8ETVf9XW6Uff5bO2Y37/l70Mm6JvSfHAPsn5/SUqDH16tBM+KVoSnOP/57iv0I7e9xuZPHjxo8+uv8HOv53ftCMswcdiPqWeCZ3l445DN06ZvV6/bfqvNr7/pBpurGswPJU3u2mnz/gE/4Oxb4/t4ZfI2vmbdTTa/cma7P76kr3/9CZs/uc+3ez98+2ttfnEwdzn6/33O5pUBG0tLaJZfaD9v803b4+vU7fJt36iMBHPQVtuPidNgatMKlkrawZhfkqJZcCnv54e9Aws2X1709e1EK7iGWX8RGjV/fEk6dNDPQesFP9Y6XgwudCbooweCzqcdLPjklrCsGo0TRoZ9XpsLju/jgSF/n+aWMBbTaOLz4Fm6IATXKRsM1drB1KVVKdt8dtKvy1WnD/gTSDqU9fWlGqwNjgVrSpnU16dGxT/L40N+HBSNESRJfUHjHFXZkm9Xw/XJenCjy0Hbf9CvF0mSeoI2o7/f51f49RTNBmWcO+Tzxz7p8+Zun0vqG4rXnU7H1x72axFH7z1i8zWjG23+C34KLkl68Mt+fXJ8nb9PW155nc2rwbxjz25/H/bs8m2OJB3Sos3v+dxdNv+jn/5Zm/+TP/wDm7/7X7/L5o/uiec2kR/Y5Nslv/op/eDaoL5uvdHnf/QmG2+b9nOjbU982B9fJ7/cdZ7e4edOM/v9HLWe9WO9XM03zB/87H02T4d8fZSk1XnfNv/M5cvCY5xL9WgQImku6GObwfcFjWDulAk60HxQxErQPabFlX4DSblgTO1bJCkfDJpri/4aTlT99z79JV/fM4Vooi/VFv36YbXqB5zRk1Jv+S3yPf78mTSe+8xXfCeYT30ZKm3fZrSD78MbpRmbH6r4MfPwmmAsKKm62/cvK85Rk3La/6RXp9O5p9Pp/I+Xvuxz6u+nJP35qf987UuiH9fJJemPvviyz6ntFyT95qn//KXTLRcAAAAAAAAAAAAAAABwITrtF34CL75u9tLXA1936s8vfI/t75V0QtKNSZJ8H/zvIQAAAAAAAAAAAAAAAMDfzZn4J72+pyRJMpJ++tR/vvTlnktP/fldvxbX6XRaSZIckHS5pPWSng7O8XK/v+h/jx0AAAAAAAAAAAAAAADoUmfzF37eI2mbpM93Op2X/oOdL/5Dsi/3jxO/+PfxP2gHAAAAAAAAAAAAAAAAfJ85K7/wkyTJP5f0v0naJemdZ+McktTpdK55mfM/Iunqs3VeAAAAAAAAAAAAAAAA4Fw547/wkyTJr0r6E0k7Jd3S6XRmv2OTF3/Bp1/f24t/P3emywYAAAAAAAAAAAAAAAB0uzP6wk+SJL8u6U8l7dDJl32mvsdmz5z6c/P32D8jaVxSS9L+M1k2AAAAAAAAAAAAAAAA4EJwxv5JryRJ/o2k90h6XNJtnU5n5mU2vUfSOyS9UdJHviO7SdJFku7tdDr1M1W2v5fj0zauNdvhIS5aP2zz9nCvzTPB+1iFnrw/fqNqc6XBJW4H+0tSNTjG2JDPm/M2fnZqj9+/7h/hNf0Fm88eP+iPL6ldbdh8aHjc5o3Kd/7I1d82FRy/2WNjFYdbNm81D/sDSNq3/zGfT32vd/f+f/2loClpNX1eHPC5ckF+/ls9us7mExMT4TFyed9m9BZ8mzC8ytfHDVu327x/YMTmpULJ5jX5+ihJPaUxm1fa/hxTM8ds/uD+XTZ/w20323xowNfnL933VZtLUiWoDvmsf94HBnzfUmsdt/nUjM8v37rW5tuuWGNzSXrl9tttvrzoP0Ne/lm9dLNvd5/cGbRp+/fZfK7i+6YLQaVcCbdJW75/qc74/uXBr99r87u/cp8/ftaPg6bnyjY/E2q1ms1npn2726gF46Tg1ft2PR5vNrLROMCXIZf1n/HI4eeCEiQ+9l2XlA1ySQvBOKIWjHlrDf8ZW82OzXPBlCnb4z9kuxVPazIZ/yzlCku4UF1uy49us/nCXj9ur9V8uzYytMrmh6b9GGJqctLmkrTngO9jN/S/3I/cnpRWfX0eG/LP4rWb/XhTy6Ixe/z/A41t3mLz+ckj/gCZPp8P+jGCCn6enl17ld9f0qZJP567++Ev27x/o7/OlxT9eHbtpx+x+Tce8nOvJQlu9QN3/o3Nb/up7v8X0jPpos3z+WC9JesvYl3RWMr3Xfmg/5ak7KJfK8gGazoF+TYp2+PHGX1532Zlan7/mcX4h7v7i77/m2pFxzhh0xVr/Ge4eGjQ5o3oMy5hTD0168tYzPr7VA+a5qEh/xmLQXuwsIT/FXRhZbC2l8bPc9cL5vEKhu1pkNdm/dzq2KRfy1he932bJFXm/Xjy2MwBn/cs2Hx8zNenbddfb3MVggXQih/TnzxG0efBGqwOB2tzURGGR32eC9bFltA3KJijaibIh4NKvzFY81kM9j8WPIu7v+hzScp/M9ggeJYCR48E4/oB/yyrz69NfvIzXwnLsOsT/9Hn8mtzl7/zHTa/7IorbP7Ka66x+Ybx7/p/9b/Lvp07bX7kru/86u87lPyz1kp9m/V7f/oxmz+5w5dvetrPKySpf9jPTa677jqb/8CP3Gbzn78h+k4k6MSD+aP8UsdJ0ZJOMVrP923CZ77g5z7/Yeef27zzsK+viyvi+zi53q816PLwEOdUtR73fyeO+3WxSjUYUwdLTu1gHNQI+tdasD5an43v48CAry/Npi9ko+T3r8mXseqnl3r1237G5oNF33dI0v2f8u1mu+brfJr3A85s3tfnNOfbnFY7XqcuFP28IRu1a3X/GRcLfv807x/mQ0d8XbhsyPe/klQo+u9Jz5Uz8gs/SZL8lk6+7POIpFvNyz6S9AlJM5LeniTJtS85Rq+k3zv1n392JsoFAAAAAAAAAAAAAAAAXGhO+xd+kiT5J5J+V9KipK9L+udJ8l2vhT7b6XQ+IEmdTmc+SZKf18kXf76aJMlHJc1KequkS0/9vX89FwAAAAAAAAAAAAAAAPg+dSb+Sa8X/12NHkm//jLbfE3SB178j06nc2eSJDdL+neSfkwnf+x/r6R/Kek/dzqdpfzYHAAAAAAAAAAAAAAAAPB957Rf+Ol0Or8j6Xf+HvvdL+lNp3t+AAAAAAAAAAAAAAAA4PtJeq4LAAAAAAAAAAAAAAAAAGDpeOEHAAAAAAAAAAAAAAAA6CK88AMAAAAAAAAAAAAAAAB0EV74AQAAAAAAAAAAAAAAALpI5lwX4Hy1fHPR5ovLmuEximnB5u123h+g7s9Ryvj9r9y63ua7eqdtPvfwIzaXJI2WfJ7WfL7Y4/Ni9Ii2bLr64hGbbxseDo4v1ebKNj9Wqdq8Xj9k89KAL0MmeC2vkPP5M7v2+w0ktVv+M/SX/HUuV2dsvuIKfx8UPEYjw6v8BpK+/VC4yTm1bp2vj4cm/XMiSbWqv09Dw302by/22zxN19l89YYbbT4+bmP5s5+ULGEbz9enX3r9b9h84qg/+nv+/S/YvFWNP0Gt1rF5Pdewef/8cZvnCr7vWT6ctbnavl3OpcH+kkoF33Bt2+jr9PN7j9j80MRuf/ytG22+b/9Om8/tnLT594uFmTmbD9/g25wDk3ttPjXj27TssO8cyrV5my9F34YVNq/V2jY/9oL/DKer3V4Mt0nrPi8U/H1SxrcZCqr8slW+Ptfq/jlSMI6RpGjUneb8uL0QjJkLQbtZr/uLnPb441fmg/GwpDS40O3WEi5Ul3tul297f+Sm620+ss6PY5554qDNH7rvyzYvzMVjtQ2jgzbfdK3/DHM1P+945fiozXtGfF3Q3ITPK/GzqkV/ji985j6bv/YW/6yPvH61P38taPTysz6XNHb7a23+hnnftu98zE88rs9tsvnVG7fZ/BOPTtl8KVYM99r8hcmF0z7H+a7c9vexJd/H9wZ9RyPoneotP6YPJ/qSlA+2CeKs/Li+VQ/6lkU/hkiDcUohnjYoG7Rb7Zq/znO5is1XD/mLtDoYJhVG/PzyaDlY15OUly/jbN3P7waCU2zo831D/+CQzUut+DPsq/m2tR2O1i4AwfLl6S6wV2Z82z/9rJ9bVcvPhOdo+aFoZPr5AAAgAElEQVSQBoMPUQ/q9KZ1wQGKy3weLHAu1uK50dGZYzafmPTrl9/61g6b3+2HiwqWe7TWx8GK1tKUgmWpDb5J0GjQLo744a7Gr7rY5ulwMP+UlM/67yyU9WPqyNit19h8y5btNr98/Qab79n3WFyIDa/wedCsfvsxf45vP/GEzS/Z6NfN1o77udXJbXz/o4tW2vj2n/sZm7/zHe+w+Wfv+qLNn7jPz0vUjFvuw7Vv23zX3V+x+R2f/RubT93yj2z+W9t+2uba7Ffbp4OpkyQN+WmDHq/6OeJffvgjNt9/16eDEkRr6UHD+kI8T9cLQQf4xrfHxziHjk37saQUr19OV/0xiiXfNhezfryY5v3+hTSYd9Ti9dVUfhyQbftr0KxG68h+/3bRX4NLb/DfnfX0jgXnl3o+5etTvR6Myat+DloLrmFajPpof40kSUG7026f3qi5FXx3Fc5xj/kCVuXHkpLUrATrTsFY6mzhF34AAAAAAAAAAAAAAACALsILPwAAAAAAAAAAAAAAAEAX4YUfAAAAAAAAAAAAAAAAoIvwwg8AAAAAAAAAAAAAAADQRXjhBwAAAAAAAAAAAAAAAOgivPADAAAAAAAAAAAAAAAAdBFe+AEAAAAAAAAAAAAAAAC6SOZcF+B8NdszafNibiQ8RqNas3m7tszmg5k1Nh8ZGLb5jHI2f/tP/CObf6BRsbkkDY5usPnhXVP+ABOzPl+sByVo2zS72LJ5f64YHF9qKyhDfc7GC83DNm+09tu8XPef4cH7pm2+ZnjU5pJ0+Q032rwy6etDmvXX4I3vuMHmq7eM2XzDJf45k6Rf+POPh9ucS5ddvtXmhw8dDI9x6PAhm/fkszbfeWDG5u957502X6j12Hxgs7+PQ6WSzSVpsNhv86jdGxkdsnm7x7fLs5P7bP78/p02v3TDa2wuSdNVX5+en37W5vv3Hbf5wIjPhwYGbT59+IjNJw7Ez2q+/ZDNLx4Zt3lrxLfNn7zzLpu/8mbfpm3ass7mxyr+OZGk/eXnwm26Xa1a9Rvk/DByz5R/VqI2rdhea/NqxY8BlmLrlmtsPlt90uaVsr9Gmby/Rvlc3uZLeTe/UCj4MviuQWkwXrxo+CKbrx3zY+KjM02b5/J9NpckLfpjtFr+Q9bri8EJ/DWMlMt+zNysxcfvH/XjtUL4rHS/EzvLNi/d7q/Bxz77GZt/6s5nbL5qyNfXt9zgx3KSdNmoH8ekgwM2by36Z+l1N/oxtSZ2+bzlj7/3cDz/K8/6+vSFz37L5sdmfdv9i+NBfSkESxh+mHNSzo8XX/eOd9j8vg9/xOZPPnafzbeN+Xn+rT93rT9+PZhjS1o76s9x6bJgXH76Xew5V1kM1mOC/ettPw+v1f3x06zvm3JF3x5IUtR71dt+nJAN+rdmy48BGvVovcTXx+HheP6XGfDrYvXDDb9/j+87iqkfQ6h+zMb5Xn8NL8oHx5e0OpjbLM4En6GQ2Hw86FtKy/z5M6V4vJkZXmnzfCG+Dt0uGom1/XKJ0uAyF4P1ktKA77uqfrnn5DbB9O7KK/y4f3zYX4VW07ebex/6hs2LJf8sPz8VrCFLevQpP8fcudvv/7xfwtX9HZ+/4ONQMHVb0jbFoIyrj/q8EOSVAz7PPOrXSvyq3kmvXuHz5JeXcBDjyu1X2vyaq3x+yRq//nllPp43vOEHb7F5teHb1T27/driJ//6UzZ/9hv3+vwpP6+RpN6BYCxzwleobVf46/T64CuNPVs2+w0mgwq9uIQaF3yGqNYff+4Jm/8fT/jvhTb87v9u8zeOb7T5PTuD+aGk8rzvHO74vJ9n7//yA+E5vKDROiP8WsD5rjI7H2+z4L/DbKX+eS83/HNQqfu8lPVj5mKfH4+28/GaVy31M7hojbUVtKutpj9+KVh/fW7Gtxf9hXiSncqPpdo9wTFawYCz7a/BieA+54I5riS1Wv4cx+b8OfqDvqURzIEzVT+LHuwN1uVq0TsJ0tGD/jv5tUtY6j4b+IUfAAAAAAAAAAAAAAAAoIvwwg8AAAAAAAAAAAAAAADQRXjhBwAAAAAAAAAAAAAAAOgivPADAAAAAAAAAAAAAAAAdBFe+AEAAAAAAAAAAAAAAAC6CC/8AAAAAAAAAAAAAAAAAF2EF34AAAAAAAAAAAAAAACALpI51wU4X3X2HrR5bV07PMZiLec3qPljpP11m68o+eNXn6rafF++YfPFYtHmkjTUN+TzVcM2z/RlbV4NXknbt3enzZevHLR5Tz5+561cmbH59PQBmxeC+5Tp6bH51PQRm8/V/f5rRq6yuSSl/X6buV2zNm8VfT6+eavNL7l8zOZ7nvL1sRusGx21+XXXbA+P8fkv3OU3yPlnrZTzbc7FYy2b7975pM3ndj7kc5t2i4t8POjbNEnSXNnnvkpLi8Hhg2ZtbuKYzW+87kqbbx2P25Ra1fdfByYmbb5n94TNM8P+Ok/Xp2z+3KQ/fzvj+88LQbqEd76jLR7d7fvgg3PTNvejEKnY9CXIBsMsKawuSlPfLjZqfv9s3g+lM6kfS9Wq/iq029EnkOq1ps0bLf8hSgV/nUdGfB/9whFf345Ov2Dzpx4L2kRJg8P+Ztea/j4emj5h845vspRdFuQ+Vqk3nnJV5n0hsll/lsteGZ6i600f9uPBTJ+vb//2N/6xzdcN9AUF8H2TJFUn99p8qu77l9wyf5/npvw1mNq9y+YK5j4H/ZBekvT0U37uc/2Nl9n84nUjNp/cud/mq8f8/LOnHfUuktb6Ojn7pO+/nn7MX+dLZ3ybVBwK5sC5is1Hr9lic0laPeyvc3U2aHsPhac479Xafm7Tks/Ttq8v9ZYftA/l+21eyMXzhsWa7xuKyts8zfsxQK3q84Wmryu1mv8M7XzB5pKULvpzlIp+TamY9fcpla9Ph+b9LPXopK8r6UC8btYc8M9CNugbelr+WVVwDXI5X8ahYnyf1PLt2ola3Ed2u1wwOQrukurB/rlgfXVwnW/7C0u4B6Xm8zYv9vpnYfXqVTbPybdZlZp/1o8d9Ouvlel4oJILxvXjvnvUBj/1UfEpnz/opx3yn1BaykpEcAr52aHkZ2envSR1RjwSFPLXTvP4IwP+WV4M2t2FYI49OuyPL0mDQ77Ot4MrPT6+xuZXB+PFv/7InTa/6z+93+aStLAnmHtc5Mu4enydzaNncSqYH2rU34e3/+Q7gzNIR8t+HPHFL3/FH2DHYz5/7os2/sU7N9v8yTv+1OZP3/llf35J/++HPmbz/d/e5w/QilquYEElXB1cwvzuAjd1MJ6cpQO9Ns80g0XU1A9UGm1/n6M+Pu0NBkLB2qQkNevBGmqwxhutHdbqvgedm/HjkMe/7dfJ1xaiUYBUC65jO+fnTvXFYJ274T/jfMNf43wwR5akfN7PUdtF38dOz/vrXF7wffCGUd//ja7yfU9aD74MkJQrBAPKc4Rf+AEAAAAAAAAAAAAAAAC6CC/8AAAAAAAAAAAAAAAAAF2EF34AAAAAAAAAAAAAAACALsILPwAAAAAAAAAAAAAAAEAX4YUfAAAAAAAAAAAAAAAAoIvwwg8AAAAAAAAAAAAAAADQRXjhBwAAAAAAAAAAAAAAAOgimXNdgPNWutLGi1O18BDLhvptXhwdsXmtfNDmPflpm4+uGbb5o/uftPmGoTGbS9JPbL/V5r/5Bz9p8yQ4/mLH55+/e7fNSz1Nm9/92U8FJZCK/T02H6n763xgYsLmR2dmbJ5bvcnml994s83nGv45k6SphZLNC6U1Nj867e/DI/fusvnMdNnm1SOLNu8GvUX/HG3YHNe33FfyNt+9a4/NR1YN2vzqret8vs2XMVvw5Rvs8+eXpPbxts2rx3zbW6nN2ryVadn84cd8u3vw0NM21zEfS1IymrN550jDHyBoF+WbPfUUfddfq/prfGjSXyNJunTzNptnsgWbP3tgyubDK327W57zz0GtVrX5XPmIzS8E2Xw8BMyk/r3wnQd821+r121eGljmC9D2z2Kxb8HvL2nBVzdNHHzMF6Ht24zp/b7Sp+lRmx/3j6qCqiJJKvb5PKgOuqjg+47l/ZfZ/OhuX197Vvln7ei3gzZP0tGeeBvL38ZwRhQ0q2F+oj8qgFSt+melsIRnodslQX1ttnybkCv6+du6VUFlmfb925Nfv8vvL+lVl43bPDPmy7Bn/16bP/OU778aqa/wpZHVNm+3gnZZ0vBKPy+YnfFlqFR8m1GrDdi8JxNUhsxS/p8mP9784z/4K5s/88gOm7/6Tdf6sw/481+ioj9/3T8HklQ56OegQ+mF36jU6r7tXQjalIsG/HpOmvH3KZP349XeNGtzSWrVfBnToHvJBmO5TNDuqumf1UbTj/Vatbj/bvf6fLA4ZPNi0a9lKGj6n97vx7N7dvu6VGzFY+rCqqC+Ff2zUp32a0bTFf8grBgJLkLPcZ9LKgQfsxCsf14IoulTj68uqgVVvjji11uG1l9l84mJh/wJJBXzvo9Os76Q01OHbd5f8vsX+/xzUl6o2Hz12KjNJWk0WFrL5f26VbHk2/Z1Y36sln7R16ev+eVPzfl4SaK5SSRagV214kqbt/v8fSoO+bGkJL3yxpuCLZ4Nj+GkQf/YE+SRerAWIkm1oI+PnsVSyY+Z63XfKL3xzbfZfPWo75sk6b+//0N+g53++4CLxzbb3PfAUj4f3Kc9j9j4o7/7zeAMkrb667Rm8+U2H3mN/+7suclJm1eD5+SjD/p2+bHHoqso7f/GTr/BofvDY3jRGOE013u+D9TacZvSHwyqs0EH0677iUGmFQx0gt6jUfX7p/no+FIm+P2SSsXPkyuzwdwqmCO3gyJelPftdiHvvx88eQ7fi0+V/UCiVvOFLGT8gLbS8udvZOLvaSvhHNbPXdKGL0P5iJ8b9Y9Eg3L/nJwI7oEkVdL4eT0X+IUfAAAAAAAAAAAAAAAAoIvwwg8AAAAAAAAAAAAAAADQRXjhBwAAAAAAAAAAAAAAAOgivPADAAAAAAAAAAAAAAAAdBFe+AEAAAAAAAAAAAAAAAC6CC/8AAAAAAAAAAAAAAAAAF2EF34AAAAAAAAAAAAAAACALpI51wU4X/UO9Nt8YfJAeIy0P2vzQqFt80vXj9j82tvW2bz/Bn/8rbv98d+45WabS9L1a7eE25yOnsTnb3nDZptPP9W0+d3NfFiG0sBqm9davhod+No3bd7slG1+1Ur/GQsFX76ZSs3mkrRv72P+HJk5m+ea/llbmKzavNJ73OaDhQGbd4Oj5RmbZ5bQGo+v8vf6a1+8x+YLs74MuaA6tOTrU3GkYPPRVWv8CSRVZ/2zMjo8ZvNNWy6x+fTsEZu/7tYbbP6hD+2yudQJcqlT89dxCYfwXvBxumHR56mvz9VyJSxCteK3uf76q2xeyBVt/oWv/o3NJyYmbR4ZHRwOt5k74Nut810ujd/5Xqi3bD4z7/uXVtvv31vs8/sHdWX1+HKbS9KoHyopI99ulUpDNt+z01+Dem3B5hs3+s8wOOzPL0mNVt3mY2MNmxcLozbftMG3iw8u823G4FDJ5lOHD9l8SUr+WSk3/FirVPJlzGb9mL6U8514Uf4aSdJisE02vfCnbZu2brL5//jE08ERfP7opV+1+asuu8TmLf8YSZIqz/pxzI3b/Byz0ufv89ef2mHzTF/O5uvy/vzROEqSNm1Ya/O77/Tj0eqkH4++4gbfLmtl0Ef3+zGEJC0+P2vzP/yov86vHgxOUPL922LBf8aBuv8M1empoADSUMnf6x/YuMrm80fDU5z3illfHxZ8ddXCvJ8cpVmf1xaD/TPxWkSjJ+g/6r7/K6T+WWoFh0+zfixXHPBjkN6Sn3dIUiNYF2vX/I0qLPP1KR/Ut2Le543g/5PMK76P9ePBuLvhn9U0G7R7iz4vFnyjVcnM++NLarR82z3YF1+HbhfNnvxdlPLBAVpFP+4v9vsxezssgdQ/5M/RX/TjkMqc738yGd/3FAb8s5oG5y8sod0sFIILnfp2LZXP1475zzC+2a8TPPmwjfWsj8+IVcuvtvkbfuRtNt9w3Wtsnhvy63Y9QZskSYNjfiw1+anfCY/hLNT8PL5a9X3P3NwSJgaBesP3oYXqMp+Xem1eDPrHQrHH5iNj8VrE//or77L5g19/yub5vK/TfsQulWd83ySd/n3SzjtsfDDKFXy5pZVB7tucP+v392nyrz8RHF9Sx899Tl88HkRgCWu4lTm/Fl87FKznl3ybkWZ9GQpFPw5pV/zaZDv+ukGZfDDWqflxRK7t50blim/7N6zcaPN1q3z/V5mMv6+ozvsyzFR8y9iWX79sBY9SccCvj7aC74ElqVX3fWwarGOr5vO07ctQPnzQ5o8+8oDNq9FYUtKhRtT/nBv8wg8AAAAAAAAAAAAAAADQRXjhBwAAAAAAAAAAAAAAAOgivPADAAAAAAAAAAAAAAAAdBFe+AEAAAAAAAAAAAAAAAC6CC/8AAAAAAAAAAAAAAAAAF2EF34AAAAAAAAAAAAAAACALsILPwAAAAAAAAAAAAAAAEAXyZzrApyvrti8yuZD160Jj7F24xabD4yM2vyWG7f745eyNk8Hh2z+MxffZvMem/7DOH7U5+22z6tFf41qhf6wDLVyzebleV+IdZuvsnk+n7f54JB/1sqNls1zwfElqdWes3na4z9jqTBg896MfxZHhtbbfNMlq23eDXpLRZsfnT4cHuOyzZtt/vgDD9u8Vav6vO3fAa1UZ22eyff5/fMFm0tSruCvU0t1m7fT4DPMlm2+er1v+6951TU2f2THt2wuSSp34m3Oorb8+YcGSjYv9fp7JEmtqm83pyYmbT62cszmr7rmOpuXZ6dtvm/vXptvCurahaDdXAi3qdZ8fekt+Tqdpos2b7R8/5UG3VepGD+LzbpvM3p6fBnaizM2X7vG15eBku/DM1k/FM8V4nZTGV+GVju4zsH7/3PVx2w+vtXfh3rLP2vjI8M2lxQO+KpN3z8NyZcxk/XXIJ/L2Txt+fKltabNJakdfMZSyZfhQrD78T1n9fhDhcTm0Zh+ZsLXJUlaO+fbnFLPEzYfv86PiT+971mbp7mGzV+oTti8XfRjOUnK1X27tPOJQzY/Ggx58wU/nv258Q02rx6O79Nf/OVdNve9lxQ03Xp84qDNZzN+nJSuCdr+StymXLzR9z+3bvbX8Y4d4SnOe0MlPwetBWOESjVYEQnGKQs536Y0Gv78klSd99sUgrl+JuvHCGnq+8dc248F+4u+/ywN+fUYSao2fZ0tz/g+vtby9aW16PN6xdfHTGGZzXPRgyCpFdTZdstfp1IuWG/Jj/jzR81i6vsOSeot+YYvDebhF4JWcJlyQeeRBlW+VvXrNfVgjl3MxPegWPTPa6kQHKPd6/cP1hKUD8bUwTUqLmFuFHxEzcz6PnoqGEfsO+QLWfdNloJl7CWtxZdKr7D5NT/0Lpu/8uabbD620Y8Revv9eDENblMwdTp5jLhpPS2zs36eL/kKX6/7+lipzIdlKJX8dSwE6x2Zgm+X8wX/NGWC8ezomvi7r3LQhw6v8Z9hz14/N3rLWv+dyvvn/PnPD9Ea8NRp5ZMfPx8G7cuDPJ6fwatMV8JtigXfB9f8139S049DjpWP2HxkxI9Xm81g3WwJryrks6fXOVSqvg8vB237K9/qv48oZnybNzET38doXtAK2r1mMH9rBeuTparPM8FnPHmSYJ4dDJYWg9FSue6f1W9N+O99nr/Dn789FD9nB/PxXP5cuPBnZAAAAAAAAAAAAAAAAMAFhBd+AAAAAAAAAAAAAAAAgC7CCz8AAAAAAAAAAAAAAABAF+GFHwAAAAAAAAAAAAAAAKCL8MIPAAAAAAAAAAAAAAAA0EV44QcAAAAAAAAAAAAAAADoIrzwAwAAAAAAAAAAAAAAAHQRXvgBAAAAAAAAAAAAAAAAukjmXBfgfPW+977b5iNJfIy2CjavBXlbNZsX1LT51Rq3eY9NpdrOxWALqTAWHKUUHKDj4xNHfL5ihc8X8j4vFob8BpJO1Ko2z+T9hywE5+gf8Psv1P3586Vhm/flopsgNVv+3b9WfcHmxQFfhqMV/6wu9hRtPnH4kM27wVM79tr8ooJvDyRp6/btNn/D7JzN903ssXl7sW7zg4cmbD460m/z4dE1NpekVvAeavSstYJneaDo99+6aavNN2zbZvPGB9o2l6TBYX8dnt7pn5UXZvx9SEpZm28YGbF5LvX1cSlvCjfr/lmanpqy+exM2eb9A76MN19/o80zQfeWTy/896EbQV2RpFyQF7O+bT82O/t3KNF3a7T8ICFt+f5RkvI5/yly2ZbNF+p+LNbO+P5xrvGCzdXwA8r2XDBQkjQ6NGjzWs3Xx3zetxmFoq9v7Yxv9xrBeLZZ98+RJLVb/j4VC35Kk0n9/j1BnU/ln7VU/hrk8nF9KwbXeWgoGNTG3c95b2MwZL1620U2v/7aLTb/hZ/5SZv/05/9LZvfXbGxJGm4ctTnfb4+5Eb8vOHo0/78g36Yo/nqCZvvqX/bH0DSiv5g7hHsXwymX6kfzur5Jw7Y/MHHdgQlkJ78yhM2/6N/8Xabv+maMZt/7C/+1OaPHd5t8z09vu0vbvbtviS9/kf9vGFhYjo4Qm94jvNdTr7dLGZ939HKBv1b3fctacv3b7Wa31+S6i3fZhSDMXE7H8wxF33/1BOcX83TzCW1a36btOEb32yPX5Nqlv11bh9v2Hywz48lB5awqtpcCOa4/X5+li/4DrJYHLB5rebnVj1ZP1aUpJFh3/Zng/VN+SFzVwimBaFWy1/ndtvnC5UZm1emnw/L0PaPikZG/Dpyqcc/8GnOPweHyr4+t1NfH5eyGrFY92Vs1/3cp1w5bvMD+/35d+zzebS6uW711cEW0pW3/5zNr77lrTYf2RiszQXtWhoM9oLHRO0lzFvO9pLM1JQfBzUavsLXgr6rWIknDoXCMZv3BuvE2UKw1tHrL2I+6/vP3qJf45WkS8bX27zd9HX685//hM2nDh60+Z//6X+xOc4UPw+XonGEb1cRK88uYUxd9G1GNfh+TlVfX9Om7z+jElaOR+tqccNfCsYxrWDtcO4Fv07d7vFlGBvy46SZWX8NDx2PO8ADu33/VJ7x/UuzGawBt/1nzAVz1HpmCQPiYB4dzoMzvoylvJ+XVDN+IDJdCNbBs8H6q6RCJt7mXLjwv9ECAAAAAAAAAAAAAAAALiC88AMAAAAAAAAAAAAAAAB0EV74AQAAAAAAAAAAAAAAALoIL/wAAAAAAAAAAAAAAAAAXYQXfgAAAAAAAAAAAAAAAIAuwgs/AAAAAAAAAAAAAAAAQBfhhR8AAAAAAAAAAAAAAACgi2TOdQHOV9cl221+WBPhMb6lb9l85kTF5tX5ts9nqjYfKuds/urcrTYfKW60uSQVSuEm1uKMz3t7Tu/4s9M+Pzo9GR5jrjpl80zW36fVa/qD/fM2n56bs/nI6Cabq1X0uaSZWsvm5blFmw/2+XNMH/TX8PFvPWDzhUbwoHSBi8e32nxoaDg8Rn+pYPNb3vomm7+6WbP5jp2P2bz2Zb//selZmw8Nxe+Yjm/YYvPV6327lAneY53O+bb70o3+Po1d7s9fGhqxuSS94qqrbN5u+vp4931fsfmxctnml64es/nCjG8XZ6cP2lySjs34dqvVbNq8VBy0+aEJfx8LRd+uXn3FlTYvz/lreNKjS9jm/HX1Na8Lt3njrb9s86Eh37+1m75/bLd9Pl/146R81o9zJGlwcMjmJ6p+LFVr+3Yvk/efIU19m5TN+KF4ridrc0nK+y5a8kVUJuvLkC/4+tTuD8YAZd831Gv+GktSK2gXlwfjkGxwEdLUX4PgNiorP2CNn1SpWPSD6mLRf8ZH7nlhCWc5vz1y77+xeV/BPwfavNrnySobv/tfv9nmt3xjrz++pGvHfR+7oc/f52Ujvs0a3txn81dse4XNc8HcqlCKJ189W8b9BjU/BlB1wcbNab9/uebnBRv8bZYkve3N19h8aI2vb5XqYZuvv2qbzWeK/jMc3fuszd+wxY8lJSk94OdfB57d5Q+Q3Bye43xXDtZL2m3fuGcW/Xi1GowX07w/fn4Jq3Ht1Ld7qeo2ryq6Bn7/dt330bWyP/6Cj08eo3bE5q3jvs1Q6ufILX8blWn5gdRI0d/H0qK/hpKkYKw20ufH1K3iMpsX/CVQed63WWlPPBZbUVxj83YjGO0Et7EbLAS3OmhS1AzmPpmMP0BaP2bzQnveF0BSIQ3GzMEwoK9/wObHK37+1jju+79m8CzuOxTXt9GCX5NpZ/zcpjTs61s6cNzme2wqlVb7McLFt/9ScASp//LrbZ4bHrV5218CpUF1bgXPelQXWsH8VJJaS2haT8fzh/0XBpWaL0Bp3ndwxWXxWnyh1zfevcE8vLDM59FaQ9Tm5PPxWvxgsCZ08ZifG+Wy/hp84MMf8gXY81WfY4nWBnlUIbv/e5vzXa3t+1dJmg2+lynP+2O0Kn4ePhiMATJ5X5+bwZi7Z0k/TeI3agbr0Plg0Dw67L+fK6S+3X1+yteF6Uo85q7O+fpWn/F5JljArAfVeariP0M7jRahpXzBDyiPBvPogZHlNu/J+jJEa7i1oE1Li8EEUpKCub4ujQ9xNvALPwAAAAAAAAAAAAAAAEAX4YUfAAAAAAAAAAAAAAAAoIvwwg8AAAAAAAAAAAAAAADQRXjhBwAAAAAAAAAAAAAAAOgivPADAAAAAAAAAAAAAAAAdJEz8sJPkiT/MUmSLydJMpkkSS1JktkkSR5LkuS3kyRZ/jL73JgkyedPbVtLkuTJJEl+PUmSnjNRJgAAAAAAAAAAAAAAAOBCdKZ+4edfSCpK+qKkP5H0YUktSb8j6ckkScZeunGSJD8s6V5JN0m6Q9L/Iykn6b2SPnqGygQAAAAAAAAAAAAAAABccDJn6FWsDsMAABnSSURBVDh9nU5n4Tv/MkmS35f0bkn/VtIvn/q7Pkn/TdKipNd2Op1vnfr735J0j6QfT5Lk7Z1O55y++LNb+21+99TD4TG+9sh9Np86OBEcoWTTTKtg8x8eu8rm9WLVn37Yx5JUO+HzzLzP0+D3nMp1n7eC/NjxIzafmj7gDyDpwHO7bZ7J5m3eqPtC9vf12Xy45G/EmqEBm08dCO7zUvT4z1ittn2+WLP5xanff8X4Kpt3gw2bX2nzY9P+OZOkb+96zOYrRtb4A/T4dzxrNf+sXrv9Ops/89gOm6eKf8Bt08aNNm8G+5dnZm0+ENSXbME/67WaL8Hrb32TzSVJ/nFXKl9n3/4jb7V5rea79pmZGZt/5gvfsnm7PWdzSao1/WfYtfN5m48Or7Z5vtmwecU/Bqo2fV1op/3+ABeAdbmV4Tbvfff7/gFKAuBMeOSe3znXRThtfVuLfoNcNB4MJh7y/d/Wt78jyJfy/8oEk5OgDNEg4XU/tNXv3mj5vObH5OpfwvJAedrnU8GYNuPHWtktfgwwnPdz4NduGfLnl6R01MYPP+Tn6fd/za8F9Jf8PP6f/cpP2nzLtZfbfPejT9hckj78gY/Z/F0/9Wab73okPMV576/+r6+e6yLgH8TUuS7AGbDnXBfgrPvt3/7tc12E01ZrLfoNgnWtRrB7j3wfnqn7SW5JwQKtpIJ8H1w/7ufx9WCYkA/yi1f6PvpYzY+T6jVffklKm/46zjX9ms7E7Hd9vfG3TC3mbD62bbPNS7f8qs1zW263uSSNrBuzeV8w1V/sDfKsz4OhXKgeDEclacHfhtP2wpQfz1arvi4UCv45uKjox6snj+HnPoVgfbJQ8BUuk/U3Ms34/QuF4EGQNFf286+Bfv+dx0V9fsz85rfeZvNnNvv69tA3/Zh58YmdNpcknYi+v/Prk2df0PDmxuNDNKLvjqI8elbO9TXqfuPXxF/UTk/79fr8sF/PKE/7xjnN+f0bed+/Zku+XcwE35dIUiPrn6Wjc34c0e7xg7HqTMXm/+mP/4PNZ+Z9XXl28lmbS9LiCX+MgUHfd9Ta/kIeX/DjxWzLtynF4WDdTlK66I+RrfuBSL7hn5XyMX+fFHx/lwv6t8xo/BlPzJ6B79zPgjPyCz/f62WfUz5+6s9NL/m7H5e0QtJHX3zZ5yXH+M1T//lLZ6JcAAAAAAAAAAAAAAAAwIXmTP2TXi/nLaf+fPIlf/e6U39+4Xtsf6+kE5JuTJLkNN8XBwAAAAAAAAAAAAAAAC48Z+qf9JIkJUnyryQtk9Qv6VpJr9HJl33e85LNLj3153f91nen02klSXJA0uWS1kt6Ojjfy/3Q9Ja/W8kBAAAAAAAAAAAAAACA7nBGX/iR9K8kvfRfiv2CpHd1Op0XXvJ3/af+LL/MMV78+4EzXDYAAAAAAAAAAAAAAACg653RF346nc6oJCVJslLSjTr5yz6PJUny5k6n8+iZPNep813zvf7+1C//XH2mzwcAAAAAAAAAAAAAAACca+nZOGin0znS6XTukPQGScslffAl8Yu/4NP/XTv+7b+fOxtlAwAAAAAAAAAAAAAAALrZWXnh50WdTmdC0k5JlydJMnzqr5859efm79w+SZKMpHFJLUn7z2bZAAAAAAAAAAAAAAAAgG50Rv9Jr5ex+tSfi6f+vEfSOyS9UdJHvmPbmyRdJOneTqdT/wco28v6/KO7bP7p+78RHqPaU7N5Zd7v36hVbD6YD97XGvaXsDjk81RNf3xJk4fbNh/ry9u8dtwfv1ZftHn5YMPm8zOHbN5f9OWTpEqlavNM1pehMu9v9MTEhM1ftX27zS/fOGLzR/bttLkkpT2+KSj1l2xeyA7Z/OjkpM1bwbuHq8ZW27wb3HLDxee6CF3hv3/qE+e6CMD3heklbON7FykaqL3vni/a/Jded5vNC8HxzwTfO518A90ZP1MFeRkzS9jG99BSPNLxgqGalp3m8Zcy4D/dz3C6zvY1kKSHK37Mu6HUcwbOcn77/V/7K5u/8bbrbb56ZdHmjbqv0Y/vutPmF6+LWkVp+8YBv0EumF+1glZnyJeh+ZSfV2Q3bvHH37XX55Ke+MaTNn98t29Z/5efeqfNC9tu9QU4dJ+N52b8HFqS7n/gXpv/2Qe+ZPO3/+QbbP4Dr7zC5uu2r7e5Bv2zvPnm4D5K+veX/Ybf4OLgWX3kGZ8DwPeZ9qJf/6wHM4d6Oxj1Nvza4+qBnM2HR3r98SXl5NepDx3y44j+/KjN+0b8zCRb8P1bXyuY2YwsYYY468cBU5NHbP7MTLDWvv6HbHztVT9h8/xWP87JrBm2uST1B134RcFliua4jWi4GjzK7WD/xkJQAEn1s/2t0Kz/hyVO1HwBTuT9OvpcbzyDLQT1oVDI2jwXlCGb9Xkm44+fL8S/EzA97VeWSqWX+wc+ThpfN2bzStW3i5Jvl7detdXm02PrguNLR6b8d0s6GKwqzft2V9ngOrf9Z1TQN+mFWZ9Liv+hlajViL9DxOkpjUUrf9Lg2KDN8/JrSu3Wd/0+x99Sbfo2o97yz0FBwf7VaOVNatX997CbUr9eshjUp6Mzvr4cmj5o83LwfX46FK/rLR9bY/PSgH8WpmZ9uzywxt+HdeP+Gkbnl6T6rG8zpib8dV6o+f2LGT8mXsz5drUn9f1jox60q5KqFf8sniun/Qs/SZJsTpLku3rvJEnSJEl+Xye/L3qg0+kcOxV9Qie/v3h7kiTXvmT7Xkm/d+o//+x0ywUAAAAAAAAAAAAAAABciM7EL/y8SdIfJElyn6QDko5KWinpZknrJU1J+vkXN+50OvNJkvy8Tr7489UkST4qaVbSWyVdeurvP3YGygUAAAAAAAAAAAAAAABccM7ECz9fkrRR0mskbZc0IKkqabekD0n6z51O52/9RlOn07kzSZKbJf07ST8mqVfSXkn/8tT2nTNQLgAAAAAAAAAAAAAAAOCCc9ov/HQ6nR2SfvXvsd/9OvnrQAAAAAAAAAAAAAAA4H+2d+dBlt1VHcC/Z9KTmck4mWQyjBNIZCAIEZcAcYlJCQZK3NcKLiWKa8AS3KDKco24FVUKLsQSEQUKrELKjUJxqWKRTVFDuSBEE0xQYmI2ZpLMJJPp9M8/3huq6fT8bndP97y5nc+nquv1u+cu563n/u477z6AFdoy6wQAAAAAAAAAAICV0/ADAAAAAAAAAAAjouEHAAAAAAAAAABGZG7WCZyuDh+9uBv/tw+8fnAdB568qxvfu2NvN75jz85u/Nyt53TjF+y6sBvfM7ejG9+Sw914kswvLHTjRx/a1o0fun9g/emv/5b/ubG/goWD3fBjH91/DJLkfVv6fXGHDva3MeSeT9zdjV+wt/847d/Zv492bDk6mMPhgd6/8/b276f9u/rP1WPHzu/GL3rME7rxRw88lwEead78weu78fvv7xfYffv2D27jrqO3d+O33H5LN/6US57ajfer27AfeM0rBuc5d0+/fj04cD9d/e3PX1VOS334wX78yWf24295398PbuP2e27rb+Ozn9yN33TzTd34Yw88tht/ymf095kf140m188PzJDkn67r3w8LA/uL3/9Fl3fj7723vy/20Y/29zeHbsJH/v2GgTmSL/mSp3fj+x4B+0J79/bfM/bsuaQbXzjW3+c9eGf/tbJ9y75ufO+OFTwGOwfGFvv39ONz/RyGbL2s/3rP9oHt7x16xSaX7L20G7/wxv57yt139A9BPCb9cUW29B+H227p16YkefBofxtPeeqTuvHPv+yybnz/3v5xgNw7MH7ceawfP3hvP54kn9F/vST3Da8DgE86utDf4zs6398PGTq+ueVof1xy5rH+e/++s4cP8V/w6f3jyHmon8MDAzmePVTDd57dj3/ao/vxg4f68SQ5p/84XHT247vx3Yf7+0oLT/z2bvzMS76qG9+1pz8K3jV8mDrzW/vxIwO7EUcHDhMvDIxhM7D+oeUfGj5MnaEUangVfbf3x385c+BoxY7+g/DQjuHX4307+p+73Let/5lKDWxibmt/hjPm+vG5rf33rCTZsaOf47l7+w/2loHPlR64v7/8/9zef1+89+jAbRh4LifJ9rn+QZsH9g28b+0e+PDrjDP68cMDyx8ceC6fNXDQKUmODLw3ZyCHgfo2fMSEIX/yE2+ddQqcIkeygn2dDXTww7PdPifHGX4AAAAAAAAAAGBENPwAAAAAAAAAAMCIaPgBAAAAAAAAAIAR0fADAAAAAAAAAAAjouEHAAAAAAAAAABGRMMPAAAAAAAAAACMiIYfAAAAAAAAAAAYkblZJ3C6+unLLzrpddy1DnmcjDfPePusj99782tPKj4G//nWWWcAMC7fculnzTqFR4SXP/cFs06BdXD1rBNYgZef5PLXXHPNuuQxS8//haHX28F++K7/7YYfc89CN37Jwv7++hf6yydJDt/fj3/s7n78/J39+Fk7+vG5bf340PB/+76B5ZPs6N/GPU8YyGF+98AGBu6j/Rd2wxd/096B9ScXHzrcjT/+7e/txndv69+P2/YM3I/bzujHM/A8WBfnnIJtAGweCwvHuvH5Y/Mntf4t80f723+gXx/n8uDgNs7e1d+P2L2zXxt2bu3fB9kxsJ8yP7D8/QP7Ueec2Y8nyY6zu+E9u3Z14/sPP7Ubv2Hu8d347m39+2DPQIl/cOAuSIbvpnsHngoL/adaMvAwDX57fOClMH/soaE15OhCf19p++AaBswfGogP3MlHBvbltq/gubptYJ/5zP7+Ztu6tRs/Ntdf/tjWgUdycH81uX97//l++J7+/Xj4zv4++QP39Z+sBweGZw8NDd+OreAFt5IXZc/AczkDt3HwBT+U3/DDmMwNPF/nh171Q/H+45wV1C8AhjnDDwAAAAAAAAAAjIiGHwAAAAAAAAAAGBENPwAAAAAAAAAAMCIafgAAAAAAAAAAYEQ0/AAAAAAAAAAAwIho+AEAAAAAAAAAgBHR8AMAAAAAAAAAACOi4QcAAAAAAAAAAEakWmuzzmHdVdV1559//tOuvvrqWacCAAAAAAAAAAAP8+pXvzq33nrrB1trl652WWf4AQAAAAAAAACAEdHwAwAAAAAAAAAAI6LhBwAAAAAAAAAARkTDDwAAAAAAAAAAjIiGHwAAAAAAAAAAGBENPwAAAAAAAAAAMCIafgAAAAAAAAAAYEQ0/AAAAAAAAAAAwIho+AEAAAAAAAAAgBHR8AMAAAAAAAAAACOi4QcAAAAAAAAAAEZEww8AAAAAAAAAAIyIhh8AAAAAAAAAABgRDT8AAAAAAAAAADAi1VqbdQ7rrqrumpub2/OoRz1q1qkAAAAAAAAAAMDD3HHHHZmfn7+7tXbeapfdrA0/NyU5O8nN00kXTy+vn0lCAGxGagsAG0F9AWC9qS0ArDe1BYCNoL7wSHUgyT2ttcetdsFN2fCzVFVdlySttUtnnQsAm4PaAsBGUF8AWG9qCwDrTW0BYCOoL7B6W2adAAAAAAAAAAAAsHIafgAAAAAAAAAAYEQ0/AAAAAAAAAAAwIho+AEAAAAAAAAAgBHR8AMAAAAAAAAAACNSrbVZ5wAAAAAAAAAAAKyQM/wAAAAAAAAAAMCIaPgBAAAAAAAAAIAR0fADAAAAAAAAAAAjouEHAAAAAAAAAABGRMMPAAAAAAAAAACMiIYfAAAAAAAAAAAYEQ0/AAAAAAAAAAAwIpu64aeqLqiq36+q/62qo1V1c1X9elWdO+vcADh9TetFO8HfbSdY5vKqeltV3V1V91fVv1bVj1TVGac6fwBmp6quqqpXVtV7quqeae1448Ayq64hVfU1VfWuqjpUVfdV1Qeq6nnrf4sAmLXV1JaqOtAZy7SqelNnO8+rqn+Y1pVD0zrzNRt3ywCYlao6r6q+r6r+tKpunI5DDlXVe6vqe6tq2c+OjF0AOJHV1hZjF1gfc7NOYKNU1UVJ3p9kX5K3JLk+yRcm+eEkX1FVV7TW7pphigCc3g4l+fVlpt+3dEJVfX2SP07yQJI/THJ3kq9N8mtJrkjynI1LE4DTzE8nuSSTevHxJBf3Zl5LDamqFyZ5ZZK7krwxyYNJrkryuqr63NbaS9brxgBwWlhVbZn6lyR/tsz0Dy03c1X9apIXT9f/u0nOTPKtSd5aVS9qrV27hrwBOH09J8lvJ7k1yTuT/HeST0/yTUlek+Qrq+o5rbV2fAFjFwAGrLq2TBm7wEmoh7+mNoeq+uskz07yQ621Vy6a/ookP5rkd1prL5hVfgCcvqrq5iRprR1YwbxnJ7kxye4kV7TW/mk6fXuSdyT54iTf1lo7YTc6AJtHVV2ZyQGHG5M8I5MDHH/QWnvuMvOuuoZU1YFMvsxwOMmlrbWbp9PPTfKPSS5Kcnlr7e825hYCcKqtsrYcSHJTkte31r5rheu/PMn7knw0yRe01j6xaF3XJdmZ5OLjNQeA8auqZ2by/v4XrbWFRdP3J/mHJBcmuaq19sfT6cYuAHStobYciLELnLRN+ZNe07P7PDvJzUl+a0n4mkx2ML+jqnae4tQA2HyuSvKoJG86frAjSVprD2TyTdwk+YFZJAbAqddae2dr7YZlvq20nLXUkO9Jsi3JtYsPXkwPcPzy9KovNgBsIqusLWtxvG780vED5tPt3pzJcbVtSb57g7YNwAy01t7RWnvr4g9kp9NvS/Kq6dUvXRQydgGgaw21ZS2MXWCJTdnwk+TK6eXfLPOmcm8mnX9nJbnsVCcGwGhsq6rnVtVPVtUPV9WVJ/g98mdOL/9qmdi7kxxJcnlVbduwTAEYq7XUkN4yf7lkHgAeuR5dVc+fjmeeX1Wf15lXbQFgsWPTy/lF04xdADgZy9WW44xd4CTMzTqBDfKk6eV/niB+QyZnAHpikrefkowAGJv9Sd6wZNpNVfXdrbW/XTTthDWntTZfVTcl+ewkj0/ykQ3JFICxWksN6S1za1UdTnJBVZ3VWjuyATkDMA5fNv37pKp6V5Lntdb+e9G0nUkek+S+1tqty6znhunlEzcoTwBOI1U1l+Q7p1cXf5hq7ALAmnRqy3HGLnASNusZfnZPLw+dIH58+jmnIBcAxue1SZ6VSdPPziSfm+R3khxI8pdVdcmiedUcANZqLTVkpcvsPkEcgM3tSJJfSHJpknOnf89I8s5MTp//9iU/cW88A8BiL0vyOUne1lr760XTjV0AWKsT1RZjF1gHm7XhBwDWrLX20unvzf5fa+1Ia+1DrbUXJHlFkh1Jfm62GQIAADxca+321trPttY+2Fo7OP17dyZnuv5Akick+b7ZZgnA6aiqfijJi5Ncn+Q7ZpwOAJtAr7YYu8D62KwNP0Od4cenHzwFuQCwebxqevn0RdPUHADWai01ZKXLnOjbTgA8ArXW5pO8ZnrVeAaAT1FVL0zyG0k+nOTK1trdS2YxdgFgVVZQW5Zl7AKrs1kbfv5jenmi3+j7zOnlw347FgA67pheLj6N5AlrzvS3aR+XZD7Jf21sagCM0FpqSG+Z8zOpUR9vrR1Z31QB2AQeNp5prR1OckuST5vWkaUcQwPY5KrqR5K8MsmHMvlA9rZlZjN2AWDFVlhbeoxdYIU2a8PPO6eXz66qT7mNVbUryRWZ/C7g35/qxAAYtcuml4sPXrxjevkVy8z/9CRnJXl/a+3oRiYGwCitpYb0lvnKJfMAwGLLjWcStQXgEauqfjzJryX550w+kL39BLMauwCwIquoLT3GLrBCm7Lhp7X20SR/k+RAkh9cEn5pJt2Ab5h2AgLAJ1XVZ1XVzmWmH0hy7fTqGxeF/ijJnUm+tao+f9H825P84vTqb29IsgCM3VpqyGuTHE3ywmltOr7MuUl+cnr1VQHgEamqnrb0y2/T6c9K8qPTq29cEj5eN35qWk+OL3Mgk+NqRzOpPwBsIlX1M0leluS6JM9qrd3Zmd3YBYBBq6ktxi6wPqq1NuscNkRVXZTk/Un2JXlLko8k+aIkV2ZyKq/LW2t3zS5DAE5HVfVzSV6c5N1JPpbk3iQXJfnqJNuTvC3JN7bWHly0zDdkcuDjgSRvSnJ3kq9L8qTp9G9um7XgAvAppjXhG6ZX9yf58ky+jfSe6bQ7W2svWTL/qmpIVb0oyW8muSvJHyZ5MMlVSS5I8vLF6wdg/FZTW6rqXZmcyv79ST4+jX9ekmdO//+Z1trxD2YXb+PlSX5suswfJTkzybckOS/Ji1pr1y5dBoDxqqrnJXldkocy+cmVQ8vMdnNr7XWLljF2AeCEVltbjF1gfWzahp8kqaoLk/x8Jqf1Oi/JrUn+NMlLW2ufmGVuAJyequoZSV6Q5KmZHEzfmeRgJqeffEMmZ4h7WPGsqiuS/FSSL86kMejGJL+f5Ddbaw+dmuwBmLVp4+g1nVk+1lo7sGSZVdeQqvraJC9J8rRMztz64STXttZef5I3AYDTzGpqS1V9b5JvTPI5SfYm2Zrk/5L8XSZ14j0nWklVfVcm34p9cpKFJB9M8iuttT8/6RsBwGllBbUlSf62tfalS5YzdgFgWautLcYusD42dcMPAAAAAAAAAABsNg/7XTwAAAAAAAAAAOD0peEHAAAAAAAAAABGRMMPAAAAAAAAAACMiIYfAAAAAAAAAAAYEQ0/AAAAAAAAAAAwIhp+AAAAAAAAAABgRDT8AAAAAAAAAADAiGj4AQAAAAAAAACAEdHwAwAAAAAAAAAAI6LhBwAAAAAAAAAARkTDDwAAAAAAAAAAjIiGHwAAAAAAAAAAGBENPwAAAAAAAAAAMCIafgAAAAAAAAAAYEQ0/AAAAAAAAAAAwIho+AEAAAAAAAAAgBHR8AMAAAAAAAAAACPy/5WT9swwppB9AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "image/png": {
              "width": 1150,
              "height": 179
            },
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OKFeRNm7mJA"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Running the Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OONRZVaZtP0f"
      },
      "source": [
        "import time\n",
        "\n",
        "def main():\n",
        "    global args, best_prec1\n",
        "    \n",
        "    # Check the save_dir exists or not\n",
        "    if not os.path.exists(args.save_dir):\n",
        "        os.makedirs(args.save_dir)\n",
        "\n",
        "    model = model =globals()[\"resnet20\"]()\n",
        "    model.cuda()\n",
        "\n",
        "\n",
        "    # define loss function (criterion) and pptimizer\n",
        "    criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "    if args.half:\n",
        "        print('half persicion is used.')\n",
        "        model.half()\n",
        "        criterion.half()\n",
        "\n",
        "    optimizer = torch.optim.SGD(model.parameters(), args.lr,\n",
        "                                momentum=args.momentum,\n",
        "                                weight_decay=args.weight_decay)\n",
        "\n",
        "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "                                                        milestones=[100, 150], last_epoch=args.start_epoch - 1)\n",
        "\n",
        "    if args.arch in ['resnet1202']:\n",
        "        # for resnet1202 original paper uses lr=0.01 for first 400 minibatches for warm-up\n",
        "        # then switch back. In this setup it will correspond for first epoch.\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = args.lr*0.1\n",
        "\n",
        "\n",
        "    if args.evaluate:\n",
        "        print('evalution mode')\n",
        "        model.load_state_dict(torch.load(os.path.join(args.save_dir, 'model.th')))\n",
        "        best_prec1 = validate(val_loader, model, criterion)\n",
        "        return best_prec1\n",
        "\n",
        "    if args.pretrained:\n",
        "        print('evalution of pretrained model')\n",
        "        args.save_dir='pretrained_models'\n",
        "        pretrained_model= args.arch +'.th'\n",
        "        model.load_state_dict(torch.load(os.path.join(args.save_dir, pretrained_model)))\n",
        "        best_prec1 = validate(val_loader, model, criterion)\n",
        "        return best_prec1\n",
        "\n",
        "    for epoch in range(args.start_epoch, args.epochs):\n",
        "\n",
        "        # train for one epoch\n",
        "        print('Training {} model'.format(args.arch))\n",
        "        print('current lr {:.5e}'.format(optimizer.param_groups[0]['lr']))\n",
        "        train(train_loader, model, criterion, optimizer, epoch)\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        # evaluate on validation set\n",
        "        prec1 = validate(val_loader, model, criterion)\n",
        "\n",
        "        # remember best prec@1 and save checkpoint\n",
        "        is_best = prec1 > best_prec1\n",
        "        best_prec1 = max(prec1, best_prec1)\n",
        "\n",
        "        if epoch > 0 and epoch % args.save_every == 0:\n",
        "            save_checkpoint(model.state_dict(), filename=os.path.join(args.save_dir, 'checkpoint.th'))\n",
        "        if is_best:\n",
        "            save_checkpoint(model.state_dict(), filename=os.path.join(args.save_dir, 'model.th'))\n",
        "\n",
        "    return best_prec1"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su50THmTmXMh"
      },
      "source": [
        "Since Google Colab imposes time restriction, we can not loop over all ResNet models to reproduce the results in one go, instead, we run each ResNet model separately by setting the name of the model (e.g. resnet20)  manually in the hyperparameters and record the results once training is finished for the specified network. We saved the trained models in a separate directory to do the inference later. One can try to run the main script several times for each network and report the mean and the variance performance for more reliable results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkntiMU2uKk6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09ef6b79-9924-41dc-c7a3-131af535476a"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "   best_prec1 = main()\n",
        "   print('The lowest error from {} model after {} epochs is {error:.3f}'.format(args.arch,args.epochs,error=100-best_prec1)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [0][0/391]\tLoss 2.8653 (2.8653)\tPrec@1 7.812 (7.812)\n",
            "Epoch: [0][55/391]\tLoss 1.8803 (2.1318)\tPrec@1 25.781 (22.907)\n",
            "Epoch: [0][110/391]\tLoss 1.7536 (1.9485)\tPrec@1 36.719 (27.949)\n",
            "Epoch: [0][165/391]\tLoss 1.5737 (1.8406)\tPrec@1 43.750 (31.796)\n",
            "Epoch: [0][220/391]\tLoss 1.4152 (1.7575)\tPrec@1 49.219 (34.972)\n",
            "Epoch: [0][275/391]\tLoss 1.4498 (1.6898)\tPrec@1 48.438 (37.582)\n",
            "Epoch: [0][330/391]\tLoss 1.2436 (1.6342)\tPrec@1 55.469 (39.707)\n",
            "Epoch: [0][385/391]\tLoss 1.3636 (1.5847)\tPrec@1 48.438 (41.562)\n",
            "Test\t  Prec@1: 45.460 (Err: 54.540 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [1][0/391]\tLoss 1.2137 (1.2137)\tPrec@1 55.469 (55.469)\n",
            "Epoch: [1][55/391]\tLoss 1.3297 (1.1998)\tPrec@1 51.562 (56.822)\n",
            "Epoch: [1][110/391]\tLoss 1.1132 (1.1716)\tPrec@1 57.812 (57.784)\n",
            "Epoch: [1][165/391]\tLoss 1.1751 (1.1446)\tPrec@1 51.562 (58.711)\n",
            "Epoch: [1][220/391]\tLoss 1.0750 (1.1259)\tPrec@1 64.062 (59.531)\n",
            "Epoch: [1][275/391]\tLoss 1.0465 (1.1063)\tPrec@1 64.062 (60.255)\n",
            "Epoch: [1][330/391]\tLoss 1.0821 (1.0805)\tPrec@1 66.406 (61.195)\n",
            "Epoch: [1][385/391]\tLoss 1.0362 (1.0600)\tPrec@1 65.625 (61.956)\n",
            "Test\t  Prec@1: 62.870 (Err: 37.130 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [2][0/391]\tLoss 0.8505 (0.8505)\tPrec@1 71.094 (71.094)\n",
            "Epoch: [2][55/391]\tLoss 0.9348 (0.8810)\tPrec@1 67.969 (69.224)\n",
            "Epoch: [2][110/391]\tLoss 0.7700 (0.8802)\tPrec@1 78.125 (69.243)\n",
            "Epoch: [2][165/391]\tLoss 0.8474 (0.8715)\tPrec@1 73.438 (69.522)\n",
            "Epoch: [2][220/391]\tLoss 0.8467 (0.8613)\tPrec@1 68.750 (69.789)\n",
            "Epoch: [2][275/391]\tLoss 0.9437 (0.8506)\tPrec@1 62.500 (70.151)\n",
            "Epoch: [2][330/391]\tLoss 0.6585 (0.8414)\tPrec@1 77.344 (70.440)\n",
            "Epoch: [2][385/391]\tLoss 0.5352 (0.8350)\tPrec@1 82.812 (70.673)\n",
            "Test\t  Prec@1: 59.810 (Err: 40.190 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [3][0/391]\tLoss 0.8367 (0.8367)\tPrec@1 70.312 (70.312)\n",
            "Epoch: [3][55/391]\tLoss 0.7777 (0.7483)\tPrec@1 70.312 (73.242)\n",
            "Epoch: [3][110/391]\tLoss 0.7537 (0.7454)\tPrec@1 73.438 (73.782)\n",
            "Epoch: [3][165/391]\tLoss 0.7774 (0.7470)\tPrec@1 75.000 (73.894)\n",
            "Epoch: [3][220/391]\tLoss 0.6523 (0.7361)\tPrec@1 79.688 (74.307)\n",
            "Epoch: [3][275/391]\tLoss 0.6406 (0.7251)\tPrec@1 78.906 (74.657)\n",
            "Epoch: [3][330/391]\tLoss 0.9939 (0.7205)\tPrec@1 71.094 (74.769)\n",
            "Epoch: [3][385/391]\tLoss 0.6959 (0.7180)\tPrec@1 72.656 (74.850)\n",
            "Test\t  Prec@1: 73.280 (Err: 26.720 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [4][0/391]\tLoss 0.5009 (0.5009)\tPrec@1 83.594 (83.594)\n",
            "Epoch: [4][55/391]\tLoss 0.7465 (0.6604)\tPrec@1 74.219 (77.037)\n",
            "Epoch: [4][110/391]\tLoss 0.6313 (0.6442)\tPrec@1 80.469 (77.576)\n",
            "Epoch: [4][165/391]\tLoss 0.6870 (0.6535)\tPrec@1 75.781 (77.038)\n",
            "Epoch: [4][220/391]\tLoss 0.5495 (0.6461)\tPrec@1 82.031 (77.453)\n",
            "Epoch: [4][275/391]\tLoss 0.5918 (0.6492)\tPrec@1 79.688 (77.344)\n",
            "Epoch: [4][330/391]\tLoss 0.6806 (0.6482)\tPrec@1 78.906 (77.341)\n",
            "Epoch: [4][385/391]\tLoss 0.4915 (0.6458)\tPrec@1 82.812 (77.425)\n",
            "Test\t  Prec@1: 71.720 (Err: 28.280 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [5][0/391]\tLoss 0.5883 (0.5883)\tPrec@1 77.344 (77.344)\n",
            "Epoch: [5][55/391]\tLoss 0.5292 (0.5985)\tPrec@1 82.031 (79.046)\n",
            "Epoch: [5][110/391]\tLoss 0.5824 (0.5944)\tPrec@1 80.469 (79.272)\n",
            "Epoch: [5][165/391]\tLoss 0.5466 (0.5919)\tPrec@1 82.812 (79.565)\n",
            "Epoch: [5][220/391]\tLoss 0.5481 (0.5909)\tPrec@1 78.906 (79.504)\n",
            "Epoch: [5][275/391]\tLoss 0.6974 (0.5931)\tPrec@1 75.781 (79.385)\n",
            "Epoch: [5][330/391]\tLoss 0.6017 (0.5935)\tPrec@1 80.469 (79.440)\n",
            "Epoch: [5][385/391]\tLoss 0.6772 (0.5916)\tPrec@1 77.344 (79.467)\n",
            "Test\t  Prec@1: 75.130 (Err: 24.870 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [6][0/391]\tLoss 0.7004 (0.7004)\tPrec@1 71.875 (71.875)\n",
            "Epoch: [6][55/391]\tLoss 0.4264 (0.5500)\tPrec@1 85.156 (80.887)\n",
            "Epoch: [6][110/391]\tLoss 0.6220 (0.5601)\tPrec@1 77.344 (80.434)\n",
            "Epoch: [6][165/391]\tLoss 0.6464 (0.5512)\tPrec@1 78.906 (80.855)\n",
            "Epoch: [6][220/391]\tLoss 0.5538 (0.5565)\tPrec@1 75.781 (80.628)\n",
            "Epoch: [6][275/391]\tLoss 0.6393 (0.5579)\tPrec@1 80.469 (80.653)\n",
            "Epoch: [6][330/391]\tLoss 0.4266 (0.5542)\tPrec@1 85.938 (80.799)\n",
            "Epoch: [6][385/391]\tLoss 0.5127 (0.5515)\tPrec@1 82.031 (80.920)\n",
            "Test\t  Prec@1: 78.900 (Err: 21.100 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [7][0/391]\tLoss 0.5219 (0.5219)\tPrec@1 83.594 (83.594)\n",
            "Epoch: [7][55/391]\tLoss 0.4821 (0.5167)\tPrec@1 84.375 (82.603)\n",
            "Epoch: [7][110/391]\tLoss 0.5528 (0.5229)\tPrec@1 83.594 (82.334)\n",
            "Epoch: [7][165/391]\tLoss 0.4648 (0.5178)\tPrec@1 85.156 (82.267)\n",
            "Epoch: [7][220/391]\tLoss 0.4202 (0.5189)\tPrec@1 85.938 (82.176)\n",
            "Epoch: [7][275/391]\tLoss 0.5198 (0.5158)\tPrec@1 83.594 (82.283)\n",
            "Epoch: [7][330/391]\tLoss 0.4627 (0.5174)\tPrec@1 79.688 (82.140)\n",
            "Epoch: [7][385/391]\tLoss 0.4579 (0.5190)\tPrec@1 85.938 (82.029)\n",
            "Test\t  Prec@1: 76.100 (Err: 23.900 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [8][0/391]\tLoss 0.6095 (0.6095)\tPrec@1 82.812 (82.812)\n",
            "Epoch: [8][55/391]\tLoss 0.4450 (0.4857)\tPrec@1 84.375 (82.952)\n",
            "Epoch: [8][110/391]\tLoss 0.4103 (0.4867)\tPrec@1 81.250 (83.038)\n",
            "Epoch: [8][165/391]\tLoss 0.5045 (0.4993)\tPrec@1 83.594 (82.742)\n",
            "Epoch: [8][220/391]\tLoss 0.4581 (0.5005)\tPrec@1 80.469 (82.703)\n",
            "Epoch: [8][275/391]\tLoss 0.4364 (0.4999)\tPrec@1 85.938 (82.725)\n",
            "Epoch: [8][330/391]\tLoss 0.4640 (0.4987)\tPrec@1 85.156 (82.699)\n",
            "Epoch: [8][385/391]\tLoss 0.4884 (0.4970)\tPrec@1 83.594 (82.746)\n",
            "Test\t  Prec@1: 80.310 (Err: 19.690 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [9][0/391]\tLoss 0.4445 (0.4445)\tPrec@1 88.281 (88.281)\n",
            "Epoch: [9][55/391]\tLoss 0.4562 (0.4488)\tPrec@1 82.031 (84.528)\n",
            "Epoch: [9][110/391]\tLoss 0.3888 (0.4627)\tPrec@1 85.938 (83.981)\n",
            "Epoch: [9][165/391]\tLoss 0.4324 (0.4683)\tPrec@1 82.812 (83.801)\n",
            "Epoch: [9][220/391]\tLoss 0.4881 (0.4750)\tPrec@1 85.156 (83.573)\n",
            "Epoch: [9][275/391]\tLoss 0.4386 (0.4790)\tPrec@1 84.375 (83.492)\n",
            "Epoch: [9][330/391]\tLoss 0.4767 (0.4806)\tPrec@1 80.469 (83.459)\n",
            "Epoch: [9][385/391]\tLoss 0.4852 (0.4811)\tPrec@1 85.156 (83.424)\n",
            "Test\t  Prec@1: 80.920 (Err: 19.080 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [10][0/391]\tLoss 0.5304 (0.5304)\tPrec@1 79.688 (79.688)\n",
            "Epoch: [10][55/391]\tLoss 0.3460 (0.4386)\tPrec@1 89.062 (85.073)\n",
            "Epoch: [10][110/391]\tLoss 0.7298 (0.4280)\tPrec@1 78.125 (85.410)\n",
            "Epoch: [10][165/391]\tLoss 0.4756 (0.4451)\tPrec@1 84.375 (84.681)\n",
            "Epoch: [10][220/391]\tLoss 0.3506 (0.4449)\tPrec@1 86.719 (84.630)\n",
            "Epoch: [10][275/391]\tLoss 0.2940 (0.4444)\tPrec@1 89.844 (84.672)\n",
            "Epoch: [10][330/391]\tLoss 0.4971 (0.4505)\tPrec@1 81.250 (84.462)\n",
            "Epoch: [10][385/391]\tLoss 0.4550 (0.4519)\tPrec@1 87.500 (84.434)\n",
            "Test\t  Prec@1: 80.120 (Err: 19.880 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [11][0/391]\tLoss 0.4301 (0.4301)\tPrec@1 85.938 (85.938)\n",
            "Epoch: [11][55/391]\tLoss 0.3801 (0.4359)\tPrec@1 88.281 (84.961)\n",
            "Epoch: [11][110/391]\tLoss 0.4221 (0.4387)\tPrec@1 85.156 (84.924)\n",
            "Epoch: [11][165/391]\tLoss 0.3426 (0.4358)\tPrec@1 85.938 (84.822)\n",
            "Epoch: [11][220/391]\tLoss 0.3629 (0.4362)\tPrec@1 88.281 (84.806)\n",
            "Epoch: [11][275/391]\tLoss 0.4222 (0.4336)\tPrec@1 85.938 (84.885)\n",
            "Epoch: [11][330/391]\tLoss 0.3955 (0.4352)\tPrec@1 84.375 (84.840)\n",
            "Epoch: [11][385/391]\tLoss 0.5148 (0.4354)\tPrec@1 82.812 (84.857)\n",
            "Test\t  Prec@1: 73.410 (Err: 26.590 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [12][0/391]\tLoss 0.6258 (0.6258)\tPrec@1 82.031 (82.031)\n",
            "Epoch: [12][55/391]\tLoss 0.4104 (0.4131)\tPrec@1 87.500 (85.282)\n",
            "Epoch: [12][110/391]\tLoss 0.3924 (0.4111)\tPrec@1 84.375 (85.698)\n",
            "Epoch: [12][165/391]\tLoss 0.4823 (0.4169)\tPrec@1 83.594 (85.537)\n",
            "Epoch: [12][220/391]\tLoss 0.4864 (0.4212)\tPrec@1 83.594 (85.319)\n",
            "Epoch: [12][275/391]\tLoss 0.4051 (0.4209)\tPrec@1 84.375 (85.323)\n",
            "Epoch: [12][330/391]\tLoss 0.5617 (0.4239)\tPrec@1 78.125 (85.147)\n",
            "Epoch: [12][385/391]\tLoss 0.3635 (0.4272)\tPrec@1 89.062 (85.071)\n",
            "Test\t  Prec@1: 81.840 (Err: 18.160 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [13][0/391]\tLoss 0.5246 (0.5246)\tPrec@1 82.812 (82.812)\n",
            "Epoch: [13][55/391]\tLoss 0.4891 (0.4079)\tPrec@1 80.469 (86.007)\n",
            "Epoch: [13][110/391]\tLoss 0.4329 (0.4093)\tPrec@1 85.156 (86.015)\n",
            "Epoch: [13][165/391]\tLoss 0.4446 (0.4110)\tPrec@1 86.719 (85.754)\n",
            "Epoch: [13][220/391]\tLoss 0.4328 (0.4147)\tPrec@1 82.812 (85.630)\n",
            "Epoch: [13][275/391]\tLoss 0.4202 (0.4139)\tPrec@1 84.375 (85.700)\n",
            "Epoch: [13][330/391]\tLoss 0.3493 (0.4124)\tPrec@1 89.062 (85.768)\n",
            "Epoch: [13][385/391]\tLoss 0.3282 (0.4128)\tPrec@1 89.844 (85.729)\n",
            "Test\t  Prec@1: 83.720 (Err: 16.280 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [14][0/391]\tLoss 0.4792 (0.4792)\tPrec@1 84.375 (84.375)\n",
            "Epoch: [14][55/391]\tLoss 0.2972 (0.4022)\tPrec@1 89.062 (85.965)\n",
            "Epoch: [14][110/391]\tLoss 0.4697 (0.3992)\tPrec@1 83.594 (86.191)\n",
            "Epoch: [14][165/391]\tLoss 0.4871 (0.3986)\tPrec@1 86.719 (86.295)\n",
            "Epoch: [14][220/391]\tLoss 0.3895 (0.4026)\tPrec@1 88.281 (86.213)\n",
            "Epoch: [14][275/391]\tLoss 0.3820 (0.3999)\tPrec@1 87.500 (86.348)\n",
            "Epoch: [14][330/391]\tLoss 0.2896 (0.4055)\tPrec@1 91.406 (86.145)\n",
            "Epoch: [14][385/391]\tLoss 0.3775 (0.4066)\tPrec@1 89.062 (86.124)\n",
            "Test\t  Prec@1: 73.220 (Err: 26.780 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [15][0/391]\tLoss 0.4953 (0.4953)\tPrec@1 83.594 (83.594)\n",
            "Epoch: [15][55/391]\tLoss 0.2826 (0.3830)\tPrec@1 86.719 (86.579)\n",
            "Epoch: [15][110/391]\tLoss 0.3479 (0.3942)\tPrec@1 83.594 (86.043)\n",
            "Epoch: [15][165/391]\tLoss 0.4901 (0.3942)\tPrec@1 80.469 (86.267)\n",
            "Epoch: [15][220/391]\tLoss 0.3426 (0.3895)\tPrec@1 87.500 (86.471)\n",
            "Epoch: [15][275/391]\tLoss 0.4829 (0.3924)\tPrec@1 82.031 (86.416)\n",
            "Epoch: [15][330/391]\tLoss 0.4988 (0.3964)\tPrec@1 84.375 (86.270)\n",
            "Epoch: [15][385/391]\tLoss 0.3356 (0.3980)\tPrec@1 85.938 (86.213)\n",
            "Test\t  Prec@1: 82.790 (Err: 17.210 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [16][0/391]\tLoss 0.3764 (0.3764)\tPrec@1 85.156 (85.156)\n",
            "Epoch: [16][55/391]\tLoss 0.5043 (0.3540)\tPrec@1 82.031 (87.486)\n",
            "Epoch: [16][110/391]\tLoss 0.5435 (0.3665)\tPrec@1 82.812 (87.317)\n",
            "Epoch: [16][165/391]\tLoss 0.3867 (0.3729)\tPrec@1 86.719 (87.189)\n",
            "Epoch: [16][220/391]\tLoss 0.3059 (0.3755)\tPrec@1 89.844 (86.994)\n",
            "Epoch: [16][275/391]\tLoss 0.3018 (0.3808)\tPrec@1 88.281 (86.812)\n",
            "Epoch: [16][330/391]\tLoss 0.4286 (0.3830)\tPrec@1 85.156 (86.662)\n",
            "Epoch: [16][385/391]\tLoss 0.4273 (0.3809)\tPrec@1 85.156 (86.692)\n",
            "Test\t  Prec@1: 84.500 (Err: 15.500 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [17][0/391]\tLoss 0.3846 (0.3846)\tPrec@1 85.938 (85.938)\n",
            "Epoch: [17][55/391]\tLoss 0.3962 (0.3864)\tPrec@1 85.938 (86.412)\n",
            "Epoch: [17][110/391]\tLoss 0.3886 (0.3860)\tPrec@1 88.281 (86.479)\n",
            "Epoch: [17][165/391]\tLoss 0.2875 (0.3801)\tPrec@1 89.844 (86.723)\n",
            "Epoch: [17][220/391]\tLoss 0.4102 (0.3747)\tPrec@1 89.844 (86.963)\n",
            "Epoch: [17][275/391]\tLoss 0.3489 (0.3752)\tPrec@1 89.062 (86.996)\n",
            "Epoch: [17][330/391]\tLoss 0.3936 (0.3770)\tPrec@1 84.375 (86.926)\n",
            "Epoch: [17][385/391]\tLoss 0.3437 (0.3738)\tPrec@1 85.938 (87.008)\n",
            "Test\t  Prec@1: 83.170 (Err: 16.830 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [18][0/391]\tLoss 0.3318 (0.3318)\tPrec@1 88.281 (88.281)\n",
            "Epoch: [18][55/391]\tLoss 0.4076 (0.3677)\tPrec@1 88.281 (87.514)\n",
            "Epoch: [18][110/391]\tLoss 0.5193 (0.3677)\tPrec@1 79.688 (87.380)\n",
            "Epoch: [18][165/391]\tLoss 0.4025 (0.3628)\tPrec@1 84.375 (87.566)\n",
            "Epoch: [18][220/391]\tLoss 0.2113 (0.3627)\tPrec@1 92.969 (87.415)\n",
            "Epoch: [18][275/391]\tLoss 0.2776 (0.3612)\tPrec@1 90.625 (87.443)\n",
            "Epoch: [18][330/391]\tLoss 0.4544 (0.3662)\tPrec@1 85.156 (87.271)\n",
            "Epoch: [18][385/391]\tLoss 0.1950 (0.3668)\tPrec@1 94.531 (87.180)\n",
            "Test\t  Prec@1: 82.080 (Err: 17.920 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [19][0/391]\tLoss 0.4521 (0.4521)\tPrec@1 84.375 (84.375)\n",
            "Epoch: [19][55/391]\tLoss 0.3360 (0.3641)\tPrec@1 86.719 (87.430)\n",
            "Epoch: [19][110/391]\tLoss 0.2445 (0.3591)\tPrec@1 90.625 (87.479)\n",
            "Epoch: [19][165/391]\tLoss 0.3532 (0.3609)\tPrec@1 86.719 (87.401)\n",
            "Epoch: [19][220/391]\tLoss 0.4395 (0.3586)\tPrec@1 84.375 (87.401)\n",
            "Epoch: [19][275/391]\tLoss 0.4834 (0.3587)\tPrec@1 82.031 (87.503)\n",
            "Epoch: [19][330/391]\tLoss 0.3019 (0.3580)\tPrec@1 88.281 (87.542)\n",
            "Epoch: [19][385/391]\tLoss 0.3416 (0.3588)\tPrec@1 86.719 (87.486)\n",
            "Test\t  Prec@1: 83.680 (Err: 16.320 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [20][0/391]\tLoss 0.2069 (0.2069)\tPrec@1 92.188 (92.188)\n",
            "Epoch: [20][55/391]\tLoss 0.3990 (0.3335)\tPrec@1 82.812 (87.821)\n",
            "Epoch: [20][110/391]\tLoss 0.4116 (0.3365)\tPrec@1 84.375 (88.225)\n",
            "Epoch: [20][165/391]\tLoss 0.3915 (0.3424)\tPrec@1 81.250 (87.900)\n",
            "Epoch: [20][220/391]\tLoss 0.4364 (0.3507)\tPrec@1 82.812 (87.687)\n",
            "Epoch: [20][275/391]\tLoss 0.3156 (0.3495)\tPrec@1 89.844 (87.806)\n",
            "Epoch: [20][330/391]\tLoss 0.3992 (0.3499)\tPrec@1 84.375 (87.800)\n",
            "Epoch: [20][385/391]\tLoss 0.3267 (0.3526)\tPrec@1 85.938 (87.666)\n",
            "Test\t  Prec@1: 83.850 (Err: 16.150 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [21][0/391]\tLoss 0.3440 (0.3440)\tPrec@1 87.500 (87.500)\n",
            "Epoch: [21][55/391]\tLoss 0.3306 (0.3309)\tPrec@1 89.062 (88.728)\n",
            "Epoch: [21][110/391]\tLoss 0.5922 (0.3348)\tPrec@1 80.469 (88.535)\n",
            "Epoch: [21][165/391]\tLoss 0.4007 (0.3401)\tPrec@1 84.375 (88.324)\n",
            "Epoch: [21][220/391]\tLoss 0.4486 (0.3450)\tPrec@1 85.156 (88.200)\n",
            "Epoch: [21][275/391]\tLoss 0.2435 (0.3493)\tPrec@1 90.625 (88.049)\n",
            "Epoch: [21][330/391]\tLoss 0.4345 (0.3500)\tPrec@1 85.938 (88.019)\n",
            "Epoch: [21][385/391]\tLoss 0.3166 (0.3505)\tPrec@1 89.844 (87.974)\n",
            "Test\t  Prec@1: 84.430 (Err: 15.570 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [22][0/391]\tLoss 0.2005 (0.2005)\tPrec@1 89.844 (89.844)\n",
            "Epoch: [22][55/391]\tLoss 0.4680 (0.3230)\tPrec@1 81.250 (88.630)\n",
            "Epoch: [22][110/391]\tLoss 0.2749 (0.3331)\tPrec@1 93.750 (88.577)\n",
            "Epoch: [22][165/391]\tLoss 0.3193 (0.3333)\tPrec@1 88.281 (88.564)\n",
            "Epoch: [22][220/391]\tLoss 0.3458 (0.3336)\tPrec@1 85.156 (88.529)\n",
            "Epoch: [22][275/391]\tLoss 0.3902 (0.3362)\tPrec@1 82.031 (88.318)\n",
            "Epoch: [22][330/391]\tLoss 0.3640 (0.3354)\tPrec@1 88.281 (88.326)\n",
            "Epoch: [22][385/391]\tLoss 0.4901 (0.3353)\tPrec@1 85.938 (88.362)\n",
            "Test\t  Prec@1: 84.570 (Err: 15.430 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [23][0/391]\tLoss 0.2851 (0.2851)\tPrec@1 90.625 (90.625)\n",
            "Epoch: [23][55/391]\tLoss 0.2961 (0.3299)\tPrec@1 88.281 (88.058)\n",
            "Epoch: [23][110/391]\tLoss 0.3176 (0.3329)\tPrec@1 89.062 (88.281)\n",
            "Epoch: [23][165/391]\tLoss 0.3454 (0.3350)\tPrec@1 91.406 (88.404)\n",
            "Epoch: [23][220/391]\tLoss 0.2391 (0.3360)\tPrec@1 93.750 (88.281)\n",
            "Epoch: [23][275/391]\tLoss 0.2936 (0.3370)\tPrec@1 89.062 (88.276)\n",
            "Epoch: [23][330/391]\tLoss 0.2903 (0.3377)\tPrec@1 90.625 (88.241)\n",
            "Epoch: [23][385/391]\tLoss 0.3141 (0.3357)\tPrec@1 89.844 (88.356)\n",
            "Test\t  Prec@1: 84.980 (Err: 15.020 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [24][0/391]\tLoss 0.3040 (0.3040)\tPrec@1 89.844 (89.844)\n",
            "Epoch: [24][55/391]\tLoss 0.2549 (0.3101)\tPrec@1 90.625 (88.853)\n",
            "Epoch: [24][110/391]\tLoss 0.2418 (0.3107)\tPrec@1 92.188 (88.964)\n",
            "Epoch: [24][165/391]\tLoss 0.4416 (0.3152)\tPrec@1 84.375 (88.964)\n",
            "Epoch: [24][220/391]\tLoss 0.3079 (0.3186)\tPrec@1 92.188 (88.928)\n",
            "Epoch: [24][275/391]\tLoss 0.3528 (0.3190)\tPrec@1 90.625 (88.904)\n",
            "Epoch: [24][330/391]\tLoss 0.3578 (0.3225)\tPrec@1 87.500 (88.756)\n",
            "Epoch: [24][385/391]\tLoss 0.3475 (0.3279)\tPrec@1 90.625 (88.540)\n",
            "Test\t  Prec@1: 84.310 (Err: 15.690 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [25][0/391]\tLoss 0.3243 (0.3243)\tPrec@1 87.500 (87.500)\n",
            "Epoch: [25][55/391]\tLoss 0.2072 (0.3127)\tPrec@1 92.188 (88.700)\n",
            "Epoch: [25][110/391]\tLoss 0.3509 (0.3226)\tPrec@1 89.062 (88.689)\n",
            "Epoch: [25][165/391]\tLoss 0.3440 (0.3205)\tPrec@1 85.938 (88.832)\n",
            "Epoch: [25][220/391]\tLoss 0.4632 (0.3191)\tPrec@1 85.156 (88.854)\n",
            "Epoch: [25][275/391]\tLoss 0.3106 (0.3218)\tPrec@1 89.062 (88.808)\n",
            "Epoch: [25][330/391]\tLoss 0.3585 (0.3241)\tPrec@1 89.062 (88.716)\n",
            "Epoch: [25][385/391]\tLoss 0.3223 (0.3242)\tPrec@1 85.938 (88.680)\n",
            "Test\t  Prec@1: 84.800 (Err: 15.200 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [26][0/391]\tLoss 0.1678 (0.1678)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [26][55/391]\tLoss 0.3472 (0.3076)\tPrec@1 88.281 (89.453)\n",
            "Epoch: [26][110/391]\tLoss 0.3173 (0.3144)\tPrec@1 87.500 (88.978)\n",
            "Epoch: [26][165/391]\tLoss 0.2595 (0.3106)\tPrec@1 92.188 (89.255)\n",
            "Epoch: [26][220/391]\tLoss 0.2433 (0.3137)\tPrec@1 89.062 (89.144)\n",
            "Epoch: [26][275/391]\tLoss 0.2439 (0.3171)\tPrec@1 91.406 (88.978)\n",
            "Epoch: [26][330/391]\tLoss 0.2733 (0.3194)\tPrec@1 92.188 (88.956)\n",
            "Epoch: [26][385/391]\tLoss 0.5474 (0.3202)\tPrec@1 79.688 (88.973)\n",
            "Test\t  Prec@1: 80.030 (Err: 19.970 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [27][0/391]\tLoss 0.3869 (0.3869)\tPrec@1 88.281 (88.281)\n",
            "Epoch: [27][55/391]\tLoss 0.3032 (0.3228)\tPrec@1 88.281 (88.616)\n",
            "Epoch: [27][110/391]\tLoss 0.2005 (0.3066)\tPrec@1 95.312 (89.119)\n",
            "Epoch: [27][165/391]\tLoss 0.3134 (0.3061)\tPrec@1 89.062 (89.227)\n",
            "Epoch: [27][220/391]\tLoss 0.2733 (0.3089)\tPrec@1 90.625 (89.193)\n",
            "Epoch: [27][275/391]\tLoss 0.2309 (0.3089)\tPrec@1 93.750 (89.232)\n",
            "Epoch: [27][330/391]\tLoss 0.4579 (0.3104)\tPrec@1 84.375 (89.211)\n",
            "Epoch: [27][385/391]\tLoss 0.2882 (0.3139)\tPrec@1 92.188 (89.125)\n",
            "Test\t  Prec@1: 83.850 (Err: 16.150 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [28][0/391]\tLoss 0.2317 (0.2317)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [28][55/391]\tLoss 0.4256 (0.2939)\tPrec@1 85.938 (90.053)\n",
            "Epoch: [28][110/391]\tLoss 0.3338 (0.2982)\tPrec@1 89.062 (89.604)\n",
            "Epoch: [28][165/391]\tLoss 0.3223 (0.3076)\tPrec@1 89.062 (89.288)\n",
            "Epoch: [28][220/391]\tLoss 0.2082 (0.3056)\tPrec@1 92.969 (89.352)\n",
            "Epoch: [28][275/391]\tLoss 0.2448 (0.3146)\tPrec@1 89.844 (89.003)\n",
            "Epoch: [28][330/391]\tLoss 0.4144 (0.3145)\tPrec@1 88.281 (88.978)\n",
            "Epoch: [28][385/391]\tLoss 0.5131 (0.3150)\tPrec@1 85.156 (89.010)\n",
            "Test\t  Prec@1: 85.830 (Err: 14.170 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [29][0/391]\tLoss 0.3304 (0.3304)\tPrec@1 89.062 (89.062)\n",
            "Epoch: [29][55/391]\tLoss 0.2940 (0.2911)\tPrec@1 89.062 (89.788)\n",
            "Epoch: [29][110/391]\tLoss 0.3859 (0.2962)\tPrec@1 83.594 (89.604)\n",
            "Epoch: [29][165/391]\tLoss 0.3455 (0.3034)\tPrec@1 86.719 (89.472)\n",
            "Epoch: [29][220/391]\tLoss 0.3743 (0.3048)\tPrec@1 82.812 (89.285)\n",
            "Epoch: [29][275/391]\tLoss 0.3204 (0.3077)\tPrec@1 89.062 (89.162)\n",
            "Epoch: [29][330/391]\tLoss 0.2758 (0.3061)\tPrec@1 89.062 (89.242)\n",
            "Epoch: [29][385/391]\tLoss 0.2371 (0.3057)\tPrec@1 92.188 (89.301)\n",
            "Test\t  Prec@1: 84.630 (Err: 15.370 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [30][0/391]\tLoss 0.3933 (0.3933)\tPrec@1 88.281 (88.281)\n",
            "Epoch: [30][55/391]\tLoss 0.2969 (0.2889)\tPrec@1 89.844 (89.955)\n",
            "Epoch: [30][110/391]\tLoss 0.2779 (0.2997)\tPrec@1 90.625 (89.647)\n",
            "Epoch: [30][165/391]\tLoss 0.3209 (0.2997)\tPrec@1 92.188 (89.698)\n",
            "Epoch: [30][220/391]\tLoss 0.2919 (0.2961)\tPrec@1 89.844 (89.759)\n",
            "Epoch: [30][275/391]\tLoss 0.3082 (0.2995)\tPrec@1 88.281 (89.654)\n",
            "Epoch: [30][330/391]\tLoss 0.3375 (0.3024)\tPrec@1 88.281 (89.539)\n",
            "Epoch: [30][385/391]\tLoss 0.2382 (0.3029)\tPrec@1 90.625 (89.473)\n",
            "Test\t  Prec@1: 83.670 (Err: 16.330 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [31][0/391]\tLoss 0.2251 (0.2251)\tPrec@1 95.312 (95.312)\n",
            "Epoch: [31][55/391]\tLoss 0.3579 (0.2615)\tPrec@1 89.062 (90.988)\n",
            "Epoch: [31][110/391]\tLoss 0.2879 (0.2853)\tPrec@1 89.062 (90.132)\n",
            "Epoch: [31][165/391]\tLoss 0.3322 (0.2974)\tPrec@1 87.500 (89.707)\n",
            "Epoch: [31][220/391]\tLoss 0.3543 (0.2974)\tPrec@1 87.500 (89.610)\n",
            "Epoch: [31][275/391]\tLoss 0.3247 (0.2955)\tPrec@1 85.938 (89.634)\n",
            "Epoch: [31][330/391]\tLoss 0.4260 (0.2964)\tPrec@1 85.938 (89.582)\n",
            "Epoch: [31][385/391]\tLoss 0.2497 (0.2985)\tPrec@1 90.625 (89.492)\n",
            "Test\t  Prec@1: 83.650 (Err: 16.350 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [32][0/391]\tLoss 0.3006 (0.3006)\tPrec@1 90.625 (90.625)\n",
            "Epoch: [32][55/391]\tLoss 0.2318 (0.2920)\tPrec@1 93.750 (89.816)\n",
            "Epoch: [32][110/391]\tLoss 0.3078 (0.2844)\tPrec@1 89.844 (90.069)\n",
            "Epoch: [32][165/391]\tLoss 0.3008 (0.2915)\tPrec@1 89.844 (89.759)\n",
            "Epoch: [32][220/391]\tLoss 0.3369 (0.2933)\tPrec@1 89.844 (89.752)\n",
            "Epoch: [32][275/391]\tLoss 0.1507 (0.2906)\tPrec@1 96.875 (89.864)\n",
            "Epoch: [32][330/391]\tLoss 0.1524 (0.2919)\tPrec@1 93.750 (89.714)\n",
            "Epoch: [32][385/391]\tLoss 0.3041 (0.2935)\tPrec@1 87.500 (89.716)\n",
            "Test\t  Prec@1: 82.090 (Err: 17.910 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [33][0/391]\tLoss 0.3744 (0.3744)\tPrec@1 86.719 (86.719)\n",
            "Epoch: [33][55/391]\tLoss 0.2064 (0.2870)\tPrec@1 92.188 (90.067)\n",
            "Epoch: [33][110/391]\tLoss 0.2653 (0.2906)\tPrec@1 92.969 (90.006)\n",
            "Epoch: [33][165/391]\tLoss 0.3333 (0.2920)\tPrec@1 86.719 (89.877)\n",
            "Epoch: [33][220/391]\tLoss 0.1827 (0.2925)\tPrec@1 93.750 (89.914)\n",
            "Epoch: [33][275/391]\tLoss 0.2746 (0.2919)\tPrec@1 89.844 (89.915)\n",
            "Epoch: [33][330/391]\tLoss 0.2640 (0.2927)\tPrec@1 91.406 (89.820)\n",
            "Epoch: [33][385/391]\tLoss 0.2534 (0.2925)\tPrec@1 92.188 (89.858)\n",
            "Test\t  Prec@1: 84.210 (Err: 15.790 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [34][0/391]\tLoss 0.2703 (0.2703)\tPrec@1 89.844 (89.844)\n",
            "Epoch: [34][55/391]\tLoss 0.2550 (0.2637)\tPrec@1 89.844 (90.751)\n",
            "Epoch: [34][110/391]\tLoss 0.3256 (0.2838)\tPrec@1 89.062 (89.985)\n",
            "Epoch: [34][165/391]\tLoss 0.3093 (0.2847)\tPrec@1 88.281 (89.905)\n",
            "Epoch: [34][220/391]\tLoss 0.2441 (0.2921)\tPrec@1 91.406 (89.610)\n",
            "Epoch: [34][275/391]\tLoss 0.3449 (0.2953)\tPrec@1 86.719 (89.515)\n",
            "Epoch: [34][330/391]\tLoss 0.1767 (0.2947)\tPrec@1 95.312 (89.645)\n",
            "Epoch: [34][385/391]\tLoss 0.2942 (0.2936)\tPrec@1 94.531 (89.751)\n",
            "Test\t  Prec@1: 85.530 (Err: 14.470 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [35][0/391]\tLoss 0.1906 (0.1906)\tPrec@1 93.750 (93.750)\n",
            "Epoch: [35][55/391]\tLoss 0.2717 (0.2618)\tPrec@1 88.281 (90.946)\n",
            "Epoch: [35][110/391]\tLoss 0.2847 (0.2739)\tPrec@1 91.406 (90.365)\n",
            "Epoch: [35][165/391]\tLoss 0.2190 (0.2736)\tPrec@1 93.750 (90.404)\n",
            "Epoch: [35][220/391]\tLoss 0.2452 (0.2756)\tPrec@1 89.062 (90.434)\n",
            "Epoch: [35][275/391]\tLoss 0.2836 (0.2835)\tPrec@1 92.188 (90.164)\n",
            "Epoch: [35][330/391]\tLoss 0.3856 (0.2851)\tPrec@1 86.719 (90.115)\n",
            "Epoch: [35][385/391]\tLoss 0.2565 (0.2843)\tPrec@1 92.969 (90.143)\n",
            "Test\t  Prec@1: 86.470 (Err: 13.530 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [36][0/391]\tLoss 0.3107 (0.3107)\tPrec@1 89.844 (89.844)\n",
            "Epoch: [36][55/391]\tLoss 0.3317 (0.2618)\tPrec@1 90.625 (90.876)\n",
            "Epoch: [36][110/391]\tLoss 0.2082 (0.2692)\tPrec@1 92.188 (90.534)\n",
            "Epoch: [36][165/391]\tLoss 0.4027 (0.2720)\tPrec@1 85.156 (90.460)\n",
            "Epoch: [36][220/391]\tLoss 0.3192 (0.2755)\tPrec@1 88.281 (90.286)\n",
            "Epoch: [36][275/391]\tLoss 0.2441 (0.2787)\tPrec@1 91.406 (90.164)\n",
            "Epoch: [36][330/391]\tLoss 0.2803 (0.2773)\tPrec@1 90.625 (90.217)\n",
            "Epoch: [36][385/391]\tLoss 0.2032 (0.2805)\tPrec@1 93.750 (90.164)\n",
            "Test\t  Prec@1: 83.350 (Err: 16.650 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [37][0/391]\tLoss 0.3184 (0.3184)\tPrec@1 89.062 (89.062)\n",
            "Epoch: [37][55/391]\tLoss 0.2792 (0.2610)\tPrec@1 89.844 (90.932)\n",
            "Epoch: [37][110/391]\tLoss 0.2272 (0.2686)\tPrec@1 92.969 (90.611)\n",
            "Epoch: [37][165/391]\tLoss 0.2909 (0.2745)\tPrec@1 89.062 (90.413)\n",
            "Epoch: [37][220/391]\tLoss 0.3201 (0.2759)\tPrec@1 88.281 (90.374)\n",
            "Epoch: [37][275/391]\tLoss 0.3579 (0.2740)\tPrec@1 88.281 (90.452)\n",
            "Epoch: [37][330/391]\tLoss 0.2729 (0.2753)\tPrec@1 90.625 (90.377)\n",
            "Epoch: [37][385/391]\tLoss 0.3216 (0.2788)\tPrec@1 88.281 (90.263)\n",
            "Test\t  Prec@1: 83.630 (Err: 16.370 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [38][0/391]\tLoss 0.3108 (0.3108)\tPrec@1 85.156 (85.156)\n",
            "Epoch: [38][55/391]\tLoss 0.3172 (0.2725)\tPrec@1 89.062 (90.193)\n",
            "Epoch: [38][110/391]\tLoss 0.4106 (0.2714)\tPrec@1 90.625 (90.449)\n",
            "Epoch: [38][165/391]\tLoss 0.1679 (0.2717)\tPrec@1 95.312 (90.470)\n",
            "Epoch: [38][220/391]\tLoss 0.4910 (0.2758)\tPrec@1 82.812 (90.360)\n",
            "Epoch: [38][275/391]\tLoss 0.3403 (0.2751)\tPrec@1 88.281 (90.359)\n",
            "Epoch: [38][330/391]\tLoss 0.2509 (0.2763)\tPrec@1 89.844 (90.287)\n",
            "Epoch: [38][385/391]\tLoss 0.3795 (0.2763)\tPrec@1 87.500 (90.307)\n",
            "Test\t  Prec@1: 86.570 (Err: 13.430 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [39][0/391]\tLoss 0.1572 (0.1572)\tPrec@1 95.312 (95.312)\n",
            "Epoch: [39][55/391]\tLoss 0.3317 (0.2640)\tPrec@1 89.844 (91.071)\n",
            "Epoch: [39][110/391]\tLoss 0.4937 (0.2737)\tPrec@1 81.250 (90.484)\n",
            "Epoch: [39][165/391]\tLoss 0.1620 (0.2806)\tPrec@1 95.312 (90.305)\n",
            "Epoch: [39][220/391]\tLoss 0.2790 (0.2798)\tPrec@1 89.844 (90.314)\n",
            "Epoch: [39][275/391]\tLoss 0.3383 (0.2797)\tPrec@1 88.281 (90.316)\n",
            "Epoch: [39][330/391]\tLoss 0.2335 (0.2781)\tPrec@1 92.188 (90.401)\n",
            "Epoch: [39][385/391]\tLoss 0.2997 (0.2795)\tPrec@1 89.844 (90.330)\n",
            "Test\t  Prec@1: 85.660 (Err: 14.340 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [40][0/391]\tLoss 0.2943 (0.2943)\tPrec@1 91.406 (91.406)\n",
            "Epoch: [40][55/391]\tLoss 0.3443 (0.2753)\tPrec@1 85.938 (90.723)\n",
            "Epoch: [40][110/391]\tLoss 0.2639 (0.2729)\tPrec@1 88.281 (90.583)\n",
            "Epoch: [40][165/391]\tLoss 0.3205 (0.2740)\tPrec@1 91.406 (90.409)\n",
            "Epoch: [40][220/391]\tLoss 0.3130 (0.2761)\tPrec@1 87.500 (90.325)\n",
            "Epoch: [40][275/391]\tLoss 0.1562 (0.2783)\tPrec@1 94.531 (90.229)\n",
            "Epoch: [40][330/391]\tLoss 0.2593 (0.2743)\tPrec@1 91.406 (90.398)\n",
            "Epoch: [40][385/391]\tLoss 0.2748 (0.2766)\tPrec@1 86.719 (90.307)\n",
            "Test\t  Prec@1: 85.910 (Err: 14.090 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [41][0/391]\tLoss 0.1751 (0.1751)\tPrec@1 92.188 (92.188)\n",
            "Epoch: [41][55/391]\tLoss 0.2391 (0.2654)\tPrec@1 90.625 (90.946)\n",
            "Epoch: [41][110/391]\tLoss 0.1670 (0.2778)\tPrec@1 92.969 (90.329)\n",
            "Epoch: [41][165/391]\tLoss 0.2291 (0.2738)\tPrec@1 92.969 (90.432)\n",
            "Epoch: [41][220/391]\tLoss 0.2573 (0.2738)\tPrec@1 88.281 (90.381)\n",
            "Epoch: [41][275/391]\tLoss 0.1868 (0.2725)\tPrec@1 91.406 (90.458)\n",
            "Epoch: [41][330/391]\tLoss 0.1347 (0.2721)\tPrec@1 96.094 (90.427)\n",
            "Epoch: [41][385/391]\tLoss 0.2497 (0.2738)\tPrec@1 94.531 (90.388)\n",
            "Test\t  Prec@1: 86.300 (Err: 13.700 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [42][0/391]\tLoss 0.2737 (0.2737)\tPrec@1 89.062 (89.062)\n",
            "Epoch: [42][55/391]\tLoss 0.1974 (0.2493)\tPrec@1 91.406 (91.211)\n",
            "Epoch: [42][110/391]\tLoss 0.3425 (0.2529)\tPrec@1 89.062 (90.928)\n",
            "Epoch: [42][165/391]\tLoss 0.2108 (0.2553)\tPrec@1 94.531 (91.002)\n",
            "Epoch: [42][220/391]\tLoss 0.1705 (0.2515)\tPrec@1 92.969 (91.092)\n",
            "Epoch: [42][275/391]\tLoss 0.3743 (0.2587)\tPrec@1 87.500 (90.871)\n",
            "Epoch: [42][330/391]\tLoss 0.2219 (0.2648)\tPrec@1 91.406 (90.743)\n",
            "Epoch: [42][385/391]\tLoss 0.2123 (0.2668)\tPrec@1 92.969 (90.686)\n",
            "Test\t  Prec@1: 85.210 (Err: 14.790 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [43][0/391]\tLoss 0.3275 (0.3275)\tPrec@1 90.625 (90.625)\n",
            "Epoch: [43][55/391]\tLoss 0.2177 (0.2595)\tPrec@1 93.750 (90.932)\n",
            "Epoch: [43][110/391]\tLoss 0.2098 (0.2589)\tPrec@1 92.969 (91.068)\n",
            "Epoch: [43][165/391]\tLoss 0.3191 (0.2616)\tPrec@1 88.281 (90.893)\n",
            "Epoch: [43][220/391]\tLoss 0.2969 (0.2694)\tPrec@1 86.719 (90.629)\n",
            "Epoch: [43][275/391]\tLoss 0.2386 (0.2691)\tPrec@1 92.969 (90.656)\n",
            "Epoch: [43][330/391]\tLoss 0.3155 (0.2692)\tPrec@1 88.281 (90.693)\n",
            "Epoch: [43][385/391]\tLoss 0.2793 (0.2707)\tPrec@1 89.062 (90.680)\n",
            "Test\t  Prec@1: 82.640 (Err: 17.360 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [44][0/391]\tLoss 0.2074 (0.2074)\tPrec@1 92.188 (92.188)\n",
            "Epoch: [44][55/391]\tLoss 0.2840 (0.2469)\tPrec@1 89.844 (91.476)\n",
            "Epoch: [44][110/391]\tLoss 0.2618 (0.2525)\tPrec@1 88.281 (91.273)\n",
            "Epoch: [44][165/391]\tLoss 0.2075 (0.2558)\tPrec@1 90.625 (91.114)\n",
            "Epoch: [44][220/391]\tLoss 0.3632 (0.2606)\tPrec@1 85.938 (90.943)\n",
            "Epoch: [44][275/391]\tLoss 0.2289 (0.2652)\tPrec@1 92.969 (90.781)\n",
            "Epoch: [44][330/391]\tLoss 0.2368 (0.2656)\tPrec@1 89.062 (90.750)\n",
            "Epoch: [44][385/391]\tLoss 0.2426 (0.2678)\tPrec@1 90.625 (90.665)\n",
            "Test\t  Prec@1: 85.540 (Err: 14.460 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [45][0/391]\tLoss 0.2921 (0.2921)\tPrec@1 90.625 (90.625)\n",
            "Epoch: [45][55/391]\tLoss 0.2594 (0.2428)\tPrec@1 91.406 (91.448)\n",
            "Epoch: [45][110/391]\tLoss 0.1749 (0.2537)\tPrec@1 94.531 (91.181)\n",
            "Epoch: [45][165/391]\tLoss 0.2004 (0.2503)\tPrec@1 92.188 (91.227)\n",
            "Epoch: [45][220/391]\tLoss 0.2224 (0.2553)\tPrec@1 89.844 (91.070)\n",
            "Epoch: [45][275/391]\tLoss 0.2283 (0.2615)\tPrec@1 92.969 (90.860)\n",
            "Epoch: [45][330/391]\tLoss 0.2486 (0.2625)\tPrec@1 92.188 (90.785)\n",
            "Epoch: [45][385/391]\tLoss 0.2062 (0.2647)\tPrec@1 93.750 (90.688)\n",
            "Test\t  Prec@1: 86.250 (Err: 13.750 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [46][0/391]\tLoss 0.2607 (0.2607)\tPrec@1 91.406 (91.406)\n",
            "Epoch: [46][55/391]\tLoss 0.2606 (0.2573)\tPrec@1 91.406 (91.155)\n",
            "Epoch: [46][110/391]\tLoss 0.3264 (0.2613)\tPrec@1 91.406 (90.998)\n",
            "Epoch: [46][165/391]\tLoss 0.1774 (0.2638)\tPrec@1 93.750 (90.870)\n",
            "Epoch: [46][220/391]\tLoss 0.3464 (0.2619)\tPrec@1 87.500 (90.968)\n",
            "Epoch: [46][275/391]\tLoss 0.3399 (0.2646)\tPrec@1 89.062 (90.849)\n",
            "Epoch: [46][330/391]\tLoss 0.2456 (0.2656)\tPrec@1 91.406 (90.750)\n",
            "Epoch: [46][385/391]\tLoss 0.1897 (0.2651)\tPrec@1 89.844 (90.777)\n",
            "Test\t  Prec@1: 85.050 (Err: 14.950 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [47][0/391]\tLoss 0.3002 (0.3002)\tPrec@1 85.156 (85.156)\n",
            "Epoch: [47][55/391]\tLoss 0.2366 (0.2509)\tPrec@1 89.844 (91.002)\n",
            "Epoch: [47][110/391]\tLoss 0.1482 (0.2488)\tPrec@1 94.531 (91.287)\n",
            "Epoch: [47][165/391]\tLoss 0.2271 (0.2519)\tPrec@1 91.406 (91.176)\n",
            "Epoch: [47][220/391]\tLoss 0.2801 (0.2558)\tPrec@1 91.406 (90.957)\n",
            "Epoch: [47][275/391]\tLoss 0.3299 (0.2582)\tPrec@1 89.062 (90.846)\n",
            "Epoch: [47][330/391]\tLoss 0.2588 (0.2636)\tPrec@1 91.406 (90.703)\n",
            "Epoch: [47][385/391]\tLoss 0.2764 (0.2663)\tPrec@1 89.062 (90.617)\n",
            "Test\t  Prec@1: 86.370 (Err: 13.630 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [48][0/391]\tLoss 0.2436 (0.2436)\tPrec@1 92.188 (92.188)\n",
            "Epoch: [48][55/391]\tLoss 0.1706 (0.2475)\tPrec@1 94.531 (91.309)\n",
            "Epoch: [48][110/391]\tLoss 0.2553 (0.2461)\tPrec@1 92.969 (91.146)\n",
            "Epoch: [48][165/391]\tLoss 0.2837 (0.2493)\tPrec@1 91.406 (91.119)\n",
            "Epoch: [48][220/391]\tLoss 0.2445 (0.2513)\tPrec@1 92.188 (91.102)\n",
            "Epoch: [48][275/391]\tLoss 0.2258 (0.2545)\tPrec@1 90.625 (90.982)\n",
            "Epoch: [48][330/391]\tLoss 0.1762 (0.2535)\tPrec@1 92.969 (91.014)\n",
            "Epoch: [48][385/391]\tLoss 0.3839 (0.2543)\tPrec@1 89.062 (90.957)\n",
            "Test\t  Prec@1: 86.480 (Err: 13.520 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [49][0/391]\tLoss 0.2642 (0.2642)\tPrec@1 90.625 (90.625)\n",
            "Epoch: [49][55/391]\tLoss 0.3931 (0.2430)\tPrec@1 89.062 (91.336)\n",
            "Epoch: [49][110/391]\tLoss 0.3040 (0.2414)\tPrec@1 88.281 (91.294)\n",
            "Epoch: [49][165/391]\tLoss 0.2189 (0.2484)\tPrec@1 90.625 (91.119)\n",
            "Epoch: [49][220/391]\tLoss 0.2655 (0.2513)\tPrec@1 91.406 (91.060)\n",
            "Epoch: [49][275/391]\tLoss 0.3122 (0.2554)\tPrec@1 87.500 (90.914)\n",
            "Epoch: [49][330/391]\tLoss 0.2090 (0.2559)\tPrec@1 90.625 (90.882)\n",
            "Epoch: [49][385/391]\tLoss 0.3375 (0.2570)\tPrec@1 89.062 (90.884)\n",
            "Test\t  Prec@1: 86.810 (Err: 13.190 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [50][0/391]\tLoss 0.1943 (0.1943)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [50][55/391]\tLoss 0.2672 (0.2377)\tPrec@1 91.406 (91.462)\n",
            "Epoch: [50][110/391]\tLoss 0.2385 (0.2371)\tPrec@1 90.625 (91.660)\n",
            "Epoch: [50][165/391]\tLoss 0.3343 (0.2460)\tPrec@1 90.625 (91.293)\n",
            "Epoch: [50][220/391]\tLoss 0.2103 (0.2499)\tPrec@1 91.406 (91.120)\n",
            "Epoch: [50][275/391]\tLoss 0.2672 (0.2497)\tPrec@1 89.844 (91.115)\n",
            "Epoch: [50][330/391]\tLoss 0.2928 (0.2526)\tPrec@1 87.500 (91.099)\n",
            "Epoch: [50][385/391]\tLoss 0.2415 (0.2560)\tPrec@1 90.625 (91.034)\n",
            "Test\t  Prec@1: 83.370 (Err: 16.630 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [51][0/391]\tLoss 0.2790 (0.2790)\tPrec@1 91.406 (91.406)\n",
            "Epoch: [51][55/391]\tLoss 0.2032 (0.2415)\tPrec@1 92.188 (91.588)\n",
            "Epoch: [51][110/391]\tLoss 0.1479 (0.2509)\tPrec@1 95.312 (91.061)\n",
            "Epoch: [51][165/391]\tLoss 0.3393 (0.2523)\tPrec@1 88.281 (91.152)\n",
            "Epoch: [51][220/391]\tLoss 0.3510 (0.2538)\tPrec@1 86.719 (91.102)\n",
            "Epoch: [51][275/391]\tLoss 0.3284 (0.2587)\tPrec@1 86.719 (90.962)\n",
            "Epoch: [51][330/391]\tLoss 0.2095 (0.2597)\tPrec@1 92.969 (91.019)\n",
            "Epoch: [51][385/391]\tLoss 0.1908 (0.2576)\tPrec@1 94.531 (91.101)\n",
            "Test\t  Prec@1: 84.510 (Err: 15.490 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [52][0/391]\tLoss 0.2100 (0.2100)\tPrec@1 92.188 (92.188)\n",
            "Epoch: [52][55/391]\tLoss 0.2996 (0.2479)\tPrec@1 88.281 (91.476)\n",
            "Epoch: [52][110/391]\tLoss 0.2645 (0.2453)\tPrec@1 89.844 (91.617)\n",
            "Epoch: [52][165/391]\tLoss 0.3421 (0.2486)\tPrec@1 86.719 (91.416)\n",
            "Epoch: [52][220/391]\tLoss 0.1618 (0.2503)\tPrec@1 92.969 (91.406)\n",
            "Epoch: [52][275/391]\tLoss 0.3258 (0.2500)\tPrec@1 87.500 (91.344)\n",
            "Epoch: [52][330/391]\tLoss 0.2906 (0.2533)\tPrec@1 89.062 (91.229)\n",
            "Epoch: [52][385/391]\tLoss 0.1917 (0.2558)\tPrec@1 92.188 (91.149)\n",
            "Test\t  Prec@1: 84.780 (Err: 15.220 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [53][0/391]\tLoss 0.2895 (0.2895)\tPrec@1 89.062 (89.062)\n",
            "Epoch: [53][55/391]\tLoss 0.2665 (0.2381)\tPrec@1 93.750 (91.448)\n",
            "Epoch: [53][110/391]\tLoss 0.2310 (0.2451)\tPrec@1 89.062 (91.329)\n",
            "Epoch: [53][165/391]\tLoss 0.2021 (0.2485)\tPrec@1 92.969 (91.180)\n",
            "Epoch: [53][220/391]\tLoss 0.2677 (0.2493)\tPrec@1 89.844 (91.169)\n",
            "Epoch: [53][275/391]\tLoss 0.3481 (0.2495)\tPrec@1 86.719 (91.259)\n",
            "Epoch: [53][330/391]\tLoss 0.2633 (0.2501)\tPrec@1 90.625 (91.267)\n",
            "Epoch: [53][385/391]\tLoss 0.3032 (0.2512)\tPrec@1 89.844 (91.238)\n",
            "Test\t  Prec@1: 83.100 (Err: 16.900 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [54][0/391]\tLoss 0.1820 (0.1820)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [54][55/391]\tLoss 0.2627 (0.2372)\tPrec@1 87.500 (91.476)\n",
            "Epoch: [54][110/391]\tLoss 0.3998 (0.2348)\tPrec@1 85.156 (91.653)\n",
            "Epoch: [54][165/391]\tLoss 0.2464 (0.2505)\tPrec@1 89.844 (91.086)\n",
            "Epoch: [54][220/391]\tLoss 0.2355 (0.2502)\tPrec@1 89.844 (91.099)\n",
            "Epoch: [54][275/391]\tLoss 0.3224 (0.2504)\tPrec@1 86.719 (91.101)\n",
            "Epoch: [54][330/391]\tLoss 0.3081 (0.2515)\tPrec@1 88.281 (91.109)\n",
            "Epoch: [54][385/391]\tLoss 0.1794 (0.2540)\tPrec@1 94.531 (91.048)\n",
            "Test\t  Prec@1: 81.100 (Err: 18.900 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [55][0/391]\tLoss 0.2432 (0.2432)\tPrec@1 91.406 (91.406)\n",
            "Epoch: [55][55/391]\tLoss 0.3483 (0.2360)\tPrec@1 89.062 (91.295)\n",
            "Epoch: [55][110/391]\tLoss 0.2275 (0.2461)\tPrec@1 92.969 (91.061)\n",
            "Epoch: [55][165/391]\tLoss 0.2447 (0.2497)\tPrec@1 92.188 (91.110)\n",
            "Epoch: [55][220/391]\tLoss 0.3042 (0.2506)\tPrec@1 92.188 (91.074)\n",
            "Epoch: [55][275/391]\tLoss 0.3261 (0.2514)\tPrec@1 89.062 (91.109)\n",
            "Epoch: [55][330/391]\tLoss 0.2345 (0.2552)\tPrec@1 91.406 (90.977)\n",
            "Epoch: [55][385/391]\tLoss 0.2516 (0.2546)\tPrec@1 90.625 (91.066)\n",
            "Test\t  Prec@1: 86.290 (Err: 13.710 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [56][0/391]\tLoss 0.1715 (0.1715)\tPrec@1 93.750 (93.750)\n",
            "Epoch: [56][55/391]\tLoss 0.3251 (0.2395)\tPrec@1 91.406 (91.588)\n",
            "Epoch: [56][110/391]\tLoss 0.2158 (0.2389)\tPrec@1 93.750 (91.624)\n",
            "Epoch: [56][165/391]\tLoss 0.2947 (0.2440)\tPrec@1 89.844 (91.472)\n",
            "Epoch: [56][220/391]\tLoss 0.2610 (0.2464)\tPrec@1 90.625 (91.456)\n",
            "Epoch: [56][275/391]\tLoss 0.2071 (0.2446)\tPrec@1 93.750 (91.437)\n",
            "Epoch: [56][330/391]\tLoss 0.3079 (0.2484)\tPrec@1 86.719 (91.312)\n",
            "Epoch: [56][385/391]\tLoss 0.2443 (0.2528)\tPrec@1 90.625 (91.200)\n",
            "Test\t  Prec@1: 87.510 (Err: 12.490 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [57][0/391]\tLoss 0.2317 (0.2317)\tPrec@1 91.406 (91.406)\n",
            "Epoch: [57][55/391]\tLoss 0.2660 (0.2280)\tPrec@1 90.625 (91.908)\n",
            "Epoch: [57][110/391]\tLoss 0.2589 (0.2286)\tPrec@1 89.844 (91.836)\n",
            "Epoch: [57][165/391]\tLoss 0.2958 (0.2356)\tPrec@1 90.625 (91.595)\n",
            "Epoch: [57][220/391]\tLoss 0.2109 (0.2419)\tPrec@1 93.750 (91.403)\n",
            "Epoch: [57][275/391]\tLoss 0.2357 (0.2433)\tPrec@1 91.406 (91.415)\n",
            "Epoch: [57][330/391]\tLoss 0.3067 (0.2445)\tPrec@1 87.500 (91.331)\n",
            "Epoch: [57][385/391]\tLoss 0.3211 (0.2467)\tPrec@1 90.625 (91.269)\n",
            "Test\t  Prec@1: 85.140 (Err: 14.860 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [58][0/391]\tLoss 0.2525 (0.2525)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [58][55/391]\tLoss 0.2988 (0.2416)\tPrec@1 90.625 (91.462)\n",
            "Epoch: [58][110/391]\tLoss 0.1836 (0.2395)\tPrec@1 93.750 (91.540)\n",
            "Epoch: [58][165/391]\tLoss 0.1761 (0.2461)\tPrec@1 92.969 (91.251)\n",
            "Epoch: [58][220/391]\tLoss 0.2299 (0.2437)\tPrec@1 92.969 (91.307)\n",
            "Epoch: [58][275/391]\tLoss 0.2029 (0.2486)\tPrec@1 92.969 (91.157)\n",
            "Epoch: [58][330/391]\tLoss 0.2921 (0.2502)\tPrec@1 89.844 (91.144)\n",
            "Epoch: [58][385/391]\tLoss 0.2180 (0.2495)\tPrec@1 93.750 (91.159)\n",
            "Test\t  Prec@1: 85.560 (Err: 14.440 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [59][0/391]\tLoss 0.2531 (0.2531)\tPrec@1 90.625 (90.625)\n",
            "Epoch: [59][55/391]\tLoss 0.2532 (0.2218)\tPrec@1 88.281 (92.146)\n",
            "Epoch: [59][110/391]\tLoss 0.3621 (0.2342)\tPrec@1 87.500 (91.765)\n",
            "Epoch: [59][165/391]\tLoss 0.2981 (0.2374)\tPrec@1 89.844 (91.675)\n",
            "Epoch: [59][220/391]\tLoss 0.2995 (0.2422)\tPrec@1 86.719 (91.491)\n",
            "Epoch: [59][275/391]\tLoss 0.2865 (0.2446)\tPrec@1 92.969 (91.426)\n",
            "Epoch: [59][330/391]\tLoss 0.2401 (0.2476)\tPrec@1 92.188 (91.317)\n",
            "Epoch: [59][385/391]\tLoss 0.2122 (0.2509)\tPrec@1 91.406 (91.252)\n",
            "Test\t  Prec@1: 82.590 (Err: 17.410 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [60][0/391]\tLoss 0.2933 (0.2933)\tPrec@1 89.062 (89.062)\n",
            "Epoch: [60][55/391]\tLoss 0.2519 (0.2234)\tPrec@1 92.188 (92.160)\n",
            "Epoch: [60][110/391]\tLoss 0.2147 (0.2302)\tPrec@1 92.188 (92.040)\n",
            "Epoch: [60][165/391]\tLoss 0.3123 (0.2334)\tPrec@1 85.156 (91.825)\n",
            "Epoch: [60][220/391]\tLoss 0.2468 (0.2328)\tPrec@1 93.750 (91.866)\n",
            "Epoch: [60][275/391]\tLoss 0.3195 (0.2370)\tPrec@1 90.625 (91.695)\n",
            "Epoch: [60][330/391]\tLoss 0.1575 (0.2403)\tPrec@1 92.969 (91.560)\n",
            "Epoch: [60][385/391]\tLoss 0.2172 (0.2399)\tPrec@1 92.188 (91.562)\n",
            "Test\t  Prec@1: 83.060 (Err: 16.940 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [61][0/391]\tLoss 0.2664 (0.2664)\tPrec@1 89.062 (89.062)\n",
            "Epoch: [61][55/391]\tLoss 0.2323 (0.2405)\tPrec@1 92.188 (91.560)\n",
            "Epoch: [61][110/391]\tLoss 0.3638 (0.2380)\tPrec@1 87.500 (91.463)\n",
            "Epoch: [61][165/391]\tLoss 0.2423 (0.2491)\tPrec@1 91.406 (91.223)\n",
            "Epoch: [61][220/391]\tLoss 0.2259 (0.2449)\tPrec@1 91.406 (91.406)\n",
            "Epoch: [61][275/391]\tLoss 0.2718 (0.2437)\tPrec@1 88.281 (91.469)\n",
            "Epoch: [61][330/391]\tLoss 0.2408 (0.2440)\tPrec@1 89.062 (91.498)\n",
            "Epoch: [61][385/391]\tLoss 0.3617 (0.2462)\tPrec@1 85.938 (91.398)\n",
            "Test\t  Prec@1: 84.820 (Err: 15.180 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [62][0/391]\tLoss 0.2338 (0.2338)\tPrec@1 89.844 (89.844)\n",
            "Epoch: [62][55/391]\tLoss 0.2189 (0.2240)\tPrec@1 92.188 (92.271)\n",
            "Epoch: [62][110/391]\tLoss 0.3007 (0.2332)\tPrec@1 89.844 (91.660)\n",
            "Epoch: [62][165/391]\tLoss 0.2053 (0.2343)\tPrec@1 91.406 (91.604)\n",
            "Epoch: [62][220/391]\tLoss 0.2036 (0.2380)\tPrec@1 91.406 (91.590)\n",
            "Epoch: [62][275/391]\tLoss 0.1774 (0.2391)\tPrec@1 94.531 (91.587)\n",
            "Epoch: [62][330/391]\tLoss 0.2335 (0.2414)\tPrec@1 92.188 (91.512)\n",
            "Epoch: [62][385/391]\tLoss 0.2619 (0.2428)\tPrec@1 88.281 (91.457)\n",
            "Test\t  Prec@1: 85.180 (Err: 14.820 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [63][0/391]\tLoss 0.1463 (0.1463)\tPrec@1 96.875 (96.875)\n",
            "Epoch: [63][55/391]\tLoss 0.2528 (0.2219)\tPrec@1 94.531 (92.341)\n",
            "Epoch: [63][110/391]\tLoss 0.2292 (0.2229)\tPrec@1 90.625 (92.230)\n",
            "Epoch: [63][165/391]\tLoss 0.2452 (0.2264)\tPrec@1 89.062 (92.037)\n",
            "Epoch: [63][220/391]\tLoss 0.2442 (0.2314)\tPrec@1 92.969 (91.859)\n",
            "Epoch: [63][275/391]\tLoss 0.1081 (0.2337)\tPrec@1 97.656 (91.769)\n",
            "Epoch: [63][330/391]\tLoss 0.3107 (0.2353)\tPrec@1 88.281 (91.706)\n",
            "Epoch: [63][385/391]\tLoss 0.4354 (0.2403)\tPrec@1 85.938 (91.487)\n",
            "Test\t  Prec@1: 86.220 (Err: 13.780 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [64][0/391]\tLoss 0.2833 (0.2833)\tPrec@1 90.625 (90.625)\n",
            "Epoch: [64][55/391]\tLoss 0.2563 (0.2260)\tPrec@1 91.406 (91.908)\n",
            "Epoch: [64][110/391]\tLoss 0.2727 (0.2319)\tPrec@1 92.188 (91.800)\n",
            "Epoch: [64][165/391]\tLoss 0.1864 (0.2362)\tPrec@1 93.750 (91.726)\n",
            "Epoch: [64][220/391]\tLoss 0.1629 (0.2375)\tPrec@1 96.875 (91.770)\n",
            "Epoch: [64][275/391]\tLoss 0.1434 (0.2363)\tPrec@1 96.875 (91.811)\n",
            "Epoch: [64][330/391]\tLoss 0.2494 (0.2384)\tPrec@1 89.844 (91.708)\n",
            "Epoch: [64][385/391]\tLoss 0.1579 (0.2400)\tPrec@1 93.750 (91.597)\n",
            "Test\t  Prec@1: 87.120 (Err: 12.880 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [65][0/391]\tLoss 0.2190 (0.2190)\tPrec@1 92.188 (92.188)\n",
            "Epoch: [65][55/391]\tLoss 0.3757 (0.2231)\tPrec@1 90.625 (92.285)\n",
            "Epoch: [65][110/391]\tLoss 0.1595 (0.2225)\tPrec@1 92.188 (92.159)\n",
            "Epoch: [65][165/391]\tLoss 0.3538 (0.2209)\tPrec@1 86.719 (92.178)\n",
            "Epoch: [65][220/391]\tLoss 0.2304 (0.2270)\tPrec@1 89.844 (91.922)\n",
            "Epoch: [65][275/391]\tLoss 0.2551 (0.2318)\tPrec@1 90.625 (91.723)\n",
            "Epoch: [65][330/391]\tLoss 0.1646 (0.2375)\tPrec@1 94.531 (91.581)\n",
            "Epoch: [65][385/391]\tLoss 0.2377 (0.2415)\tPrec@1 89.062 (91.453)\n",
            "Test\t  Prec@1: 85.630 (Err: 14.370 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [66][0/391]\tLoss 0.2887 (0.2887)\tPrec@1 92.188 (92.188)\n",
            "Epoch: [66][55/391]\tLoss 0.2913 (0.2316)\tPrec@1 90.625 (91.699)\n",
            "Epoch: [66][110/391]\tLoss 0.2335 (0.2340)\tPrec@1 90.625 (91.646)\n",
            "Epoch: [66][165/391]\tLoss 0.2507 (0.2311)\tPrec@1 92.188 (91.726)\n",
            "Epoch: [66][220/391]\tLoss 0.3591 (0.2355)\tPrec@1 87.500 (91.611)\n",
            "Epoch: [66][275/391]\tLoss 0.1633 (0.2373)\tPrec@1 94.531 (91.613)\n",
            "Epoch: [66][330/391]\tLoss 0.2909 (0.2365)\tPrec@1 92.969 (91.633)\n",
            "Epoch: [66][385/391]\tLoss 0.2309 (0.2374)\tPrec@1 91.406 (91.607)\n",
            "Test\t  Prec@1: 84.360 (Err: 15.640 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [67][0/391]\tLoss 0.2333 (0.2333)\tPrec@1 90.625 (90.625)\n",
            "Epoch: [67][55/391]\tLoss 0.2089 (0.2166)\tPrec@1 92.188 (92.146)\n",
            "Epoch: [67][110/391]\tLoss 0.2932 (0.2256)\tPrec@1 89.062 (92.019)\n",
            "Epoch: [67][165/391]\tLoss 0.1985 (0.2263)\tPrec@1 92.969 (92.060)\n",
            "Epoch: [67][220/391]\tLoss 0.2830 (0.2282)\tPrec@1 88.281 (91.975)\n",
            "Epoch: [67][275/391]\tLoss 0.3782 (0.2291)\tPrec@1 85.156 (91.975)\n",
            "Epoch: [67][330/391]\tLoss 0.2263 (0.2318)\tPrec@1 90.625 (91.836)\n",
            "Epoch: [67][385/391]\tLoss 0.2544 (0.2370)\tPrec@1 88.281 (91.629)\n",
            "Test\t  Prec@1: 85.890 (Err: 14.110 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [68][0/391]\tLoss 0.1657 (0.1657)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [68][55/391]\tLoss 0.2420 (0.2234)\tPrec@1 92.969 (92.327)\n",
            "Epoch: [68][110/391]\tLoss 0.1489 (0.2207)\tPrec@1 95.312 (92.406)\n",
            "Epoch: [68][165/391]\tLoss 0.1490 (0.2251)\tPrec@1 96.094 (92.272)\n",
            "Epoch: [68][220/391]\tLoss 0.2395 (0.2250)\tPrec@1 89.844 (92.272)\n",
            "Epoch: [68][275/391]\tLoss 0.1444 (0.2291)\tPrec@1 96.875 (92.131)\n",
            "Epoch: [68][330/391]\tLoss 0.1743 (0.2317)\tPrec@1 94.531 (92.029)\n",
            "Epoch: [68][385/391]\tLoss 0.2415 (0.2347)\tPrec@1 91.406 (91.886)\n",
            "Test\t  Prec@1: 84.270 (Err: 15.730 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [69][0/391]\tLoss 0.2488 (0.2488)\tPrec@1 91.406 (91.406)\n",
            "Epoch: [69][55/391]\tLoss 0.2310 (0.2150)\tPrec@1 92.188 (92.369)\n",
            "Epoch: [69][110/391]\tLoss 0.1903 (0.2244)\tPrec@1 92.188 (92.096)\n",
            "Epoch: [69][165/391]\tLoss 0.1691 (0.2207)\tPrec@1 92.188 (92.357)\n",
            "Epoch: [69][220/391]\tLoss 0.3059 (0.2246)\tPrec@1 90.625 (92.269)\n",
            "Epoch: [69][275/391]\tLoss 0.3895 (0.2307)\tPrec@1 88.281 (92.035)\n",
            "Epoch: [69][330/391]\tLoss 0.3656 (0.2337)\tPrec@1 88.281 (91.918)\n",
            "Epoch: [69][385/391]\tLoss 0.2371 (0.2352)\tPrec@1 92.969 (91.888)\n",
            "Test\t  Prec@1: 82.700 (Err: 17.300 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [70][0/391]\tLoss 0.3216 (0.3216)\tPrec@1 89.062 (89.062)\n",
            "Epoch: [70][55/391]\tLoss 0.2119 (0.2274)\tPrec@1 93.750 (91.811)\n",
            "Epoch: [70][110/391]\tLoss 0.2696 (0.2306)\tPrec@1 92.188 (91.864)\n",
            "Epoch: [70][165/391]\tLoss 0.2407 (0.2360)\tPrec@1 91.406 (91.745)\n",
            "Epoch: [70][220/391]\tLoss 0.1947 (0.2359)\tPrec@1 92.969 (91.742)\n",
            "Epoch: [70][275/391]\tLoss 0.2503 (0.2392)\tPrec@1 89.844 (91.621)\n",
            "Epoch: [70][330/391]\tLoss 0.2785 (0.2388)\tPrec@1 89.844 (91.652)\n",
            "Epoch: [70][385/391]\tLoss 0.3542 (0.2379)\tPrec@1 89.844 (91.692)\n",
            "Test\t  Prec@1: 83.410 (Err: 16.590 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [71][0/391]\tLoss 0.2824 (0.2824)\tPrec@1 89.062 (89.062)\n",
            "Epoch: [71][55/391]\tLoss 0.3187 (0.2324)\tPrec@1 88.281 (91.518)\n",
            "Epoch: [71][110/391]\tLoss 0.2359 (0.2338)\tPrec@1 91.406 (91.737)\n",
            "Epoch: [71][165/391]\tLoss 0.3118 (0.2341)\tPrec@1 87.500 (91.750)\n",
            "Epoch: [71][220/391]\tLoss 0.2187 (0.2351)\tPrec@1 92.188 (91.707)\n",
            "Epoch: [71][275/391]\tLoss 0.2179 (0.2380)\tPrec@1 90.625 (91.633)\n",
            "Epoch: [71][330/391]\tLoss 0.2478 (0.2373)\tPrec@1 90.625 (91.647)\n",
            "Epoch: [71][385/391]\tLoss 0.2170 (0.2371)\tPrec@1 90.625 (91.637)\n",
            "Test\t  Prec@1: 84.590 (Err: 15.410 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [72][0/391]\tLoss 0.3007 (0.3007)\tPrec@1 89.062 (89.062)\n",
            "Epoch: [72][55/391]\tLoss 0.1986 (0.2080)\tPrec@1 92.188 (92.815)\n",
            "Epoch: [72][110/391]\tLoss 0.2134 (0.2170)\tPrec@1 92.188 (92.399)\n",
            "Epoch: [72][165/391]\tLoss 0.2256 (0.2198)\tPrec@1 91.406 (92.366)\n",
            "Epoch: [72][220/391]\tLoss 0.2943 (0.2221)\tPrec@1 87.500 (92.241)\n",
            "Epoch: [72][275/391]\tLoss 0.1997 (0.2240)\tPrec@1 93.750 (92.122)\n",
            "Epoch: [72][330/391]\tLoss 0.1577 (0.2263)\tPrec@1 93.750 (92.058)\n",
            "Epoch: [72][385/391]\tLoss 0.3470 (0.2278)\tPrec@1 87.500 (92.020)\n",
            "Test\t  Prec@1: 86.430 (Err: 13.570 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [73][0/391]\tLoss 0.3957 (0.3957)\tPrec@1 85.156 (85.156)\n",
            "Epoch: [73][55/391]\tLoss 0.1951 (0.2106)\tPrec@1 95.312 (92.425)\n",
            "Epoch: [73][110/391]\tLoss 0.2546 (0.2222)\tPrec@1 92.188 (92.033)\n",
            "Epoch: [73][165/391]\tLoss 0.1669 (0.2252)\tPrec@1 93.750 (91.891)\n",
            "Epoch: [73][220/391]\tLoss 0.1800 (0.2255)\tPrec@1 92.188 (91.873)\n",
            "Epoch: [73][275/391]\tLoss 0.3541 (0.2251)\tPrec@1 89.844 (91.953)\n",
            "Epoch: [73][330/391]\tLoss 0.3115 (0.2292)\tPrec@1 88.281 (91.850)\n",
            "Epoch: [73][385/391]\tLoss 0.0973 (0.2319)\tPrec@1 97.656 (91.762)\n",
            "Test\t  Prec@1: 86.770 (Err: 13.230 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [74][0/391]\tLoss 0.1807 (0.1807)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [74][55/391]\tLoss 0.2721 (0.2199)\tPrec@1 90.625 (92.439)\n",
            "Epoch: [74][110/391]\tLoss 0.2613 (0.2231)\tPrec@1 91.406 (92.258)\n",
            "Epoch: [74][165/391]\tLoss 0.2347 (0.2246)\tPrec@1 91.406 (92.150)\n",
            "Epoch: [74][220/391]\tLoss 0.4098 (0.2258)\tPrec@1 87.500 (92.120)\n",
            "Epoch: [74][275/391]\tLoss 0.3665 (0.2306)\tPrec@1 85.938 (91.924)\n",
            "Epoch: [74][330/391]\tLoss 0.1697 (0.2327)\tPrec@1 93.750 (91.824)\n",
            "Epoch: [74][385/391]\tLoss 0.2721 (0.2319)\tPrec@1 91.406 (91.872)\n",
            "Test\t  Prec@1: 86.840 (Err: 13.160 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [75][0/391]\tLoss 0.1927 (0.1927)\tPrec@1 90.625 (90.625)\n",
            "Epoch: [75][55/391]\tLoss 0.2361 (0.2249)\tPrec@1 92.188 (92.034)\n",
            "Epoch: [75][110/391]\tLoss 0.2170 (0.2246)\tPrec@1 92.188 (92.096)\n",
            "Epoch: [75][165/391]\tLoss 0.2655 (0.2223)\tPrec@1 89.062 (92.046)\n",
            "Epoch: [75][220/391]\tLoss 0.2600 (0.2243)\tPrec@1 89.062 (91.954)\n",
            "Epoch: [75][275/391]\tLoss 0.3211 (0.2287)\tPrec@1 86.719 (91.848)\n",
            "Epoch: [75][330/391]\tLoss 0.2517 (0.2291)\tPrec@1 92.188 (91.881)\n",
            "Epoch: [75][385/391]\tLoss 0.1848 (0.2307)\tPrec@1 93.750 (91.833)\n",
            "Test\t  Prec@1: 86.980 (Err: 13.020 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [76][0/391]\tLoss 0.1578 (0.1578)\tPrec@1 96.875 (96.875)\n",
            "Epoch: [76][55/391]\tLoss 0.1514 (0.2267)\tPrec@1 95.312 (92.634)\n",
            "Epoch: [76][110/391]\tLoss 0.2220 (0.2160)\tPrec@1 91.406 (92.842)\n",
            "Epoch: [76][165/391]\tLoss 0.1912 (0.2149)\tPrec@1 90.625 (92.705)\n",
            "Epoch: [76][220/391]\tLoss 0.2233 (0.2132)\tPrec@1 89.844 (92.679)\n",
            "Epoch: [76][275/391]\tLoss 0.1865 (0.2169)\tPrec@1 94.531 (92.507)\n",
            "Epoch: [76][330/391]\tLoss 0.3151 (0.2216)\tPrec@1 88.281 (92.268)\n",
            "Epoch: [76][385/391]\tLoss 0.2108 (0.2272)\tPrec@1 93.750 (92.090)\n",
            "Test\t  Prec@1: 84.580 (Err: 15.420 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [77][0/391]\tLoss 0.2217 (0.2217)\tPrec@1 92.188 (92.188)\n",
            "Epoch: [77][55/391]\tLoss 0.3024 (0.2223)\tPrec@1 86.719 (92.243)\n",
            "Epoch: [77][110/391]\tLoss 0.1785 (0.2154)\tPrec@1 92.188 (92.561)\n",
            "Epoch: [77][165/391]\tLoss 0.3439 (0.2206)\tPrec@1 90.625 (92.277)\n",
            "Epoch: [77][220/391]\tLoss 0.1923 (0.2196)\tPrec@1 92.969 (92.248)\n",
            "Epoch: [77][275/391]\tLoss 0.2172 (0.2211)\tPrec@1 92.188 (92.162)\n",
            "Epoch: [77][330/391]\tLoss 0.2049 (0.2245)\tPrec@1 92.188 (92.048)\n",
            "Epoch: [77][385/391]\tLoss 0.2077 (0.2254)\tPrec@1 92.188 (92.046)\n",
            "Test\t  Prec@1: 87.240 (Err: 12.760 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [78][0/391]\tLoss 0.1726 (0.1726)\tPrec@1 95.312 (95.312)\n",
            "Epoch: [78][55/391]\tLoss 0.2599 (0.2103)\tPrec@1 89.062 (92.592)\n",
            "Epoch: [78][110/391]\tLoss 0.1280 (0.2069)\tPrec@1 96.094 (92.814)\n",
            "Epoch: [78][165/391]\tLoss 0.2579 (0.2157)\tPrec@1 92.188 (92.578)\n",
            "Epoch: [78][220/391]\tLoss 0.1936 (0.2207)\tPrec@1 94.531 (92.385)\n",
            "Epoch: [78][275/391]\tLoss 0.2297 (0.2229)\tPrec@1 90.625 (92.355)\n",
            "Epoch: [78][330/391]\tLoss 0.1634 (0.2260)\tPrec@1 93.750 (92.183)\n",
            "Epoch: [78][385/391]\tLoss 0.1818 (0.2278)\tPrec@1 94.531 (92.109)\n",
            "Test\t  Prec@1: 86.350 (Err: 13.650 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [79][0/391]\tLoss 0.2888 (0.2888)\tPrec@1 89.062 (89.062)\n",
            "Epoch: [79][55/391]\tLoss 0.1490 (0.2074)\tPrec@1 94.531 (92.690)\n",
            "Epoch: [79][110/391]\tLoss 0.2113 (0.2143)\tPrec@1 92.969 (92.469)\n",
            "Epoch: [79][165/391]\tLoss 0.2551 (0.2223)\tPrec@1 90.625 (92.192)\n",
            "Epoch: [79][220/391]\tLoss 0.2125 (0.2230)\tPrec@1 91.406 (92.117)\n",
            "Epoch: [79][275/391]\tLoss 0.2446 (0.2258)\tPrec@1 89.062 (92.015)\n",
            "Epoch: [79][330/391]\tLoss 0.2165 (0.2298)\tPrec@1 91.406 (91.928)\n",
            "Epoch: [79][385/391]\tLoss 0.2231 (0.2287)\tPrec@1 92.969 (91.967)\n",
            "Test\t  Prec@1: 86.390 (Err: 13.610 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [80][0/391]\tLoss 0.1467 (0.1467)\tPrec@1 93.750 (93.750)\n",
            "Epoch: [80][55/391]\tLoss 0.1922 (0.2041)\tPrec@1 92.188 (92.690)\n",
            "Epoch: [80][110/391]\tLoss 0.2817 (0.2112)\tPrec@1 89.844 (92.370)\n",
            "Epoch: [80][165/391]\tLoss 0.2610 (0.2148)\tPrec@1 92.188 (92.249)\n",
            "Epoch: [80][220/391]\tLoss 0.1966 (0.2202)\tPrec@1 93.750 (92.092)\n",
            "Epoch: [80][275/391]\tLoss 0.1650 (0.2230)\tPrec@1 95.312 (91.967)\n",
            "Epoch: [80][330/391]\tLoss 0.2391 (0.2254)\tPrec@1 90.625 (91.890)\n",
            "Epoch: [80][385/391]\tLoss 0.1892 (0.2290)\tPrec@1 92.969 (91.801)\n",
            "Test\t  Prec@1: 82.770 (Err: 17.230 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [81][0/391]\tLoss 0.2194 (0.2194)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [81][55/391]\tLoss 0.1813 (0.2227)\tPrec@1 92.188 (92.257)\n",
            "Epoch: [81][110/391]\tLoss 0.2073 (0.2168)\tPrec@1 94.531 (92.525)\n",
            "Epoch: [81][165/391]\tLoss 0.3608 (0.2149)\tPrec@1 89.062 (92.635)\n",
            "Epoch: [81][220/391]\tLoss 0.1871 (0.2214)\tPrec@1 92.969 (92.407)\n",
            "Epoch: [81][275/391]\tLoss 0.1856 (0.2297)\tPrec@1 93.750 (92.080)\n",
            "Epoch: [81][330/391]\tLoss 0.1570 (0.2284)\tPrec@1 95.312 (92.093)\n",
            "Epoch: [81][385/391]\tLoss 0.2010 (0.2315)\tPrec@1 92.188 (91.987)\n",
            "Test\t  Prec@1: 85.110 (Err: 14.890 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [82][0/391]\tLoss 0.1768 (0.1768)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [82][55/391]\tLoss 0.2393 (0.2194)\tPrec@1 92.188 (92.578)\n",
            "Epoch: [82][110/391]\tLoss 0.1914 (0.2208)\tPrec@1 91.406 (92.413)\n",
            "Epoch: [82][165/391]\tLoss 0.2288 (0.2222)\tPrec@1 94.531 (92.338)\n",
            "Epoch: [82][220/391]\tLoss 0.2089 (0.2216)\tPrec@1 92.188 (92.357)\n",
            "Epoch: [82][275/391]\tLoss 0.1528 (0.2207)\tPrec@1 96.094 (92.340)\n",
            "Epoch: [82][330/391]\tLoss 0.2550 (0.2233)\tPrec@1 89.844 (92.206)\n",
            "Epoch: [82][385/391]\tLoss 0.1978 (0.2262)\tPrec@1 95.312 (92.098)\n",
            "Test\t  Prec@1: 86.470 (Err: 13.530 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [83][0/391]\tLoss 0.2372 (0.2372)\tPrec@1 92.188 (92.188)\n",
            "Epoch: [83][55/391]\tLoss 0.2198 (0.1898)\tPrec@1 95.312 (93.583)\n",
            "Epoch: [83][110/391]\tLoss 0.2253 (0.1971)\tPrec@1 96.094 (93.293)\n",
            "Epoch: [83][165/391]\tLoss 0.3296 (0.2057)\tPrec@1 88.281 (92.922)\n",
            "Epoch: [83][220/391]\tLoss 0.2439 (0.2115)\tPrec@1 92.188 (92.718)\n",
            "Epoch: [83][275/391]\tLoss 0.1234 (0.2145)\tPrec@1 94.531 (92.553)\n",
            "Epoch: [83][330/391]\tLoss 0.3287 (0.2170)\tPrec@1 88.281 (92.445)\n",
            "Epoch: [83][385/391]\tLoss 0.2380 (0.2232)\tPrec@1 92.188 (92.260)\n",
            "Test\t  Prec@1: 83.420 (Err: 16.580 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [84][0/391]\tLoss 0.1947 (0.1947)\tPrec@1 92.188 (92.188)\n",
            "Epoch: [84][55/391]\tLoss 0.2386 (0.2182)\tPrec@1 89.844 (92.355)\n",
            "Epoch: [84][110/391]\tLoss 0.1735 (0.2196)\tPrec@1 91.406 (92.328)\n",
            "Epoch: [84][165/391]\tLoss 0.2927 (0.2262)\tPrec@1 91.406 (92.065)\n",
            "Epoch: [84][220/391]\tLoss 0.3276 (0.2230)\tPrec@1 89.844 (92.117)\n",
            "Epoch: [84][275/391]\tLoss 0.2800 (0.2248)\tPrec@1 89.062 (92.001)\n",
            "Epoch: [84][330/391]\tLoss 0.1682 (0.2260)\tPrec@1 92.969 (91.928)\n",
            "Epoch: [84][385/391]\tLoss 0.2307 (0.2269)\tPrec@1 92.969 (91.910)\n",
            "Test\t  Prec@1: 81.720 (Err: 18.280 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [85][0/391]\tLoss 0.1617 (0.1617)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [85][55/391]\tLoss 0.1195 (0.2024)\tPrec@1 94.531 (92.676)\n",
            "Epoch: [85][110/391]\tLoss 0.2371 (0.2035)\tPrec@1 92.188 (92.638)\n",
            "Epoch: [85][165/391]\tLoss 0.2634 (0.2100)\tPrec@1 90.625 (92.493)\n",
            "Epoch: [85][220/391]\tLoss 0.3401 (0.2165)\tPrec@1 92.969 (92.255)\n",
            "Epoch: [85][275/391]\tLoss 0.2158 (0.2190)\tPrec@1 92.969 (92.188)\n",
            "Epoch: [85][330/391]\tLoss 0.2411 (0.2233)\tPrec@1 90.625 (92.079)\n",
            "Epoch: [85][385/391]\tLoss 0.1741 (0.2239)\tPrec@1 94.531 (92.032)\n",
            "Test\t  Prec@1: 85.960 (Err: 14.040 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [86][0/391]\tLoss 0.2081 (0.2081)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [86][55/391]\tLoss 0.2493 (0.2102)\tPrec@1 91.406 (92.592)\n",
            "Epoch: [86][110/391]\tLoss 0.2010 (0.2137)\tPrec@1 92.969 (92.546)\n",
            "Epoch: [86][165/391]\tLoss 0.2399 (0.2115)\tPrec@1 90.625 (92.606)\n",
            "Epoch: [86][220/391]\tLoss 0.1929 (0.2135)\tPrec@1 92.188 (92.562)\n",
            "Epoch: [86][275/391]\tLoss 0.2323 (0.2164)\tPrec@1 92.969 (92.397)\n",
            "Epoch: [86][330/391]\tLoss 0.2191 (0.2201)\tPrec@1 90.625 (92.263)\n",
            "Epoch: [86][385/391]\tLoss 0.2243 (0.2206)\tPrec@1 92.188 (92.287)\n",
            "Test\t  Prec@1: 84.510 (Err: 15.490 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [87][0/391]\tLoss 0.1320 (0.1320)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [87][55/391]\tLoss 0.3264 (0.2107)\tPrec@1 89.844 (92.439)\n",
            "Epoch: [87][110/391]\tLoss 0.1132 (0.2023)\tPrec@1 96.094 (92.842)\n",
            "Epoch: [87][165/391]\tLoss 0.3545 (0.2107)\tPrec@1 82.812 (92.517)\n",
            "Epoch: [87][220/391]\tLoss 0.2530 (0.2164)\tPrec@1 89.062 (92.336)\n",
            "Epoch: [87][275/391]\tLoss 0.1550 (0.2179)\tPrec@1 94.531 (92.261)\n",
            "Epoch: [87][330/391]\tLoss 0.2664 (0.2207)\tPrec@1 87.500 (92.183)\n",
            "Epoch: [87][385/391]\tLoss 0.2171 (0.2223)\tPrec@1 93.750 (92.147)\n",
            "Test\t  Prec@1: 85.430 (Err: 14.570 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [88][0/391]\tLoss 0.2586 (0.2586)\tPrec@1 92.188 (92.188)\n",
            "Epoch: [88][55/391]\tLoss 0.2108 (0.2063)\tPrec@1 92.969 (92.759)\n",
            "Epoch: [88][110/391]\tLoss 0.2851 (0.2138)\tPrec@1 89.062 (92.490)\n",
            "Epoch: [88][165/391]\tLoss 0.2267 (0.2167)\tPrec@1 88.281 (92.399)\n",
            "Epoch: [88][220/391]\tLoss 0.2159 (0.2150)\tPrec@1 92.969 (92.460)\n",
            "Epoch: [88][275/391]\tLoss 0.2562 (0.2182)\tPrec@1 90.625 (92.363)\n",
            "Epoch: [88][330/391]\tLoss 0.1237 (0.2206)\tPrec@1 96.094 (92.270)\n",
            "Epoch: [88][385/391]\tLoss 0.3366 (0.2233)\tPrec@1 88.281 (92.192)\n",
            "Test\t  Prec@1: 83.440 (Err: 16.560 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [89][0/391]\tLoss 0.1855 (0.1855)\tPrec@1 93.750 (93.750)\n",
            "Epoch: [89][55/391]\tLoss 0.2481 (0.2082)\tPrec@1 90.625 (92.397)\n",
            "Epoch: [89][110/391]\tLoss 0.2520 (0.2104)\tPrec@1 92.969 (92.462)\n",
            "Epoch: [89][165/391]\tLoss 0.1991 (0.2130)\tPrec@1 92.969 (92.348)\n",
            "Epoch: [89][220/391]\tLoss 0.0756 (0.2145)\tPrec@1 97.656 (92.382)\n",
            "Epoch: [89][275/391]\tLoss 0.1654 (0.2173)\tPrec@1 93.750 (92.326)\n",
            "Epoch: [89][330/391]\tLoss 0.1459 (0.2201)\tPrec@1 95.312 (92.228)\n",
            "Epoch: [89][385/391]\tLoss 0.1355 (0.2209)\tPrec@1 96.094 (92.212)\n",
            "Test\t  Prec@1: 86.940 (Err: 13.060 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [90][0/391]\tLoss 0.1349 (0.1349)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [90][55/391]\tLoss 0.2704 (0.2179)\tPrec@1 91.406 (92.188)\n",
            "Epoch: [90][110/391]\tLoss 0.1520 (0.2225)\tPrec@1 92.969 (92.124)\n",
            "Epoch: [90][165/391]\tLoss 0.1497 (0.2217)\tPrec@1 96.875 (92.183)\n",
            "Epoch: [90][220/391]\tLoss 0.2631 (0.2254)\tPrec@1 90.625 (92.004)\n",
            "Epoch: [90][275/391]\tLoss 0.2836 (0.2264)\tPrec@1 91.406 (92.023)\n",
            "Epoch: [90][330/391]\tLoss 0.3146 (0.2276)\tPrec@1 91.406 (91.980)\n",
            "Epoch: [90][385/391]\tLoss 0.2224 (0.2266)\tPrec@1 93.750 (92.015)\n",
            "Test\t  Prec@1: 87.980 (Err: 12.020 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [91][0/391]\tLoss 0.1497 (0.1497)\tPrec@1 95.312 (95.312)\n",
            "Epoch: [91][55/391]\tLoss 0.2590 (0.1978)\tPrec@1 91.406 (93.220)\n",
            "Epoch: [91][110/391]\tLoss 0.2546 (0.2042)\tPrec@1 92.188 (92.694)\n",
            "Epoch: [91][165/391]\tLoss 0.2116 (0.2038)\tPrec@1 94.531 (92.691)\n",
            "Epoch: [91][220/391]\tLoss 0.1753 (0.2105)\tPrec@1 94.531 (92.520)\n",
            "Epoch: [91][275/391]\tLoss 0.3267 (0.2141)\tPrec@1 86.719 (92.349)\n",
            "Epoch: [91][330/391]\tLoss 0.2466 (0.2163)\tPrec@1 91.406 (92.284)\n",
            "Epoch: [91][385/391]\tLoss 0.2873 (0.2198)\tPrec@1 88.281 (92.157)\n",
            "Test\t  Prec@1: 85.590 (Err: 14.410 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [92][0/391]\tLoss 0.2181 (0.2181)\tPrec@1 94.531 (94.531)\n",
            "Epoch: [92][55/391]\tLoss 0.1751 (0.2093)\tPrec@1 92.969 (92.620)\n",
            "Epoch: [92][110/391]\tLoss 0.2653 (0.2032)\tPrec@1 89.844 (92.807)\n",
            "Epoch: [92][165/391]\tLoss 0.2336 (0.2114)\tPrec@1 92.188 (92.555)\n",
            "Epoch: [92][220/391]\tLoss 0.2037 (0.2119)\tPrec@1 93.750 (92.590)\n",
            "Epoch: [92][275/391]\tLoss 0.1751 (0.2150)\tPrec@1 92.969 (92.462)\n",
            "Epoch: [92][330/391]\tLoss 0.2484 (0.2190)\tPrec@1 90.625 (92.308)\n",
            "Epoch: [92][385/391]\tLoss 0.2033 (0.2193)\tPrec@1 91.406 (92.321)\n",
            "Test\t  Prec@1: 84.610 (Err: 15.390 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [93][0/391]\tLoss 0.2119 (0.2119)\tPrec@1 91.406 (91.406)\n",
            "Epoch: [93][55/391]\tLoss 0.1749 (0.2052)\tPrec@1 91.406 (92.801)\n",
            "Epoch: [93][110/391]\tLoss 0.1603 (0.2101)\tPrec@1 92.969 (92.624)\n",
            "Epoch: [93][165/391]\tLoss 0.2289 (0.2154)\tPrec@1 91.406 (92.428)\n",
            "Epoch: [93][220/391]\tLoss 0.2042 (0.2195)\tPrec@1 94.531 (92.283)\n",
            "Epoch: [93][275/391]\tLoss 0.2027 (0.2200)\tPrec@1 91.406 (92.318)\n",
            "Epoch: [93][330/391]\tLoss 0.2347 (0.2200)\tPrec@1 89.844 (92.313)\n",
            "Epoch: [93][385/391]\tLoss 0.3068 (0.2222)\tPrec@1 90.625 (92.210)\n",
            "Test\t  Prec@1: 85.430 (Err: 14.570 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [94][0/391]\tLoss 0.1738 (0.1738)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [94][55/391]\tLoss 0.1857 (0.2014)\tPrec@1 93.750 (92.857)\n",
            "Epoch: [94][110/391]\tLoss 0.1943 (0.2053)\tPrec@1 92.188 (92.828)\n",
            "Epoch: [94][165/391]\tLoss 0.1821 (0.2034)\tPrec@1 94.531 (92.922)\n",
            "Epoch: [94][220/391]\tLoss 0.2296 (0.2092)\tPrec@1 92.969 (92.739)\n",
            "Epoch: [94][275/391]\tLoss 0.2751 (0.2149)\tPrec@1 88.281 (92.482)\n",
            "Epoch: [94][330/391]\tLoss 0.1534 (0.2147)\tPrec@1 95.312 (92.504)\n",
            "Epoch: [94][385/391]\tLoss 0.1859 (0.2164)\tPrec@1 91.406 (92.434)\n",
            "Test\t  Prec@1: 86.160 (Err: 13.840 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [95][0/391]\tLoss 0.1711 (0.1711)\tPrec@1 91.406 (91.406)\n",
            "Epoch: [95][55/391]\tLoss 0.1835 (0.2068)\tPrec@1 93.750 (92.843)\n",
            "Epoch: [95][110/391]\tLoss 0.2156 (0.2056)\tPrec@1 91.406 (92.807)\n",
            "Epoch: [95][165/391]\tLoss 0.2657 (0.2090)\tPrec@1 93.750 (92.653)\n",
            "Epoch: [95][220/391]\tLoss 0.1799 (0.2130)\tPrec@1 94.531 (92.506)\n",
            "Epoch: [95][275/391]\tLoss 0.2843 (0.2178)\tPrec@1 89.062 (92.374)\n",
            "Epoch: [95][330/391]\tLoss 0.2120 (0.2200)\tPrec@1 92.188 (92.320)\n",
            "Epoch: [95][385/391]\tLoss 0.2294 (0.2227)\tPrec@1 89.844 (92.240)\n",
            "Test\t  Prec@1: 83.940 (Err: 16.060 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [96][0/391]\tLoss 0.2845 (0.2845)\tPrec@1 92.188 (92.188)\n",
            "Epoch: [96][55/391]\tLoss 0.2181 (0.2060)\tPrec@1 92.969 (92.983)\n",
            "Epoch: [96][110/391]\tLoss 0.1554 (0.1994)\tPrec@1 94.531 (93.067)\n",
            "Epoch: [96][165/391]\tLoss 0.2746 (0.2025)\tPrec@1 88.281 (92.912)\n",
            "Epoch: [96][220/391]\tLoss 0.2645 (0.2048)\tPrec@1 89.844 (92.824)\n",
            "Epoch: [96][275/391]\tLoss 0.2048 (0.2086)\tPrec@1 92.188 (92.691)\n",
            "Epoch: [96][330/391]\tLoss 0.2205 (0.2091)\tPrec@1 90.625 (92.749)\n",
            "Epoch: [96][385/391]\tLoss 0.1889 (0.2108)\tPrec@1 91.406 (92.633)\n",
            "Test\t  Prec@1: 86.770 (Err: 13.230 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [97][0/391]\tLoss 0.2571 (0.2571)\tPrec@1 89.844 (89.844)\n",
            "Epoch: [97][55/391]\tLoss 0.2523 (0.2065)\tPrec@1 90.625 (93.011)\n",
            "Epoch: [97][110/391]\tLoss 0.2582 (0.2124)\tPrec@1 90.625 (92.652)\n",
            "Epoch: [97][165/391]\tLoss 0.1014 (0.2124)\tPrec@1 98.438 (92.733)\n",
            "Epoch: [97][220/391]\tLoss 0.2755 (0.2094)\tPrec@1 91.406 (92.693)\n",
            "Epoch: [97][275/391]\tLoss 0.1863 (0.2094)\tPrec@1 92.969 (92.646)\n",
            "Epoch: [97][330/391]\tLoss 0.3795 (0.2128)\tPrec@1 91.406 (92.518)\n",
            "Epoch: [97][385/391]\tLoss 0.3023 (0.2150)\tPrec@1 89.062 (92.489)\n",
            "Test\t  Prec@1: 87.660 (Err: 12.340 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [98][0/391]\tLoss 0.2119 (0.2119)\tPrec@1 91.406 (91.406)\n",
            "Epoch: [98][55/391]\tLoss 0.1633 (0.1885)\tPrec@1 94.531 (93.345)\n",
            "Epoch: [98][110/391]\tLoss 0.2023 (0.1958)\tPrec@1 91.406 (93.131)\n",
            "Epoch: [98][165/391]\tLoss 0.1888 (0.2097)\tPrec@1 91.406 (92.649)\n",
            "Epoch: [98][220/391]\tLoss 0.1792 (0.2127)\tPrec@1 93.750 (92.580)\n",
            "Epoch: [98][275/391]\tLoss 0.2427 (0.2143)\tPrec@1 89.062 (92.482)\n",
            "Epoch: [98][330/391]\tLoss 0.1841 (0.2169)\tPrec@1 95.312 (92.416)\n",
            "Epoch: [98][385/391]\tLoss 0.2177 (0.2174)\tPrec@1 93.750 (92.370)\n",
            "Test\t  Prec@1: 85.070 (Err: 14.930 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-01\n",
            "Epoch: [99][0/391]\tLoss 0.2224 (0.2224)\tPrec@1 91.406 (91.406)\n",
            "Epoch: [99][55/391]\tLoss 0.2179 (0.2091)\tPrec@1 90.625 (92.522)\n",
            "Epoch: [99][110/391]\tLoss 0.1868 (0.2047)\tPrec@1 92.188 (92.680)\n",
            "Epoch: [99][165/391]\tLoss 0.2551 (0.2099)\tPrec@1 89.062 (92.531)\n",
            "Epoch: [99][220/391]\tLoss 0.1035 (0.2137)\tPrec@1 96.875 (92.357)\n",
            "Epoch: [99][275/391]\tLoss 0.2909 (0.2202)\tPrec@1 89.062 (92.128)\n",
            "Epoch: [99][330/391]\tLoss 0.2692 (0.2205)\tPrec@1 91.406 (92.100)\n",
            "Epoch: [99][385/391]\tLoss 0.2257 (0.2218)\tPrec@1 92.969 (92.109)\n",
            "Test\t  Prec@1: 86.310 (Err: 13.690 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [100][0/391]\tLoss 0.2051 (0.2051)\tPrec@1 92.969 (92.969)\n",
            "Epoch: [100][55/391]\tLoss 0.1540 (0.1717)\tPrec@1 94.531 (93.973)\n",
            "Epoch: [100][110/391]\tLoss 0.1387 (0.1601)\tPrec@1 95.312 (94.341)\n",
            "Epoch: [100][165/391]\tLoss 0.0981 (0.1557)\tPrec@1 96.875 (94.597)\n",
            "Epoch: [100][220/391]\tLoss 0.1820 (0.1494)\tPrec@1 95.312 (94.786)\n",
            "Epoch: [100][275/391]\tLoss 0.1179 (0.1432)\tPrec@1 95.312 (95.063)\n",
            "Epoch: [100][330/391]\tLoss 0.0890 (0.1391)\tPrec@1 95.312 (95.235)\n",
            "Epoch: [100][385/391]\tLoss 0.0659 (0.1369)\tPrec@1 99.219 (95.349)\n",
            "Test\t  Prec@1: 91.430 (Err: 8.570 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [101][0/391]\tLoss 0.1193 (0.1193)\tPrec@1 96.094 (96.094)\n",
            "Epoch: [101][55/391]\tLoss 0.0980 (0.1107)\tPrec@1 97.656 (96.680)\n",
            "Epoch: [101][110/391]\tLoss 0.0696 (0.1020)\tPrec@1 97.656 (96.924)\n",
            "Epoch: [101][165/391]\tLoss 0.0918 (0.1075)\tPrec@1 98.438 (96.635)\n",
            "Epoch: [101][220/391]\tLoss 0.1359 (0.1085)\tPrec@1 96.094 (96.567)\n",
            "Epoch: [101][275/391]\tLoss 0.1722 (0.1087)\tPrec@1 92.188 (96.530)\n",
            "Epoch: [101][330/391]\tLoss 0.1037 (0.1097)\tPrec@1 96.875 (96.441)\n",
            "Epoch: [101][385/391]\tLoss 0.1234 (0.1072)\tPrec@1 96.094 (96.507)\n",
            "Test\t  Prec@1: 91.430 (Err: 8.570 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [102][0/391]\tLoss 0.0863 (0.0863)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [102][55/391]\tLoss 0.0997 (0.0950)\tPrec@1 97.656 (96.945)\n",
            "Epoch: [102][110/391]\tLoss 0.1514 (0.0951)\tPrec@1 91.406 (96.748)\n",
            "Epoch: [102][165/391]\tLoss 0.0415 (0.0954)\tPrec@1 98.438 (96.767)\n",
            "Epoch: [102][220/391]\tLoss 0.0697 (0.0955)\tPrec@1 97.656 (96.730)\n",
            "Epoch: [102][275/391]\tLoss 0.0847 (0.0966)\tPrec@1 97.656 (96.728)\n",
            "Epoch: [102][330/391]\tLoss 0.0886 (0.0955)\tPrec@1 96.094 (96.774)\n",
            "Epoch: [102][385/391]\tLoss 0.0917 (0.0958)\tPrec@1 97.656 (96.743)\n",
            "Test\t  Prec@1: 91.610 (Err: 8.390 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [103][0/391]\tLoss 0.0978 (0.0978)\tPrec@1 96.875 (96.875)\n",
            "Epoch: [103][55/391]\tLoss 0.0939 (0.0919)\tPrec@1 96.875 (96.763)\n",
            "Epoch: [103][110/391]\tLoss 0.1046 (0.0877)\tPrec@1 96.875 (97.072)\n",
            "Epoch: [103][165/391]\tLoss 0.0861 (0.0898)\tPrec@1 96.094 (97.035)\n",
            "Epoch: [103][220/391]\tLoss 0.0704 (0.0885)\tPrec@1 96.875 (97.115)\n",
            "Epoch: [103][275/391]\tLoss 0.0878 (0.0886)\tPrec@1 96.094 (97.113)\n",
            "Epoch: [103][330/391]\tLoss 0.0963 (0.0895)\tPrec@1 96.875 (97.080)\n",
            "Epoch: [103][385/391]\tLoss 0.0749 (0.0902)\tPrec@1 98.438 (97.029)\n",
            "Test\t  Prec@1: 91.690 (Err: 8.310 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [104][0/391]\tLoss 0.0597 (0.0597)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [104][55/391]\tLoss 0.0967 (0.0773)\tPrec@1 99.219 (97.754)\n",
            "Epoch: [104][110/391]\tLoss 0.0537 (0.0792)\tPrec@1 99.219 (97.537)\n",
            "Epoch: [104][165/391]\tLoss 0.0624 (0.0818)\tPrec@1 98.438 (97.379)\n",
            "Epoch: [104][220/391]\tLoss 0.0685 (0.0827)\tPrec@1 99.219 (97.306)\n",
            "Epoch: [104][275/391]\tLoss 0.1036 (0.0834)\tPrec@1 97.656 (97.274)\n",
            "Epoch: [104][330/391]\tLoss 0.1071 (0.0842)\tPrec@1 96.094 (97.208)\n",
            "Epoch: [104][385/391]\tLoss 0.1097 (0.0845)\tPrec@1 95.312 (97.203)\n",
            "Test\t  Prec@1: 91.810 (Err: 8.190 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [105][0/391]\tLoss 0.0644 (0.0644)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [105][55/391]\tLoss 0.0554 (0.0834)\tPrec@1 98.438 (97.154)\n",
            "Epoch: [105][110/391]\tLoss 0.1325 (0.0792)\tPrec@1 95.312 (97.389)\n",
            "Epoch: [105][165/391]\tLoss 0.1673 (0.0824)\tPrec@1 95.312 (97.252)\n",
            "Epoch: [105][220/391]\tLoss 0.0688 (0.0830)\tPrec@1 97.656 (97.267)\n",
            "Epoch: [105][275/391]\tLoss 0.0428 (0.0824)\tPrec@1 100.000 (97.283)\n",
            "Epoch: [105][330/391]\tLoss 0.0681 (0.0820)\tPrec@1 96.875 (97.283)\n",
            "Epoch: [105][385/391]\tLoss 0.0872 (0.0813)\tPrec@1 97.656 (97.345)\n",
            "Test\t  Prec@1: 91.730 (Err: 8.270 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [106][0/391]\tLoss 0.0366 (0.0366)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [106][55/391]\tLoss 0.1091 (0.0768)\tPrec@1 96.094 (97.642)\n",
            "Epoch: [106][110/391]\tLoss 0.0935 (0.0759)\tPrec@1 97.656 (97.600)\n",
            "Epoch: [106][165/391]\tLoss 0.0359 (0.0765)\tPrec@1 99.219 (97.515)\n",
            "Epoch: [106][220/391]\tLoss 0.1867 (0.0766)\tPrec@1 93.750 (97.472)\n",
            "Epoch: [106][275/391]\tLoss 0.1233 (0.0770)\tPrec@1 94.531 (97.475)\n",
            "Epoch: [106][330/391]\tLoss 0.1204 (0.0781)\tPrec@1 96.094 (97.472)\n",
            "Epoch: [106][385/391]\tLoss 0.0565 (0.0781)\tPrec@1 98.438 (97.436)\n",
            "Test\t  Prec@1: 91.780 (Err: 8.220 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [107][0/391]\tLoss 0.0847 (0.0847)\tPrec@1 96.094 (96.094)\n",
            "Epoch: [107][55/391]\tLoss 0.0973 (0.0718)\tPrec@1 97.656 (97.712)\n",
            "Epoch: [107][110/391]\tLoss 0.0744 (0.0714)\tPrec@1 96.094 (97.670)\n",
            "Epoch: [107][165/391]\tLoss 0.0690 (0.0723)\tPrec@1 97.656 (97.609)\n",
            "Epoch: [107][220/391]\tLoss 0.0860 (0.0720)\tPrec@1 97.656 (97.677)\n",
            "Epoch: [107][275/391]\tLoss 0.0841 (0.0721)\tPrec@1 96.094 (97.651)\n",
            "Epoch: [107][330/391]\tLoss 0.0886 (0.0722)\tPrec@1 96.094 (97.616)\n",
            "Epoch: [107][385/391]\tLoss 0.0375 (0.0729)\tPrec@1 98.438 (97.585)\n",
            "Test\t  Prec@1: 91.740 (Err: 8.260 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [108][0/391]\tLoss 0.0381 (0.0381)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [108][55/391]\tLoss 0.0832 (0.0702)\tPrec@1 96.875 (97.684)\n",
            "Epoch: [108][110/391]\tLoss 0.0543 (0.0705)\tPrec@1 99.219 (97.720)\n",
            "Epoch: [108][165/391]\tLoss 0.0700 (0.0711)\tPrec@1 97.656 (97.680)\n",
            "Epoch: [108][220/391]\tLoss 0.1322 (0.0714)\tPrec@1 93.750 (97.667)\n",
            "Epoch: [108][275/391]\tLoss 0.1323 (0.0709)\tPrec@1 94.531 (97.662)\n",
            "Epoch: [108][330/391]\tLoss 0.0575 (0.0706)\tPrec@1 98.438 (97.689)\n",
            "Epoch: [108][385/391]\tLoss 0.0883 (0.0701)\tPrec@1 96.875 (97.717)\n",
            "Test\t  Prec@1: 91.930 (Err: 8.070 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [109][0/391]\tLoss 0.0705 (0.0705)\tPrec@1 96.875 (96.875)\n",
            "Epoch: [109][55/391]\tLoss 0.0203 (0.0676)\tPrec@1 100.000 (97.879)\n",
            "Epoch: [109][110/391]\tLoss 0.0370 (0.0671)\tPrec@1 99.219 (97.776)\n",
            "Epoch: [109][165/391]\tLoss 0.0629 (0.0663)\tPrec@1 97.656 (97.802)\n",
            "Epoch: [109][220/391]\tLoss 0.1282 (0.0672)\tPrec@1 95.312 (97.734)\n",
            "Epoch: [109][275/391]\tLoss 0.0714 (0.0674)\tPrec@1 96.875 (97.704)\n",
            "Epoch: [109][330/391]\tLoss 0.0485 (0.0674)\tPrec@1 97.656 (97.720)\n",
            "Epoch: [109][385/391]\tLoss 0.0311 (0.0679)\tPrec@1 99.219 (97.709)\n",
            "Test\t  Prec@1: 91.990 (Err: 8.010 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [110][0/391]\tLoss 0.0751 (0.0751)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [110][55/391]\tLoss 0.0546 (0.0695)\tPrec@1 98.438 (97.824)\n",
            "Epoch: [110][110/391]\tLoss 0.0636 (0.0654)\tPrec@1 98.438 (97.952)\n",
            "Epoch: [110][165/391]\tLoss 0.0528 (0.0661)\tPrec@1 99.219 (97.830)\n",
            "Epoch: [110][220/391]\tLoss 0.0974 (0.0654)\tPrec@1 96.094 (97.893)\n",
            "Epoch: [110][275/391]\tLoss 0.0667 (0.0648)\tPrec@1 97.656 (97.880)\n",
            "Epoch: [110][330/391]\tLoss 0.0631 (0.0653)\tPrec@1 97.656 (97.862)\n",
            "Epoch: [110][385/391]\tLoss 0.0893 (0.0664)\tPrec@1 95.312 (97.806)\n",
            "Test\t  Prec@1: 91.960 (Err: 8.040 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [111][0/391]\tLoss 0.0891 (0.0891)\tPrec@1 96.875 (96.875)\n",
            "Epoch: [111][55/391]\tLoss 0.0364 (0.0595)\tPrec@1 99.219 (98.089)\n",
            "Epoch: [111][110/391]\tLoss 0.0373 (0.0579)\tPrec@1 99.219 (98.114)\n",
            "Epoch: [111][165/391]\tLoss 0.0269 (0.0590)\tPrec@1 99.219 (98.089)\n",
            "Epoch: [111][220/391]\tLoss 0.0403 (0.0607)\tPrec@1 99.219 (98.003)\n",
            "Epoch: [111][275/391]\tLoss 0.0553 (0.0618)\tPrec@1 99.219 (97.996)\n",
            "Epoch: [111][330/391]\tLoss 0.0288 (0.0621)\tPrec@1 98.438 (97.987)\n",
            "Epoch: [111][385/391]\tLoss 0.0347 (0.0633)\tPrec@1 99.219 (97.934)\n",
            "Test\t  Prec@1: 91.910 (Err: 8.090 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [112][0/391]\tLoss 0.0694 (0.0694)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [112][55/391]\tLoss 0.0900 (0.0580)\tPrec@1 97.656 (98.047)\n",
            "Epoch: [112][110/391]\tLoss 0.0593 (0.0616)\tPrec@1 97.656 (97.874)\n",
            "Epoch: [112][165/391]\tLoss 0.0706 (0.0604)\tPrec@1 97.656 (97.915)\n",
            "Epoch: [112][220/391]\tLoss 0.0466 (0.0612)\tPrec@1 99.219 (97.964)\n",
            "Epoch: [112][275/391]\tLoss 0.0912 (0.0621)\tPrec@1 94.531 (97.925)\n",
            "Epoch: [112][330/391]\tLoss 0.0435 (0.0620)\tPrec@1 99.219 (97.937)\n",
            "Epoch: [112][385/391]\tLoss 0.0276 (0.0621)\tPrec@1 99.219 (97.927)\n",
            "Test\t  Prec@1: 91.920 (Err: 8.080 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [113][0/391]\tLoss 0.0504 (0.0504)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [113][55/391]\tLoss 0.0585 (0.0609)\tPrec@1 96.875 (98.047)\n",
            "Epoch: [113][110/391]\tLoss 0.0703 (0.0613)\tPrec@1 97.656 (97.987)\n",
            "Epoch: [113][165/391]\tLoss 0.0777 (0.0616)\tPrec@1 97.656 (97.995)\n",
            "Epoch: [113][220/391]\tLoss 0.0657 (0.0604)\tPrec@1 98.438 (98.042)\n",
            "Epoch: [113][275/391]\tLoss 0.0839 (0.0590)\tPrec@1 97.656 (98.084)\n",
            "Epoch: [113][330/391]\tLoss 0.0302 (0.0595)\tPrec@1 100.000 (98.024)\n",
            "Epoch: [113][385/391]\tLoss 0.0656 (0.0598)\tPrec@1 98.438 (97.994)\n",
            "Test\t  Prec@1: 91.920 (Err: 8.080 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [114][0/391]\tLoss 0.0763 (0.0763)\tPrec@1 96.094 (96.094)\n",
            "Epoch: [114][55/391]\tLoss 0.0700 (0.0569)\tPrec@1 98.438 (98.047)\n",
            "Epoch: [114][110/391]\tLoss 0.1315 (0.0575)\tPrec@1 94.531 (98.036)\n",
            "Epoch: [114][165/391]\tLoss 0.0453 (0.0584)\tPrec@1 97.656 (98.005)\n",
            "Epoch: [114][220/391]\tLoss 0.1382 (0.0581)\tPrec@1 95.312 (98.031)\n",
            "Epoch: [114][275/391]\tLoss 0.0939 (0.0583)\tPrec@1 97.656 (98.050)\n",
            "Epoch: [114][330/391]\tLoss 0.0791 (0.0576)\tPrec@1 96.875 (98.057)\n",
            "Epoch: [114][385/391]\tLoss 0.0435 (0.0581)\tPrec@1 100.000 (98.055)\n",
            "Test\t  Prec@1: 91.910 (Err: 8.090 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [115][0/391]\tLoss 0.0751 (0.0751)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [115][55/391]\tLoss 0.0708 (0.0565)\tPrec@1 96.875 (98.200)\n",
            "Epoch: [115][110/391]\tLoss 0.0486 (0.0563)\tPrec@1 99.219 (98.191)\n",
            "Epoch: [115][165/391]\tLoss 0.0750 (0.0565)\tPrec@1 97.656 (98.169)\n",
            "Epoch: [115][220/391]\tLoss 0.0658 (0.0547)\tPrec@1 97.656 (98.208)\n",
            "Epoch: [115][275/391]\tLoss 0.0367 (0.0550)\tPrec@1 99.219 (98.174)\n",
            "Epoch: [115][330/391]\tLoss 0.0695 (0.0563)\tPrec@1 97.656 (98.145)\n",
            "Epoch: [115][385/391]\tLoss 0.0422 (0.0570)\tPrec@1 98.438 (98.104)\n",
            "Test\t  Prec@1: 91.850 (Err: 8.150 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [116][0/391]\tLoss 0.0232 (0.0232)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [116][55/391]\tLoss 0.0205 (0.0541)\tPrec@1 100.000 (98.103)\n",
            "Epoch: [116][110/391]\tLoss 0.0534 (0.0556)\tPrec@1 99.219 (98.072)\n",
            "Epoch: [116][165/391]\tLoss 0.0380 (0.0549)\tPrec@1 99.219 (98.146)\n",
            "Epoch: [116][220/391]\tLoss 0.1228 (0.0569)\tPrec@1 95.312 (98.123)\n",
            "Epoch: [116][275/391]\tLoss 0.0620 (0.0562)\tPrec@1 97.656 (98.140)\n",
            "Epoch: [116][330/391]\tLoss 0.0954 (0.0568)\tPrec@1 96.875 (98.102)\n",
            "Epoch: [116][385/391]\tLoss 0.0290 (0.0575)\tPrec@1 99.219 (98.071)\n",
            "Test\t  Prec@1: 92.020 (Err: 7.980 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [117][0/391]\tLoss 0.0735 (0.0735)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [117][55/391]\tLoss 0.0596 (0.0528)\tPrec@1 98.438 (98.256)\n",
            "Epoch: [117][110/391]\tLoss 0.0361 (0.0487)\tPrec@1 99.219 (98.445)\n",
            "Epoch: [117][165/391]\tLoss 0.0197 (0.0493)\tPrec@1 99.219 (98.409)\n",
            "Epoch: [117][220/391]\tLoss 0.0362 (0.0518)\tPrec@1 98.438 (98.289)\n",
            "Epoch: [117][275/391]\tLoss 0.1126 (0.0530)\tPrec@1 94.531 (98.234)\n",
            "Epoch: [117][330/391]\tLoss 0.0498 (0.0529)\tPrec@1 98.438 (98.242)\n",
            "Epoch: [117][385/391]\tLoss 0.0312 (0.0529)\tPrec@1 98.438 (98.243)\n",
            "Test\t  Prec@1: 92.120 (Err: 7.880 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [118][0/391]\tLoss 0.0235 (0.0235)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [118][55/391]\tLoss 0.0395 (0.0532)\tPrec@1 98.438 (98.200)\n",
            "Epoch: [118][110/391]\tLoss 0.0562 (0.0514)\tPrec@1 97.656 (98.262)\n",
            "Epoch: [118][165/391]\tLoss 0.0598 (0.0512)\tPrec@1 97.656 (98.230)\n",
            "Epoch: [118][220/391]\tLoss 0.0443 (0.0521)\tPrec@1 98.438 (98.165)\n",
            "Epoch: [118][275/391]\tLoss 0.1001 (0.0521)\tPrec@1 96.875 (98.180)\n",
            "Epoch: [118][330/391]\tLoss 0.0340 (0.0522)\tPrec@1 99.219 (98.206)\n",
            "Epoch: [118][385/391]\tLoss 0.0443 (0.0525)\tPrec@1 97.656 (98.221)\n",
            "Test\t  Prec@1: 92.150 (Err: 7.850 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [119][0/391]\tLoss 0.0635 (0.0635)\tPrec@1 96.875 (96.875)\n",
            "Epoch: [119][55/391]\tLoss 0.0322 (0.0484)\tPrec@1 99.219 (98.438)\n",
            "Epoch: [119][110/391]\tLoss 0.0314 (0.0490)\tPrec@1 99.219 (98.466)\n",
            "Epoch: [119][165/391]\tLoss 0.0488 (0.0480)\tPrec@1 97.656 (98.485)\n",
            "Epoch: [119][220/391]\tLoss 0.0681 (0.0488)\tPrec@1 97.656 (98.420)\n",
            "Epoch: [119][275/391]\tLoss 0.0857 (0.0505)\tPrec@1 96.875 (98.344)\n",
            "Epoch: [119][330/391]\tLoss 0.0521 (0.0510)\tPrec@1 97.656 (98.308)\n",
            "Epoch: [119][385/391]\tLoss 0.0467 (0.0508)\tPrec@1 99.219 (98.320)\n",
            "Test\t  Prec@1: 92.050 (Err: 7.950 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [120][0/391]\tLoss 0.0495 (0.0495)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [120][55/391]\tLoss 0.0333 (0.0475)\tPrec@1 99.219 (98.424)\n",
            "Epoch: [120][110/391]\tLoss 0.0240 (0.0465)\tPrec@1 100.000 (98.430)\n",
            "Epoch: [120][165/391]\tLoss 0.0244 (0.0479)\tPrec@1 99.219 (98.419)\n",
            "Epoch: [120][220/391]\tLoss 0.1003 (0.0491)\tPrec@1 97.656 (98.356)\n",
            "Epoch: [120][275/391]\tLoss 0.0452 (0.0496)\tPrec@1 96.875 (98.330)\n",
            "Epoch: [120][330/391]\tLoss 0.0648 (0.0508)\tPrec@1 97.656 (98.282)\n",
            "Epoch: [120][385/391]\tLoss 0.0234 (0.0507)\tPrec@1 100.000 (98.298)\n",
            "Test\t  Prec@1: 91.880 (Err: 8.120 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [121][0/391]\tLoss 0.0749 (0.0749)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [121][55/391]\tLoss 0.0232 (0.0432)\tPrec@1 100.000 (98.521)\n",
            "Epoch: [121][110/391]\tLoss 0.0234 (0.0482)\tPrec@1 100.000 (98.381)\n",
            "Epoch: [121][165/391]\tLoss 0.0377 (0.0483)\tPrec@1 99.219 (98.381)\n",
            "Epoch: [121][220/391]\tLoss 0.0827 (0.0476)\tPrec@1 96.094 (98.399)\n",
            "Epoch: [121][275/391]\tLoss 0.0191 (0.0484)\tPrec@1 100.000 (98.372)\n",
            "Epoch: [121][330/391]\tLoss 0.0543 (0.0480)\tPrec@1 98.438 (98.395)\n",
            "Epoch: [121][385/391]\tLoss 0.0348 (0.0485)\tPrec@1 98.438 (98.369)\n",
            "Test\t  Prec@1: 91.910 (Err: 8.090 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [122][0/391]\tLoss 0.0279 (0.0279)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [122][55/391]\tLoss 0.0182 (0.0443)\tPrec@1 100.000 (98.591)\n",
            "Epoch: [122][110/391]\tLoss 0.0468 (0.0457)\tPrec@1 98.438 (98.430)\n",
            "Epoch: [122][165/391]\tLoss 0.0815 (0.0461)\tPrec@1 97.656 (98.438)\n",
            "Epoch: [122][220/391]\tLoss 0.0534 (0.0462)\tPrec@1 97.656 (98.430)\n",
            "Epoch: [122][275/391]\tLoss 0.0663 (0.0464)\tPrec@1 96.875 (98.432)\n",
            "Epoch: [122][330/391]\tLoss 0.0455 (0.0463)\tPrec@1 98.438 (98.433)\n",
            "Epoch: [122][385/391]\tLoss 0.0480 (0.0465)\tPrec@1 99.219 (98.435)\n",
            "Test\t  Prec@1: 91.940 (Err: 8.060 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [123][0/391]\tLoss 0.0538 (0.0538)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [123][55/391]\tLoss 0.0628 (0.0442)\tPrec@1 97.656 (98.424)\n",
            "Epoch: [123][110/391]\tLoss 0.0553 (0.0434)\tPrec@1 96.875 (98.522)\n",
            "Epoch: [123][165/391]\tLoss 0.0429 (0.0437)\tPrec@1 98.438 (98.532)\n",
            "Epoch: [123][220/391]\tLoss 0.0924 (0.0446)\tPrec@1 97.656 (98.519)\n",
            "Epoch: [123][275/391]\tLoss 0.0227 (0.0455)\tPrec@1 100.000 (98.522)\n",
            "Epoch: [123][330/391]\tLoss 0.0240 (0.0461)\tPrec@1 98.438 (98.508)\n",
            "Epoch: [123][385/391]\tLoss 0.0389 (0.0458)\tPrec@1 99.219 (98.500)\n",
            "Test\t  Prec@1: 92.120 (Err: 7.880 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [124][0/391]\tLoss 0.0765 (0.0765)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [124][55/391]\tLoss 0.0253 (0.0433)\tPrec@1 99.219 (98.465)\n",
            "Epoch: [124][110/391]\tLoss 0.0438 (0.0417)\tPrec@1 98.438 (98.564)\n",
            "Epoch: [124][165/391]\tLoss 0.0723 (0.0439)\tPrec@1 96.094 (98.508)\n",
            "Epoch: [124][220/391]\tLoss 0.0483 (0.0441)\tPrec@1 97.656 (98.522)\n",
            "Epoch: [124][275/391]\tLoss 0.0366 (0.0448)\tPrec@1 98.438 (98.469)\n",
            "Epoch: [124][330/391]\tLoss 0.0430 (0.0445)\tPrec@1 98.438 (98.475)\n",
            "Epoch: [124][385/391]\tLoss 0.0737 (0.0441)\tPrec@1 97.656 (98.482)\n",
            "Test\t  Prec@1: 91.930 (Err: 8.070 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [125][0/391]\tLoss 0.0371 (0.0371)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [125][55/391]\tLoss 0.0317 (0.0355)\tPrec@1 99.219 (98.856)\n",
            "Epoch: [125][110/391]\tLoss 0.0371 (0.0378)\tPrec@1 98.438 (98.867)\n",
            "Epoch: [125][165/391]\tLoss 0.0135 (0.0385)\tPrec@1 100.000 (98.767)\n",
            "Epoch: [125][220/391]\tLoss 0.0339 (0.0419)\tPrec@1 99.219 (98.653)\n",
            "Epoch: [125][275/391]\tLoss 0.0414 (0.0418)\tPrec@1 98.438 (98.658)\n",
            "Epoch: [125][330/391]\tLoss 0.0367 (0.0420)\tPrec@1 98.438 (98.638)\n",
            "Epoch: [125][385/391]\tLoss 0.0505 (0.0424)\tPrec@1 98.438 (98.605)\n",
            "Test\t  Prec@1: 92.040 (Err: 7.960 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [126][0/391]\tLoss 0.0634 (0.0634)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [126][55/391]\tLoss 0.0814 (0.0422)\tPrec@1 97.656 (98.661)\n",
            "Epoch: [126][110/391]\tLoss 0.0632 (0.0426)\tPrec@1 98.438 (98.670)\n",
            "Epoch: [126][165/391]\tLoss 0.0372 (0.0422)\tPrec@1 98.438 (98.654)\n",
            "Epoch: [126][220/391]\tLoss 0.0198 (0.0434)\tPrec@1 100.000 (98.643)\n",
            "Epoch: [126][275/391]\tLoss 0.0362 (0.0440)\tPrec@1 99.219 (98.636)\n",
            "Epoch: [126][330/391]\tLoss 0.0455 (0.0435)\tPrec@1 97.656 (98.643)\n",
            "Epoch: [126][385/391]\tLoss 0.0573 (0.0443)\tPrec@1 99.219 (98.599)\n",
            "Test\t  Prec@1: 91.910 (Err: 8.090 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [127][0/391]\tLoss 0.0576 (0.0576)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [127][55/391]\tLoss 0.0448 (0.0427)\tPrec@1 98.438 (98.563)\n",
            "Epoch: [127][110/391]\tLoss 0.0457 (0.0427)\tPrec@1 98.438 (98.578)\n",
            "Epoch: [127][165/391]\tLoss 0.0358 (0.0428)\tPrec@1 99.219 (98.555)\n",
            "Epoch: [127][220/391]\tLoss 0.0205 (0.0422)\tPrec@1 99.219 (98.611)\n",
            "Epoch: [127][275/391]\tLoss 0.0350 (0.0425)\tPrec@1 98.438 (98.605)\n",
            "Epoch: [127][330/391]\tLoss 0.0108 (0.0422)\tPrec@1 100.000 (98.603)\n",
            "Epoch: [127][385/391]\tLoss 0.0313 (0.0422)\tPrec@1 99.219 (98.626)\n",
            "Test\t  Prec@1: 92.040 (Err: 7.960 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [128][0/391]\tLoss 0.0756 (0.0756)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [128][55/391]\tLoss 0.0251 (0.0455)\tPrec@1 99.219 (98.465)\n",
            "Epoch: [128][110/391]\tLoss 0.0198 (0.0445)\tPrec@1 100.000 (98.494)\n",
            "Epoch: [128][165/391]\tLoss 0.0527 (0.0448)\tPrec@1 97.656 (98.485)\n",
            "Epoch: [128][220/391]\tLoss 0.0323 (0.0441)\tPrec@1 99.219 (98.515)\n",
            "Epoch: [128][275/391]\tLoss 0.0765 (0.0435)\tPrec@1 96.875 (98.539)\n",
            "Epoch: [128][330/391]\tLoss 0.0346 (0.0442)\tPrec@1 99.219 (98.485)\n",
            "Epoch: [128][385/391]\tLoss 0.0468 (0.0436)\tPrec@1 98.438 (98.508)\n",
            "Test\t  Prec@1: 91.900 (Err: 8.100 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [129][0/391]\tLoss 0.0211 (0.0211)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [129][55/391]\tLoss 0.0130 (0.0415)\tPrec@1 100.000 (98.744)\n",
            "Epoch: [129][110/391]\tLoss 0.0181 (0.0383)\tPrec@1 99.219 (98.825)\n",
            "Epoch: [129][165/391]\tLoss 0.0107 (0.0394)\tPrec@1 100.000 (98.776)\n",
            "Epoch: [129][220/391]\tLoss 0.0440 (0.0404)\tPrec@1 98.438 (98.706)\n",
            "Epoch: [129][275/391]\tLoss 0.0313 (0.0396)\tPrec@1 98.438 (98.752)\n",
            "Epoch: [129][330/391]\tLoss 0.0669 (0.0395)\tPrec@1 97.656 (98.749)\n",
            "Epoch: [129][385/391]\tLoss 0.0883 (0.0396)\tPrec@1 96.094 (98.745)\n",
            "Test\t  Prec@1: 91.870 (Err: 8.130 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [130][0/391]\tLoss 0.0423 (0.0423)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [130][55/391]\tLoss 0.0329 (0.0391)\tPrec@1 99.219 (98.647)\n",
            "Epoch: [130][110/391]\tLoss 0.0428 (0.0396)\tPrec@1 99.219 (98.656)\n",
            "Epoch: [130][165/391]\tLoss 0.0288 (0.0398)\tPrec@1 99.219 (98.663)\n",
            "Epoch: [130][220/391]\tLoss 0.0740 (0.0417)\tPrec@1 96.875 (98.586)\n",
            "Epoch: [130][275/391]\tLoss 0.0271 (0.0408)\tPrec@1 99.219 (98.624)\n",
            "Epoch: [130][330/391]\tLoss 0.0107 (0.0409)\tPrec@1 100.000 (98.617)\n",
            "Epoch: [130][385/391]\tLoss 0.0232 (0.0418)\tPrec@1 99.219 (98.595)\n",
            "Test\t  Prec@1: 91.950 (Err: 8.050 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [131][0/391]\tLoss 0.0159 (0.0159)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [131][55/391]\tLoss 0.0241 (0.0366)\tPrec@1 99.219 (98.856)\n",
            "Epoch: [131][110/391]\tLoss 0.0255 (0.0367)\tPrec@1 99.219 (98.825)\n",
            "Epoch: [131][165/391]\tLoss 0.0150 (0.0365)\tPrec@1 100.000 (98.795)\n",
            "Epoch: [131][220/391]\tLoss 0.0417 (0.0364)\tPrec@1 98.438 (98.812)\n",
            "Epoch: [131][275/391]\tLoss 0.0667 (0.0364)\tPrec@1 97.656 (98.834)\n",
            "Epoch: [131][330/391]\tLoss 0.0401 (0.0374)\tPrec@1 99.219 (98.799)\n",
            "Epoch: [131][385/391]\tLoss 0.0232 (0.0382)\tPrec@1 99.219 (98.776)\n",
            "Test\t  Prec@1: 91.850 (Err: 8.150 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [132][0/391]\tLoss 0.0184 (0.0184)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [132][55/391]\tLoss 0.0434 (0.0333)\tPrec@1 98.438 (98.870)\n",
            "Epoch: [132][110/391]\tLoss 0.0492 (0.0356)\tPrec@1 98.438 (98.846)\n",
            "Epoch: [132][165/391]\tLoss 0.0228 (0.0350)\tPrec@1 99.219 (98.894)\n",
            "Epoch: [132][220/391]\tLoss 0.0314 (0.0378)\tPrec@1 99.219 (98.745)\n",
            "Epoch: [132][275/391]\tLoss 0.0349 (0.0376)\tPrec@1 99.219 (98.757)\n",
            "Epoch: [132][330/391]\tLoss 0.0327 (0.0372)\tPrec@1 99.219 (98.787)\n",
            "Epoch: [132][385/391]\tLoss 0.0368 (0.0371)\tPrec@1 99.219 (98.788)\n",
            "Test\t  Prec@1: 91.950 (Err: 8.050 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [133][0/391]\tLoss 0.0611 (0.0611)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [133][55/391]\tLoss 0.0254 (0.0365)\tPrec@1 100.000 (98.912)\n",
            "Epoch: [133][110/391]\tLoss 0.0406 (0.0356)\tPrec@1 98.438 (98.860)\n",
            "Epoch: [133][165/391]\tLoss 0.0150 (0.0349)\tPrec@1 100.000 (98.861)\n",
            "Epoch: [133][220/391]\tLoss 0.0544 (0.0356)\tPrec@1 96.875 (98.855)\n",
            "Epoch: [133][275/391]\tLoss 0.0425 (0.0360)\tPrec@1 98.438 (98.817)\n",
            "Epoch: [133][330/391]\tLoss 0.0475 (0.0363)\tPrec@1 97.656 (98.806)\n",
            "Epoch: [133][385/391]\tLoss 0.0387 (0.0370)\tPrec@1 98.438 (98.773)\n",
            "Test\t  Prec@1: 91.810 (Err: 8.190 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [134][0/391]\tLoss 0.0636 (0.0636)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [134][55/391]\tLoss 0.0257 (0.0358)\tPrec@1 99.219 (98.870)\n",
            "Epoch: [134][110/391]\tLoss 0.0252 (0.0376)\tPrec@1 99.219 (98.818)\n",
            "Epoch: [134][165/391]\tLoss 0.0451 (0.0359)\tPrec@1 98.438 (98.894)\n",
            "Epoch: [134][220/391]\tLoss 0.0161 (0.0362)\tPrec@1 100.000 (98.869)\n",
            "Epoch: [134][275/391]\tLoss 0.0173 (0.0359)\tPrec@1 100.000 (98.890)\n",
            "Epoch: [134][330/391]\tLoss 0.0200 (0.0360)\tPrec@1 99.219 (98.862)\n",
            "Epoch: [134][385/391]\tLoss 0.0430 (0.0362)\tPrec@1 98.438 (98.840)\n",
            "Test\t  Prec@1: 91.820 (Err: 8.180 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [135][0/391]\tLoss 0.0165 (0.0165)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [135][55/391]\tLoss 0.0659 (0.0338)\tPrec@1 97.656 (98.870)\n",
            "Epoch: [135][110/391]\tLoss 0.0782 (0.0333)\tPrec@1 96.875 (98.930)\n",
            "Epoch: [135][165/391]\tLoss 0.0706 (0.0334)\tPrec@1 96.875 (98.955)\n",
            "Epoch: [135][220/391]\tLoss 0.0247 (0.0346)\tPrec@1 100.000 (98.918)\n",
            "Epoch: [135][275/391]\tLoss 0.0503 (0.0348)\tPrec@1 98.438 (98.905)\n",
            "Epoch: [135][330/391]\tLoss 0.0393 (0.0350)\tPrec@1 99.219 (98.879)\n",
            "Epoch: [135][385/391]\tLoss 0.0204 (0.0353)\tPrec@1 99.219 (98.850)\n",
            "Test\t  Prec@1: 91.670 (Err: 8.330 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [136][0/391]\tLoss 0.0228 (0.0228)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [136][55/391]\tLoss 0.0290 (0.0362)\tPrec@1 98.438 (98.856)\n",
            "Epoch: [136][110/391]\tLoss 0.0631 (0.0362)\tPrec@1 98.438 (98.867)\n",
            "Epoch: [136][165/391]\tLoss 0.0523 (0.0363)\tPrec@1 97.656 (98.842)\n",
            "Epoch: [136][220/391]\tLoss 0.0396 (0.0370)\tPrec@1 98.438 (98.773)\n",
            "Epoch: [136][275/391]\tLoss 0.0240 (0.0364)\tPrec@1 99.219 (98.794)\n",
            "Epoch: [136][330/391]\tLoss 0.0425 (0.0364)\tPrec@1 100.000 (98.796)\n",
            "Epoch: [136][385/391]\tLoss 0.0137 (0.0366)\tPrec@1 99.219 (98.808)\n",
            "Test\t  Prec@1: 91.740 (Err: 8.260 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [137][0/391]\tLoss 0.0281 (0.0281)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [137][55/391]\tLoss 0.0301 (0.0336)\tPrec@1 98.438 (98.898)\n",
            "Epoch: [137][110/391]\tLoss 0.0217 (0.0338)\tPrec@1 99.219 (98.874)\n",
            "Epoch: [137][165/391]\tLoss 0.0107 (0.0326)\tPrec@1 100.000 (98.927)\n",
            "Epoch: [137][220/391]\tLoss 0.0156 (0.0333)\tPrec@1 99.219 (98.915)\n",
            "Epoch: [137][275/391]\tLoss 0.0699 (0.0332)\tPrec@1 96.875 (98.924)\n",
            "Epoch: [137][330/391]\tLoss 0.0143 (0.0344)\tPrec@1 100.000 (98.884)\n",
            "Epoch: [137][385/391]\tLoss 0.0256 (0.0351)\tPrec@1 99.219 (98.865)\n",
            "Test\t  Prec@1: 91.830 (Err: 8.170 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [138][0/391]\tLoss 0.0169 (0.0169)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [138][55/391]\tLoss 0.0696 (0.0313)\tPrec@1 97.656 (99.023)\n",
            "Epoch: [138][110/391]\tLoss 0.0334 (0.0322)\tPrec@1 99.219 (98.986)\n",
            "Epoch: [138][165/391]\tLoss 0.0250 (0.0318)\tPrec@1 98.438 (98.993)\n",
            "Epoch: [138][220/391]\tLoss 0.0284 (0.0336)\tPrec@1 99.219 (98.918)\n",
            "Epoch: [138][275/391]\tLoss 0.0253 (0.0337)\tPrec@1 99.219 (98.905)\n",
            "Epoch: [138][330/391]\tLoss 0.0607 (0.0333)\tPrec@1 98.438 (98.931)\n",
            "Epoch: [138][385/391]\tLoss 0.0802 (0.0346)\tPrec@1 96.094 (98.869)\n",
            "Test\t  Prec@1: 91.830 (Err: 8.170 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [139][0/391]\tLoss 0.0232 (0.0232)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [139][55/391]\tLoss 0.0329 (0.0348)\tPrec@1 98.438 (98.828)\n",
            "Epoch: [139][110/391]\tLoss 0.0438 (0.0352)\tPrec@1 98.438 (98.846)\n",
            "Epoch: [139][165/391]\tLoss 0.0264 (0.0342)\tPrec@1 99.219 (98.885)\n",
            "Epoch: [139][220/391]\tLoss 0.0210 (0.0345)\tPrec@1 99.219 (98.904)\n",
            "Epoch: [139][275/391]\tLoss 0.0342 (0.0348)\tPrec@1 98.438 (98.868)\n",
            "Epoch: [139][330/391]\tLoss 0.0788 (0.0345)\tPrec@1 97.656 (98.874)\n",
            "Epoch: [139][385/391]\tLoss 0.0298 (0.0341)\tPrec@1 100.000 (98.899)\n",
            "Test\t  Prec@1: 91.600 (Err: 8.400 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [140][0/391]\tLoss 0.0808 (0.0808)\tPrec@1 96.875 (96.875)\n",
            "Epoch: [140][55/391]\tLoss 0.0053 (0.0351)\tPrec@1 100.000 (98.898)\n",
            "Epoch: [140][110/391]\tLoss 0.0083 (0.0330)\tPrec@1 100.000 (98.994)\n",
            "Epoch: [140][165/391]\tLoss 0.0259 (0.0321)\tPrec@1 99.219 (99.030)\n",
            "Epoch: [140][220/391]\tLoss 0.0360 (0.0326)\tPrec@1 99.219 (99.017)\n",
            "Epoch: [140][275/391]\tLoss 0.0959 (0.0336)\tPrec@1 96.094 (98.944)\n",
            "Epoch: [140][330/391]\tLoss 0.0572 (0.0339)\tPrec@1 97.656 (98.926)\n",
            "Epoch: [140][385/391]\tLoss 0.0337 (0.0336)\tPrec@1 99.219 (98.941)\n",
            "Test\t  Prec@1: 92.000 (Err: 8.000 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [141][0/391]\tLoss 0.0488 (0.0488)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [141][55/391]\tLoss 0.0188 (0.0360)\tPrec@1 99.219 (98.703)\n",
            "Epoch: [141][110/391]\tLoss 0.0449 (0.0335)\tPrec@1 99.219 (98.958)\n",
            "Epoch: [141][165/391]\tLoss 0.0117 (0.0333)\tPrec@1 100.000 (98.974)\n",
            "Epoch: [141][220/391]\tLoss 0.0369 (0.0336)\tPrec@1 98.438 (98.975)\n",
            "Epoch: [141][275/391]\tLoss 0.0341 (0.0335)\tPrec@1 98.438 (98.970)\n",
            "Epoch: [141][330/391]\tLoss 0.0240 (0.0330)\tPrec@1 99.219 (98.969)\n",
            "Epoch: [141][385/391]\tLoss 0.0464 (0.0338)\tPrec@1 98.438 (98.931)\n",
            "Test\t  Prec@1: 91.870 (Err: 8.130 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [142][0/391]\tLoss 0.0289 (0.0289)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [142][55/391]\tLoss 0.0465 (0.0309)\tPrec@1 97.656 (98.968)\n",
            "Epoch: [142][110/391]\tLoss 0.0202 (0.0343)\tPrec@1 99.219 (98.782)\n",
            "Epoch: [142][165/391]\tLoss 0.0238 (0.0340)\tPrec@1 100.000 (98.809)\n",
            "Epoch: [142][220/391]\tLoss 0.0519 (0.0338)\tPrec@1 98.438 (98.844)\n",
            "Epoch: [142][275/391]\tLoss 0.0293 (0.0335)\tPrec@1 98.438 (98.868)\n",
            "Epoch: [142][330/391]\tLoss 0.0512 (0.0341)\tPrec@1 98.438 (98.848)\n",
            "Epoch: [142][385/391]\tLoss 0.0083 (0.0343)\tPrec@1 100.000 (98.844)\n",
            "Test\t  Prec@1: 91.890 (Err: 8.110 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [143][0/391]\tLoss 0.0258 (0.0258)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [143][55/391]\tLoss 0.0406 (0.0316)\tPrec@1 97.656 (99.037)\n",
            "Epoch: [143][110/391]\tLoss 0.0160 (0.0309)\tPrec@1 100.000 (99.036)\n",
            "Epoch: [143][165/391]\tLoss 0.0433 (0.0306)\tPrec@1 97.656 (99.040)\n",
            "Epoch: [143][220/391]\tLoss 0.0598 (0.0328)\tPrec@1 97.656 (98.929)\n",
            "Epoch: [143][275/391]\tLoss 0.0188 (0.0342)\tPrec@1 99.219 (98.890)\n",
            "Epoch: [143][330/391]\tLoss 0.0139 (0.0341)\tPrec@1 100.000 (98.884)\n",
            "Epoch: [143][385/391]\tLoss 0.0472 (0.0345)\tPrec@1 98.438 (98.877)\n",
            "Test\t  Prec@1: 91.670 (Err: 8.330 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [144][0/391]\tLoss 0.0077 (0.0077)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [144][55/391]\tLoss 0.0427 (0.0310)\tPrec@1 98.438 (98.996)\n",
            "Epoch: [144][110/391]\tLoss 0.0558 (0.0300)\tPrec@1 98.438 (99.043)\n",
            "Epoch: [144][165/391]\tLoss 0.0073 (0.0314)\tPrec@1 100.000 (98.988)\n",
            "Epoch: [144][220/391]\tLoss 0.0439 (0.0321)\tPrec@1 98.438 (98.943)\n",
            "Epoch: [144][275/391]\tLoss 0.0307 (0.0324)\tPrec@1 98.438 (98.922)\n",
            "Epoch: [144][330/391]\tLoss 0.0097 (0.0322)\tPrec@1 100.000 (98.945)\n",
            "Epoch: [144][385/391]\tLoss 0.0253 (0.0324)\tPrec@1 99.219 (98.939)\n",
            "Test\t  Prec@1: 91.770 (Err: 8.230 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [145][0/391]\tLoss 0.0289 (0.0289)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [145][55/391]\tLoss 0.0130 (0.0314)\tPrec@1 100.000 (98.912)\n",
            "Epoch: [145][110/391]\tLoss 0.0720 (0.0319)\tPrec@1 97.656 (98.944)\n",
            "Epoch: [145][165/391]\tLoss 0.0270 (0.0324)\tPrec@1 99.219 (98.932)\n",
            "Epoch: [145][220/391]\tLoss 0.0328 (0.0331)\tPrec@1 99.219 (98.886)\n",
            "Epoch: [145][275/391]\tLoss 0.0241 (0.0330)\tPrec@1 99.219 (98.913)\n",
            "Epoch: [145][330/391]\tLoss 0.0171 (0.0328)\tPrec@1 100.000 (98.917)\n",
            "Epoch: [145][385/391]\tLoss 0.0416 (0.0333)\tPrec@1 99.219 (98.887)\n",
            "Test\t  Prec@1: 91.990 (Err: 8.010 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [146][0/391]\tLoss 0.0082 (0.0082)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [146][55/391]\tLoss 0.0314 (0.0353)\tPrec@1 100.000 (98.870)\n",
            "Epoch: [146][110/391]\tLoss 0.0141 (0.0329)\tPrec@1 100.000 (98.937)\n",
            "Epoch: [146][165/391]\tLoss 0.0206 (0.0323)\tPrec@1 100.000 (98.950)\n",
            "Epoch: [146][220/391]\tLoss 0.0308 (0.0330)\tPrec@1 98.438 (98.908)\n",
            "Epoch: [146][275/391]\tLoss 0.0164 (0.0325)\tPrec@1 100.000 (98.936)\n",
            "Epoch: [146][330/391]\tLoss 0.0860 (0.0327)\tPrec@1 95.312 (98.914)\n",
            "Epoch: [146][385/391]\tLoss 0.0725 (0.0327)\tPrec@1 96.875 (98.929)\n",
            "Test\t  Prec@1: 91.790 (Err: 8.210 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [147][0/391]\tLoss 0.0377 (0.0377)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [147][55/391]\tLoss 0.0360 (0.0302)\tPrec@1 98.438 (98.982)\n",
            "Epoch: [147][110/391]\tLoss 0.0323 (0.0292)\tPrec@1 99.219 (99.015)\n",
            "Epoch: [147][165/391]\tLoss 0.0166 (0.0298)\tPrec@1 100.000 (99.030)\n",
            "Epoch: [147][220/391]\tLoss 0.0372 (0.0300)\tPrec@1 99.219 (98.996)\n",
            "Epoch: [147][275/391]\tLoss 0.0558 (0.0302)\tPrec@1 97.656 (99.012)\n",
            "Epoch: [147][330/391]\tLoss 0.0176 (0.0307)\tPrec@1 100.000 (98.987)\n",
            "Epoch: [147][385/391]\tLoss 0.0473 (0.0310)\tPrec@1 96.875 (98.972)\n",
            "Test\t  Prec@1: 91.540 (Err: 8.460 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [148][0/391]\tLoss 0.0401 (0.0401)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [148][55/391]\tLoss 0.0480 (0.0339)\tPrec@1 98.438 (99.009)\n",
            "Epoch: [148][110/391]\tLoss 0.0218 (0.0335)\tPrec@1 99.219 (98.951)\n",
            "Epoch: [148][165/391]\tLoss 0.0305 (0.0328)\tPrec@1 99.219 (98.922)\n",
            "Epoch: [148][220/391]\tLoss 0.0271 (0.0323)\tPrec@1 98.438 (98.918)\n",
            "Epoch: [148][275/391]\tLoss 0.0586 (0.0328)\tPrec@1 97.656 (98.907)\n",
            "Epoch: [148][330/391]\tLoss 0.0475 (0.0323)\tPrec@1 97.656 (98.919)\n",
            "Epoch: [148][385/391]\tLoss 0.0492 (0.0326)\tPrec@1 97.656 (98.919)\n",
            "Test\t  Prec@1: 91.600 (Err: 8.400 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-02\n",
            "Epoch: [149][0/391]\tLoss 0.0280 (0.0280)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [149][55/391]\tLoss 0.0225 (0.0323)\tPrec@1 100.000 (98.772)\n",
            "Epoch: [149][110/391]\tLoss 0.0330 (0.0308)\tPrec@1 99.219 (98.937)\n",
            "Epoch: [149][165/391]\tLoss 0.0228 (0.0315)\tPrec@1 100.000 (98.936)\n",
            "Epoch: [149][220/391]\tLoss 0.0131 (0.0311)\tPrec@1 100.000 (98.989)\n",
            "Epoch: [149][275/391]\tLoss 0.0270 (0.0310)\tPrec@1 98.438 (98.970)\n",
            "Epoch: [149][330/391]\tLoss 0.0330 (0.0308)\tPrec@1 99.219 (98.987)\n",
            "Epoch: [149][385/391]\tLoss 0.0568 (0.0310)\tPrec@1 97.656 (98.986)\n",
            "Test\t  Prec@1: 91.660 (Err: 8.340 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [150][0/391]\tLoss 0.0521 (0.0521)\tPrec@1 97.656 (97.656)\n",
            "Epoch: [150][55/391]\tLoss 0.0454 (0.0308)\tPrec@1 98.438 (99.009)\n",
            "Epoch: [150][110/391]\tLoss 0.0150 (0.0297)\tPrec@1 100.000 (99.015)\n",
            "Epoch: [150][165/391]\tLoss 0.0125 (0.0297)\tPrec@1 100.000 (99.040)\n",
            "Epoch: [150][220/391]\tLoss 0.0194 (0.0283)\tPrec@1 99.219 (99.091)\n",
            "Epoch: [150][275/391]\tLoss 0.0303 (0.0278)\tPrec@1 98.438 (99.120)\n",
            "Epoch: [150][330/391]\tLoss 0.0522 (0.0283)\tPrec@1 98.438 (99.087)\n",
            "Epoch: [150][385/391]\tLoss 0.0250 (0.0282)\tPrec@1 99.219 (99.081)\n",
            "Test\t  Prec@1: 91.710 (Err: 8.290 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [151][0/391]\tLoss 0.0137 (0.0137)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [151][55/391]\tLoss 0.0111 (0.0239)\tPrec@1 100.000 (99.330)\n",
            "Epoch: [151][110/391]\tLoss 0.0319 (0.0239)\tPrec@1 98.438 (99.352)\n",
            "Epoch: [151][165/391]\tLoss 0.0133 (0.0240)\tPrec@1 100.000 (99.341)\n",
            "Epoch: [151][220/391]\tLoss 0.0249 (0.0243)\tPrec@1 99.219 (99.318)\n",
            "Epoch: [151][275/391]\tLoss 0.0369 (0.0249)\tPrec@1 98.438 (99.287)\n",
            "Epoch: [151][330/391]\tLoss 0.0272 (0.0250)\tPrec@1 100.000 (99.278)\n",
            "Epoch: [151][385/391]\tLoss 0.0120 (0.0253)\tPrec@1 100.000 (99.249)\n",
            "Test\t  Prec@1: 91.890 (Err: 8.110 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [152][0/391]\tLoss 0.0083 (0.0083)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [152][55/391]\tLoss 0.0160 (0.0246)\tPrec@1 99.219 (99.247)\n",
            "Epoch: [152][110/391]\tLoss 0.0123 (0.0244)\tPrec@1 100.000 (99.226)\n",
            "Epoch: [152][165/391]\tLoss 0.0342 (0.0244)\tPrec@1 99.219 (99.252)\n",
            "Epoch: [152][220/391]\tLoss 0.0089 (0.0249)\tPrec@1 100.000 (99.236)\n",
            "Epoch: [152][275/391]\tLoss 0.0132 (0.0245)\tPrec@1 100.000 (99.253)\n",
            "Epoch: [152][330/391]\tLoss 0.0248 (0.0246)\tPrec@1 99.219 (99.252)\n",
            "Epoch: [152][385/391]\tLoss 0.0213 (0.0241)\tPrec@1 99.219 (99.273)\n",
            "Test\t  Prec@1: 91.820 (Err: 8.180 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [153][0/391]\tLoss 0.0135 (0.0135)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [153][55/391]\tLoss 0.0111 (0.0220)\tPrec@1 100.000 (99.414)\n",
            "Epoch: [153][110/391]\tLoss 0.0496 (0.0229)\tPrec@1 98.438 (99.324)\n",
            "Epoch: [153][165/391]\tLoss 0.0267 (0.0227)\tPrec@1 99.219 (99.355)\n",
            "Epoch: [153][220/391]\tLoss 0.0078 (0.0238)\tPrec@1 100.000 (99.328)\n",
            "Epoch: [153][275/391]\tLoss 0.0188 (0.0238)\tPrec@1 100.000 (99.312)\n",
            "Epoch: [153][330/391]\tLoss 0.0080 (0.0236)\tPrec@1 100.000 (99.313)\n",
            "Epoch: [153][385/391]\tLoss 0.0136 (0.0232)\tPrec@1 100.000 (99.328)\n",
            "Test\t  Prec@1: 92.030 (Err: 7.970 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [154][0/391]\tLoss 0.0222 (0.0222)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [154][55/391]\tLoss 0.0146 (0.0203)\tPrec@1 100.000 (99.372)\n",
            "Epoch: [154][110/391]\tLoss 0.0126 (0.0202)\tPrec@1 100.000 (99.409)\n",
            "Epoch: [154][165/391]\tLoss 0.0092 (0.0205)\tPrec@1 100.000 (99.388)\n",
            "Epoch: [154][220/391]\tLoss 0.0155 (0.0214)\tPrec@1 99.219 (99.342)\n",
            "Epoch: [154][275/391]\tLoss 0.0761 (0.0217)\tPrec@1 96.094 (99.343)\n",
            "Epoch: [154][330/391]\tLoss 0.0092 (0.0220)\tPrec@1 100.000 (99.330)\n",
            "Epoch: [154][385/391]\tLoss 0.0342 (0.0221)\tPrec@1 98.438 (99.330)\n",
            "Test\t  Prec@1: 92.000 (Err: 8.000 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [155][0/391]\tLoss 0.0159 (0.0159)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [155][55/391]\tLoss 0.0093 (0.0209)\tPrec@1 99.219 (99.526)\n",
            "Epoch: [155][110/391]\tLoss 0.0164 (0.0214)\tPrec@1 99.219 (99.388)\n",
            "Epoch: [155][165/391]\tLoss 0.0368 (0.0220)\tPrec@1 98.438 (99.346)\n",
            "Epoch: [155][220/391]\tLoss 0.0200 (0.0220)\tPrec@1 99.219 (99.335)\n",
            "Epoch: [155][275/391]\tLoss 0.0231 (0.0220)\tPrec@1 99.219 (99.352)\n",
            "Epoch: [155][330/391]\tLoss 0.0063 (0.0218)\tPrec@1 100.000 (99.353)\n",
            "Epoch: [155][385/391]\tLoss 0.0112 (0.0215)\tPrec@1 100.000 (99.369)\n",
            "Test\t  Prec@1: 91.970 (Err: 8.030 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [156][0/391]\tLoss 0.0168 (0.0168)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [156][55/391]\tLoss 0.0293 (0.0232)\tPrec@1 99.219 (99.372)\n",
            "Epoch: [156][110/391]\tLoss 0.0081 (0.0230)\tPrec@1 100.000 (99.303)\n",
            "Epoch: [156][165/391]\tLoss 0.0099 (0.0234)\tPrec@1 100.000 (99.275)\n",
            "Epoch: [156][220/391]\tLoss 0.0045 (0.0229)\tPrec@1 100.000 (99.300)\n",
            "Epoch: [156][275/391]\tLoss 0.0388 (0.0230)\tPrec@1 99.219 (99.284)\n",
            "Epoch: [156][330/391]\tLoss 0.0243 (0.0224)\tPrec@1 98.438 (99.311)\n",
            "Epoch: [156][385/391]\tLoss 0.0239 (0.0224)\tPrec@1 99.219 (99.320)\n",
            "Test\t  Prec@1: 91.960 (Err: 8.040 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [157][0/391]\tLoss 0.0249 (0.0249)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [157][55/391]\tLoss 0.0252 (0.0222)\tPrec@1 98.438 (99.219)\n",
            "Epoch: [157][110/391]\tLoss 0.0436 (0.0218)\tPrec@1 98.438 (99.324)\n",
            "Epoch: [157][165/391]\tLoss 0.0090 (0.0214)\tPrec@1 100.000 (99.383)\n",
            "Epoch: [157][220/391]\tLoss 0.0238 (0.0212)\tPrec@1 99.219 (99.371)\n",
            "Epoch: [157][275/391]\tLoss 0.0118 (0.0206)\tPrec@1 100.000 (99.400)\n",
            "Epoch: [157][330/391]\tLoss 0.0241 (0.0206)\tPrec@1 98.438 (99.408)\n",
            "Epoch: [157][385/391]\tLoss 0.0160 (0.0207)\tPrec@1 99.219 (99.413)\n",
            "Test\t  Prec@1: 91.980 (Err: 8.020 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [158][0/391]\tLoss 0.0289 (0.0289)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [158][55/391]\tLoss 0.0168 (0.0180)\tPrec@1 99.219 (99.595)\n",
            "Epoch: [158][110/391]\tLoss 0.0322 (0.0186)\tPrec@1 98.438 (99.528)\n",
            "Epoch: [158][165/391]\tLoss 0.0114 (0.0191)\tPrec@1 100.000 (99.534)\n",
            "Epoch: [158][220/391]\tLoss 0.0117 (0.0191)\tPrec@1 100.000 (99.523)\n",
            "Epoch: [158][275/391]\tLoss 0.0103 (0.0196)\tPrec@1 100.000 (99.490)\n",
            "Epoch: [158][330/391]\tLoss 0.0448 (0.0198)\tPrec@1 98.438 (99.485)\n",
            "Epoch: [158][385/391]\tLoss 0.0121 (0.0200)\tPrec@1 100.000 (99.468)\n",
            "Test\t  Prec@1: 92.000 (Err: 8.000 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [159][0/391]\tLoss 0.0098 (0.0098)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [159][55/391]\tLoss 0.0252 (0.0216)\tPrec@1 99.219 (99.414)\n",
            "Epoch: [159][110/391]\tLoss 0.0184 (0.0215)\tPrec@1 100.000 (99.437)\n",
            "Epoch: [159][165/391]\tLoss 0.0171 (0.0219)\tPrec@1 99.219 (99.421)\n",
            "Epoch: [159][220/391]\tLoss 0.0111 (0.0224)\tPrec@1 100.000 (99.374)\n",
            "Epoch: [159][275/391]\tLoss 0.0429 (0.0223)\tPrec@1 99.219 (99.366)\n",
            "Epoch: [159][330/391]\tLoss 0.0195 (0.0220)\tPrec@1 99.219 (99.363)\n",
            "Epoch: [159][385/391]\tLoss 0.0145 (0.0216)\tPrec@1 99.219 (99.401)\n",
            "Test\t  Prec@1: 92.000 (Err: 8.000 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [160][0/391]\tLoss 0.0169 (0.0169)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [160][55/391]\tLoss 0.0147 (0.0196)\tPrec@1 100.000 (99.428)\n",
            "Epoch: [160][110/391]\tLoss 0.0049 (0.0201)\tPrec@1 100.000 (99.437)\n",
            "Epoch: [160][165/391]\tLoss 0.0353 (0.0203)\tPrec@1 97.656 (99.435)\n",
            "Epoch: [160][220/391]\tLoss 0.0149 (0.0203)\tPrec@1 100.000 (99.420)\n",
            "Epoch: [160][275/391]\tLoss 0.0147 (0.0205)\tPrec@1 99.219 (99.423)\n",
            "Epoch: [160][330/391]\tLoss 0.0112 (0.0208)\tPrec@1 100.000 (99.424)\n",
            "Epoch: [160][385/391]\tLoss 0.0295 (0.0208)\tPrec@1 99.219 (99.413)\n",
            "Test\t  Prec@1: 92.020 (Err: 7.980 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [161][0/391]\tLoss 0.0126 (0.0126)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [161][55/391]\tLoss 0.0173 (0.0189)\tPrec@1 99.219 (99.442)\n",
            "Epoch: [161][110/391]\tLoss 0.0214 (0.0194)\tPrec@1 99.219 (99.458)\n",
            "Epoch: [161][165/391]\tLoss 0.0130 (0.0197)\tPrec@1 100.000 (99.454)\n",
            "Epoch: [161][220/391]\tLoss 0.0091 (0.0198)\tPrec@1 100.000 (99.449)\n",
            "Epoch: [161][275/391]\tLoss 0.0082 (0.0198)\tPrec@1 100.000 (99.457)\n",
            "Epoch: [161][330/391]\tLoss 0.0340 (0.0198)\tPrec@1 99.219 (99.467)\n",
            "Epoch: [161][385/391]\tLoss 0.0191 (0.0199)\tPrec@1 99.219 (99.468)\n",
            "Test\t  Prec@1: 92.140 (Err: 7.860 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [162][0/391]\tLoss 0.0077 (0.0077)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [162][55/391]\tLoss 0.0244 (0.0191)\tPrec@1 99.219 (99.498)\n",
            "Epoch: [162][110/391]\tLoss 0.0221 (0.0199)\tPrec@1 99.219 (99.451)\n",
            "Epoch: [162][165/391]\tLoss 0.0331 (0.0198)\tPrec@1 99.219 (99.440)\n",
            "Epoch: [162][220/391]\tLoss 0.0150 (0.0196)\tPrec@1 100.000 (99.434)\n",
            "Epoch: [162][275/391]\tLoss 0.0065 (0.0193)\tPrec@1 100.000 (99.442)\n",
            "Epoch: [162][330/391]\tLoss 0.0082 (0.0195)\tPrec@1 100.000 (99.448)\n",
            "Epoch: [162][385/391]\tLoss 0.0066 (0.0196)\tPrec@1 100.000 (99.437)\n",
            "Test\t  Prec@1: 92.080 (Err: 7.920 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [163][0/391]\tLoss 0.0239 (0.0239)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [163][55/391]\tLoss 0.0212 (0.0192)\tPrec@1 99.219 (99.540)\n",
            "Epoch: [163][110/391]\tLoss 0.0198 (0.0189)\tPrec@1 99.219 (99.514)\n",
            "Epoch: [163][165/391]\tLoss 0.0115 (0.0199)\tPrec@1 100.000 (99.468)\n",
            "Epoch: [163][220/391]\tLoss 0.0281 (0.0205)\tPrec@1 99.219 (99.441)\n",
            "Epoch: [163][275/391]\tLoss 0.0035 (0.0204)\tPrec@1 100.000 (99.431)\n",
            "Epoch: [163][330/391]\tLoss 0.0157 (0.0202)\tPrec@1 100.000 (99.443)\n",
            "Epoch: [163][385/391]\tLoss 0.0335 (0.0203)\tPrec@1 98.438 (99.433)\n",
            "Test\t  Prec@1: 92.040 (Err: 7.960 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [164][0/391]\tLoss 0.0169 (0.0169)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [164][55/391]\tLoss 0.0200 (0.0191)\tPrec@1 100.000 (99.526)\n",
            "Epoch: [164][110/391]\tLoss 0.0087 (0.0196)\tPrec@1 100.000 (99.451)\n",
            "Epoch: [164][165/391]\tLoss 0.0107 (0.0200)\tPrec@1 100.000 (99.445)\n",
            "Epoch: [164][220/391]\tLoss 0.0452 (0.0200)\tPrec@1 99.219 (99.456)\n",
            "Epoch: [164][275/391]\tLoss 0.0081 (0.0195)\tPrec@1 100.000 (99.459)\n",
            "Epoch: [164][330/391]\tLoss 0.0246 (0.0192)\tPrec@1 98.438 (99.471)\n",
            "Epoch: [164][385/391]\tLoss 0.0365 (0.0192)\tPrec@1 99.219 (99.478)\n",
            "Test\t  Prec@1: 91.950 (Err: 8.050 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [165][0/391]\tLoss 0.0233 (0.0233)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [165][55/391]\tLoss 0.0204 (0.0174)\tPrec@1 99.219 (99.554)\n",
            "Epoch: [165][110/391]\tLoss 0.0546 (0.0188)\tPrec@1 98.438 (99.493)\n",
            "Epoch: [165][165/391]\tLoss 0.0175 (0.0186)\tPrec@1 100.000 (99.501)\n",
            "Epoch: [165][220/391]\tLoss 0.0141 (0.0192)\tPrec@1 99.219 (99.491)\n",
            "Epoch: [165][275/391]\tLoss 0.0070 (0.0191)\tPrec@1 100.000 (99.502)\n",
            "Epoch: [165][330/391]\tLoss 0.0206 (0.0193)\tPrec@1 99.219 (99.483)\n",
            "Epoch: [165][385/391]\tLoss 0.0296 (0.0192)\tPrec@1 99.219 (99.494)\n",
            "Test\t  Prec@1: 92.120 (Err: 7.880 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [166][0/391]\tLoss 0.0067 (0.0067)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [166][55/391]\tLoss 0.0259 (0.0184)\tPrec@1 100.000 (99.498)\n",
            "Epoch: [166][110/391]\tLoss 0.0170 (0.0198)\tPrec@1 99.219 (99.444)\n",
            "Epoch: [166][165/391]\tLoss 0.0048 (0.0193)\tPrec@1 100.000 (99.473)\n",
            "Epoch: [166][220/391]\tLoss 0.0125 (0.0191)\tPrec@1 100.000 (99.473)\n",
            "Epoch: [166][275/391]\tLoss 0.0202 (0.0199)\tPrec@1 99.219 (99.451)\n",
            "Epoch: [166][330/391]\tLoss 0.0078 (0.0203)\tPrec@1 100.000 (99.445)\n",
            "Epoch: [166][385/391]\tLoss 0.0284 (0.0202)\tPrec@1 98.438 (99.452)\n",
            "Test\t  Prec@1: 92.170 (Err: 7.830 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [167][0/391]\tLoss 0.0225 (0.0225)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [167][55/391]\tLoss 0.0210 (0.0194)\tPrec@1 100.000 (99.400)\n",
            "Epoch: [167][110/391]\tLoss 0.0123 (0.0211)\tPrec@1 100.000 (99.381)\n",
            "Epoch: [167][165/391]\tLoss 0.0054 (0.0198)\tPrec@1 100.000 (99.468)\n",
            "Epoch: [167][220/391]\tLoss 0.0132 (0.0194)\tPrec@1 100.000 (99.459)\n",
            "Epoch: [167][275/391]\tLoss 0.0109 (0.0192)\tPrec@1 100.000 (99.482)\n",
            "Epoch: [167][330/391]\tLoss 0.0200 (0.0189)\tPrec@1 100.000 (99.497)\n",
            "Epoch: [167][385/391]\tLoss 0.0069 (0.0187)\tPrec@1 100.000 (99.514)\n",
            "Test\t  Prec@1: 92.140 (Err: 7.860 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [168][0/391]\tLoss 0.0561 (0.0561)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [168][55/391]\tLoss 0.0154 (0.0212)\tPrec@1 100.000 (99.442)\n",
            "Epoch: [168][110/391]\tLoss 0.0146 (0.0198)\tPrec@1 100.000 (99.472)\n",
            "Epoch: [168][165/391]\tLoss 0.0239 (0.0192)\tPrec@1 99.219 (99.515)\n",
            "Epoch: [168][220/391]\tLoss 0.0090 (0.0193)\tPrec@1 100.000 (99.523)\n",
            "Epoch: [168][275/391]\tLoss 0.0282 (0.0195)\tPrec@1 99.219 (99.513)\n",
            "Epoch: [168][330/391]\tLoss 0.0210 (0.0189)\tPrec@1 99.219 (99.530)\n",
            "Epoch: [168][385/391]\tLoss 0.0106 (0.0191)\tPrec@1 100.000 (99.506)\n",
            "Test\t  Prec@1: 92.220 (Err: 7.780 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [169][0/391]\tLoss 0.0412 (0.0412)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [169][55/391]\tLoss 0.0100 (0.0179)\tPrec@1 100.000 (99.554)\n",
            "Epoch: [169][110/391]\tLoss 0.0169 (0.0179)\tPrec@1 100.000 (99.550)\n",
            "Epoch: [169][165/391]\tLoss 0.0193 (0.0179)\tPrec@1 100.000 (99.576)\n",
            "Epoch: [169][220/391]\tLoss 0.0388 (0.0188)\tPrec@1 97.656 (99.519)\n",
            "Epoch: [169][275/391]\tLoss 0.0080 (0.0190)\tPrec@1 100.000 (99.507)\n",
            "Epoch: [169][330/391]\tLoss 0.0033 (0.0188)\tPrec@1 100.000 (99.514)\n",
            "Epoch: [169][385/391]\tLoss 0.0274 (0.0192)\tPrec@1 98.438 (99.492)\n",
            "Test\t  Prec@1: 92.150 (Err: 7.850 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [170][0/391]\tLoss 0.0495 (0.0495)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [170][55/391]\tLoss 0.0073 (0.0214)\tPrec@1 100.000 (99.456)\n",
            "Epoch: [170][110/391]\tLoss 0.0119 (0.0203)\tPrec@1 100.000 (99.451)\n",
            "Epoch: [170][165/391]\tLoss 0.0139 (0.0193)\tPrec@1 100.000 (99.515)\n",
            "Epoch: [170][220/391]\tLoss 0.0116 (0.0190)\tPrec@1 100.000 (99.526)\n",
            "Epoch: [170][275/391]\tLoss 0.0600 (0.0193)\tPrec@1 97.656 (99.510)\n",
            "Epoch: [170][330/391]\tLoss 0.0164 (0.0193)\tPrec@1 99.219 (99.504)\n",
            "Epoch: [170][385/391]\tLoss 0.0104 (0.0197)\tPrec@1 100.000 (99.482)\n",
            "Test\t  Prec@1: 92.210 (Err: 7.790 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [171][0/391]\tLoss 0.0192 (0.0192)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [171][55/391]\tLoss 0.0307 (0.0206)\tPrec@1 99.219 (99.484)\n",
            "Epoch: [171][110/391]\tLoss 0.0198 (0.0212)\tPrec@1 99.219 (99.395)\n",
            "Epoch: [171][165/391]\tLoss 0.0337 (0.0211)\tPrec@1 98.438 (99.402)\n",
            "Epoch: [171][220/391]\tLoss 0.0312 (0.0206)\tPrec@1 98.438 (99.420)\n",
            "Epoch: [171][275/391]\tLoss 0.0119 (0.0199)\tPrec@1 100.000 (99.434)\n",
            "Epoch: [171][330/391]\tLoss 0.0712 (0.0196)\tPrec@1 96.875 (99.441)\n",
            "Epoch: [171][385/391]\tLoss 0.0238 (0.0199)\tPrec@1 99.219 (99.427)\n",
            "Test\t  Prec@1: 92.110 (Err: 7.890 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [172][0/391]\tLoss 0.0234 (0.0234)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [172][55/391]\tLoss 0.0356 (0.0210)\tPrec@1 98.438 (99.386)\n",
            "Epoch: [172][110/391]\tLoss 0.0055 (0.0193)\tPrec@1 100.000 (99.479)\n",
            "Epoch: [172][165/391]\tLoss 0.0177 (0.0192)\tPrec@1 99.219 (99.463)\n",
            "Epoch: [172][220/391]\tLoss 0.0187 (0.0197)\tPrec@1 99.219 (99.466)\n",
            "Epoch: [172][275/391]\tLoss 0.0130 (0.0197)\tPrec@1 99.219 (99.454)\n",
            "Epoch: [172][330/391]\tLoss 0.0132 (0.0192)\tPrec@1 99.219 (99.478)\n",
            "Epoch: [172][385/391]\tLoss 0.0091 (0.0191)\tPrec@1 100.000 (99.482)\n",
            "Test\t  Prec@1: 92.150 (Err: 7.850 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [173][0/391]\tLoss 0.0108 (0.0108)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [173][55/391]\tLoss 0.0089 (0.0158)\tPrec@1 100.000 (99.581)\n",
            "Epoch: [173][110/391]\tLoss 0.0530 (0.0171)\tPrec@1 97.656 (99.557)\n",
            "Epoch: [173][165/391]\tLoss 0.0081 (0.0180)\tPrec@1 100.000 (99.520)\n",
            "Epoch: [173][220/391]\tLoss 0.0152 (0.0180)\tPrec@1 99.219 (99.523)\n",
            "Epoch: [173][275/391]\tLoss 0.0132 (0.0183)\tPrec@1 100.000 (99.510)\n",
            "Epoch: [173][330/391]\tLoss 0.0137 (0.0187)\tPrec@1 99.219 (99.485)\n",
            "Epoch: [173][385/391]\tLoss 0.0364 (0.0188)\tPrec@1 98.438 (99.468)\n",
            "Test\t  Prec@1: 92.170 (Err: 7.830 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [174][0/391]\tLoss 0.0321 (0.0321)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [174][55/391]\tLoss 0.0256 (0.0209)\tPrec@1 99.219 (99.414)\n",
            "Epoch: [174][110/391]\tLoss 0.0327 (0.0210)\tPrec@1 99.219 (99.345)\n",
            "Epoch: [174][165/391]\tLoss 0.0209 (0.0201)\tPrec@1 99.219 (99.402)\n",
            "Epoch: [174][220/391]\tLoss 0.0100 (0.0199)\tPrec@1 100.000 (99.410)\n",
            "Epoch: [174][275/391]\tLoss 0.0077 (0.0192)\tPrec@1 100.000 (99.454)\n",
            "Epoch: [174][330/391]\tLoss 0.0192 (0.0190)\tPrec@1 99.219 (99.471)\n",
            "Epoch: [174][385/391]\tLoss 0.0137 (0.0187)\tPrec@1 100.000 (99.480)\n",
            "Test\t  Prec@1: 92.100 (Err: 7.900 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [175][0/391]\tLoss 0.0117 (0.0117)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [175][55/391]\tLoss 0.0050 (0.0189)\tPrec@1 100.000 (99.498)\n",
            "Epoch: [175][110/391]\tLoss 0.0113 (0.0179)\tPrec@1 100.000 (99.564)\n",
            "Epoch: [175][165/391]\tLoss 0.0101 (0.0184)\tPrec@1 100.000 (99.501)\n",
            "Epoch: [175][220/391]\tLoss 0.0504 (0.0188)\tPrec@1 99.219 (99.512)\n",
            "Epoch: [175][275/391]\tLoss 0.0283 (0.0183)\tPrec@1 99.219 (99.519)\n",
            "Epoch: [175][330/391]\tLoss 0.0061 (0.0184)\tPrec@1 100.000 (99.521)\n",
            "Epoch: [175][385/391]\tLoss 0.0139 (0.0184)\tPrec@1 100.000 (99.528)\n",
            "Test\t  Prec@1: 92.120 (Err: 7.880 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [176][0/391]\tLoss 0.0076 (0.0076)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [176][55/391]\tLoss 0.0151 (0.0185)\tPrec@1 99.219 (99.498)\n",
            "Epoch: [176][110/391]\tLoss 0.0054 (0.0187)\tPrec@1 100.000 (99.444)\n",
            "Epoch: [176][165/391]\tLoss 0.0219 (0.0187)\tPrec@1 99.219 (99.445)\n",
            "Epoch: [176][220/391]\tLoss 0.0310 (0.0184)\tPrec@1 99.219 (99.473)\n",
            "Epoch: [176][275/391]\tLoss 0.0135 (0.0182)\tPrec@1 100.000 (99.485)\n",
            "Epoch: [176][330/391]\tLoss 0.0119 (0.0180)\tPrec@1 100.000 (99.493)\n",
            "Epoch: [176][385/391]\tLoss 0.0132 (0.0180)\tPrec@1 100.000 (99.494)\n",
            "Test\t  Prec@1: 92.060 (Err: 7.940 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [177][0/391]\tLoss 0.0314 (0.0314)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [177][55/391]\tLoss 0.0092 (0.0185)\tPrec@1 100.000 (99.526)\n",
            "Epoch: [177][110/391]\tLoss 0.0080 (0.0189)\tPrec@1 100.000 (99.507)\n",
            "Epoch: [177][165/391]\tLoss 0.0055 (0.0192)\tPrec@1 100.000 (99.478)\n",
            "Epoch: [177][220/391]\tLoss 0.0101 (0.0191)\tPrec@1 100.000 (99.480)\n",
            "Epoch: [177][275/391]\tLoss 0.0108 (0.0193)\tPrec@1 100.000 (99.468)\n",
            "Epoch: [177][330/391]\tLoss 0.0266 (0.0196)\tPrec@1 98.438 (99.445)\n",
            "Epoch: [177][385/391]\tLoss 0.0175 (0.0196)\tPrec@1 99.219 (99.445)\n",
            "Test\t  Prec@1: 92.150 (Err: 7.850 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [178][0/391]\tLoss 0.0122 (0.0122)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [178][55/391]\tLoss 0.0113 (0.0190)\tPrec@1 100.000 (99.498)\n",
            "Epoch: [178][110/391]\tLoss 0.0112 (0.0181)\tPrec@1 100.000 (99.507)\n",
            "Epoch: [178][165/391]\tLoss 0.0082 (0.0182)\tPrec@1 100.000 (99.515)\n",
            "Epoch: [178][220/391]\tLoss 0.0152 (0.0179)\tPrec@1 99.219 (99.530)\n",
            "Epoch: [178][275/391]\tLoss 0.0126 (0.0184)\tPrec@1 99.219 (99.507)\n",
            "Epoch: [178][330/391]\tLoss 0.0068 (0.0188)\tPrec@1 100.000 (99.483)\n",
            "Epoch: [178][385/391]\tLoss 0.0077 (0.0187)\tPrec@1 100.000 (99.482)\n",
            "Test\t  Prec@1: 92.110 (Err: 7.890 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [179][0/391]\tLoss 0.0260 (0.0260)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [179][55/391]\tLoss 0.0064 (0.0164)\tPrec@1 100.000 (99.526)\n",
            "Epoch: [179][110/391]\tLoss 0.0068 (0.0171)\tPrec@1 100.000 (99.543)\n",
            "Epoch: [179][165/391]\tLoss 0.0185 (0.0180)\tPrec@1 99.219 (99.506)\n",
            "Epoch: [179][220/391]\tLoss 0.0081 (0.0179)\tPrec@1 100.000 (99.505)\n",
            "Epoch: [179][275/391]\tLoss 0.0157 (0.0184)\tPrec@1 100.000 (99.476)\n",
            "Epoch: [179][330/391]\tLoss 0.0143 (0.0189)\tPrec@1 99.219 (99.445)\n",
            "Epoch: [179][385/391]\tLoss 0.0086 (0.0187)\tPrec@1 100.000 (99.470)\n",
            "Test\t  Prec@1: 92.170 (Err: 7.830 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [180][0/391]\tLoss 0.0125 (0.0125)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [180][55/391]\tLoss 0.0236 (0.0193)\tPrec@1 99.219 (99.456)\n",
            "Epoch: [180][110/391]\tLoss 0.0115 (0.0188)\tPrec@1 100.000 (99.451)\n",
            "Epoch: [180][165/391]\tLoss 0.0095 (0.0187)\tPrec@1 100.000 (99.463)\n",
            "Epoch: [180][220/391]\tLoss 0.0165 (0.0187)\tPrec@1 100.000 (99.463)\n",
            "Epoch: [180][275/391]\tLoss 0.0315 (0.0186)\tPrec@1 99.219 (99.462)\n",
            "Epoch: [180][330/391]\tLoss 0.0349 (0.0188)\tPrec@1 98.438 (99.474)\n",
            "Epoch: [180][385/391]\tLoss 0.0120 (0.0188)\tPrec@1 100.000 (99.490)\n",
            "Test\t  Prec@1: 92.090 (Err: 7.910 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [181][0/391]\tLoss 0.0199 (0.0199)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [181][55/391]\tLoss 0.0193 (0.0192)\tPrec@1 99.219 (99.484)\n",
            "Epoch: [181][110/391]\tLoss 0.0223 (0.0187)\tPrec@1 98.438 (99.479)\n",
            "Epoch: [181][165/391]\tLoss 0.0100 (0.0190)\tPrec@1 100.000 (99.468)\n",
            "Epoch: [181][220/391]\tLoss 0.0087 (0.0186)\tPrec@1 100.000 (99.473)\n",
            "Epoch: [181][275/391]\tLoss 0.0144 (0.0186)\tPrec@1 100.000 (99.485)\n",
            "Epoch: [181][330/391]\tLoss 0.0130 (0.0182)\tPrec@1 100.000 (99.514)\n",
            "Epoch: [181][385/391]\tLoss 0.0142 (0.0182)\tPrec@1 99.219 (99.510)\n",
            "Test\t  Prec@1: 92.060 (Err: 7.940 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [182][0/391]\tLoss 0.0194 (0.0194)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [182][55/391]\tLoss 0.0164 (0.0170)\tPrec@1 99.219 (99.554)\n",
            "Epoch: [182][110/391]\tLoss 0.0201 (0.0162)\tPrec@1 99.219 (99.592)\n",
            "Epoch: [182][165/391]\tLoss 0.0124 (0.0169)\tPrec@1 100.000 (99.600)\n",
            "Epoch: [182][220/391]\tLoss 0.0161 (0.0171)\tPrec@1 100.000 (99.590)\n",
            "Epoch: [182][275/391]\tLoss 0.0236 (0.0176)\tPrec@1 100.000 (99.575)\n",
            "Epoch: [182][330/391]\tLoss 0.0174 (0.0173)\tPrec@1 99.219 (99.580)\n",
            "Epoch: [182][385/391]\tLoss 0.0035 (0.0172)\tPrec@1 100.000 (99.575)\n",
            "Test\t  Prec@1: 92.090 (Err: 7.910 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [183][0/391]\tLoss 0.0035 (0.0035)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [183][55/391]\tLoss 0.0202 (0.0191)\tPrec@1 99.219 (99.498)\n",
            "Epoch: [183][110/391]\tLoss 0.0063 (0.0185)\tPrec@1 100.000 (99.514)\n",
            "Epoch: [183][165/391]\tLoss 0.0107 (0.0180)\tPrec@1 99.219 (99.548)\n",
            "Epoch: [183][220/391]\tLoss 0.0285 (0.0183)\tPrec@1 98.438 (99.537)\n",
            "Epoch: [183][275/391]\tLoss 0.0199 (0.0182)\tPrec@1 99.219 (99.527)\n",
            "Epoch: [183][330/391]\tLoss 0.0116 (0.0182)\tPrec@1 100.000 (99.542)\n",
            "Epoch: [183][385/391]\tLoss 0.0091 (0.0180)\tPrec@1 100.000 (99.545)\n",
            "Test\t  Prec@1: 92.020 (Err: 7.980 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [184][0/391]\tLoss 0.0290 (0.0290)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [184][55/391]\tLoss 0.0211 (0.0186)\tPrec@1 99.219 (99.470)\n",
            "Epoch: [184][110/391]\tLoss 0.0231 (0.0176)\tPrec@1 99.219 (99.528)\n",
            "Epoch: [184][165/391]\tLoss 0.0178 (0.0184)\tPrec@1 100.000 (99.487)\n",
            "Epoch: [184][220/391]\tLoss 0.0072 (0.0182)\tPrec@1 100.000 (99.509)\n",
            "Epoch: [184][275/391]\tLoss 0.0073 (0.0181)\tPrec@1 100.000 (99.505)\n",
            "Epoch: [184][330/391]\tLoss 0.0556 (0.0181)\tPrec@1 98.438 (99.509)\n",
            "Epoch: [184][385/391]\tLoss 0.0130 (0.0178)\tPrec@1 100.000 (99.520)\n",
            "Test\t  Prec@1: 92.090 (Err: 7.910 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [185][0/391]\tLoss 0.0173 (0.0173)\tPrec@1 99.219 (99.219)\n",
            "Epoch: [185][55/391]\tLoss 0.0197 (0.0182)\tPrec@1 100.000 (99.484)\n",
            "Epoch: [185][110/391]\tLoss 0.0065 (0.0182)\tPrec@1 100.000 (99.521)\n",
            "Epoch: [185][165/391]\tLoss 0.0143 (0.0175)\tPrec@1 99.219 (99.553)\n",
            "Epoch: [185][220/391]\tLoss 0.0266 (0.0177)\tPrec@1 99.219 (99.526)\n",
            "Epoch: [185][275/391]\tLoss 0.0520 (0.0181)\tPrec@1 98.438 (99.524)\n",
            "Epoch: [185][330/391]\tLoss 0.0261 (0.0178)\tPrec@1 99.219 (99.542)\n",
            "Epoch: [185][385/391]\tLoss 0.0323 (0.0181)\tPrec@1 98.438 (99.524)\n",
            "Test\t  Prec@1: 91.980 (Err: 8.020 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [186][0/391]\tLoss 0.0246 (0.0246)\tPrec@1 98.438 (98.438)\n",
            "Epoch: [186][55/391]\tLoss 0.0051 (0.0167)\tPrec@1 100.000 (99.540)\n",
            "Epoch: [186][110/391]\tLoss 0.0250 (0.0176)\tPrec@1 99.219 (99.500)\n",
            "Epoch: [186][165/391]\tLoss 0.0043 (0.0175)\tPrec@1 100.000 (99.511)\n",
            "Epoch: [186][220/391]\tLoss 0.0206 (0.0170)\tPrec@1 100.000 (99.562)\n",
            "Epoch: [186][275/391]\tLoss 0.0270 (0.0171)\tPrec@1 99.219 (99.547)\n",
            "Epoch: [186][330/391]\tLoss 0.0102 (0.0176)\tPrec@1 99.219 (99.519)\n",
            "Epoch: [186][385/391]\tLoss 0.0075 (0.0172)\tPrec@1 100.000 (99.541)\n",
            "Test\t  Prec@1: 92.050 (Err: 7.950 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [187][0/391]\tLoss 0.0117 (0.0117)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [187][55/391]\tLoss 0.0182 (0.0170)\tPrec@1 100.000 (99.456)\n",
            "Epoch: [187][110/391]\tLoss 0.0118 (0.0175)\tPrec@1 100.000 (99.528)\n",
            "Epoch: [187][165/391]\tLoss 0.0411 (0.0176)\tPrec@1 99.219 (99.529)\n",
            "Epoch: [187][220/391]\tLoss 0.0124 (0.0181)\tPrec@1 100.000 (99.523)\n",
            "Epoch: [187][275/391]\tLoss 0.0060 (0.0177)\tPrec@1 100.000 (99.527)\n",
            "Epoch: [187][330/391]\tLoss 0.0242 (0.0183)\tPrec@1 99.219 (99.504)\n",
            "Epoch: [187][385/391]\tLoss 0.0185 (0.0184)\tPrec@1 99.219 (99.492)\n",
            "Test\t  Prec@1: 92.020 (Err: 7.980 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [188][0/391]\tLoss 0.0119 (0.0119)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [188][55/391]\tLoss 0.0279 (0.0166)\tPrec@1 98.438 (99.470)\n",
            "Epoch: [188][110/391]\tLoss 0.0055 (0.0169)\tPrec@1 100.000 (99.514)\n",
            "Epoch: [188][165/391]\tLoss 0.0275 (0.0175)\tPrec@1 99.219 (99.487)\n",
            "Epoch: [188][220/391]\tLoss 0.0171 (0.0171)\tPrec@1 100.000 (99.509)\n",
            "Epoch: [188][275/391]\tLoss 0.0183 (0.0175)\tPrec@1 99.219 (99.485)\n",
            "Epoch: [188][330/391]\tLoss 0.0177 (0.0174)\tPrec@1 99.219 (99.500)\n",
            "Epoch: [188][385/391]\tLoss 0.0403 (0.0177)\tPrec@1 98.438 (99.494)\n",
            "Test\t  Prec@1: 92.090 (Err: 7.910 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [189][0/391]\tLoss 0.0048 (0.0048)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [189][55/391]\tLoss 0.0030 (0.0158)\tPrec@1 100.000 (99.526)\n",
            "Epoch: [189][110/391]\tLoss 0.0039 (0.0148)\tPrec@1 100.000 (99.620)\n",
            "Epoch: [189][165/391]\tLoss 0.0083 (0.0162)\tPrec@1 100.000 (99.600)\n",
            "Epoch: [189][220/391]\tLoss 0.0145 (0.0168)\tPrec@1 99.219 (99.565)\n",
            "Epoch: [189][275/391]\tLoss 0.0288 (0.0171)\tPrec@1 99.219 (99.550)\n",
            "Epoch: [189][330/391]\tLoss 0.0222 (0.0167)\tPrec@1 100.000 (99.561)\n",
            "Epoch: [189][385/391]\tLoss 0.0086 (0.0167)\tPrec@1 100.000 (99.561)\n",
            "Test\t  Prec@1: 92.050 (Err: 7.950 )\n",
            "\n",
            "Training resnet20 model\n",
            "current lr 1.00000e-03\n",
            "Epoch: [190][0/391]\tLoss 0.0099 (0.0099)\tPrec@1 100.000 (100.000)\n",
            "Epoch: [190][55/391]\tLoss 0.0165 (0.0169)\tPrec@1 99.219 (99.540)\n",
            "Epoch: [190][110/391]\tLoss 0.0105 (0.0177)\tPrec@1 100.000 (99.521)\n",
            "Epoch: [190][165/391]\tLoss 0.0164 (0.0178)\tPrec@1 100.000 (99.515)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNU4-9Tvzg4Z"
      },
      "source": [
        "We conclude here that the results from the original ResNet paper are reproducible for the CIFAR-10 dataset. \n",
        "\n",
        "\n"
      ]
    }
  ]
}